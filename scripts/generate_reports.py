#!/usr/bin/env python
"""Generate analysis reports from W&B experiment data.

This script generates three analysis reports answering the core research question:
- ANAL-01 (format_comparison.md): Sweet-spot format analysis per layer type
- ANAL-02 (failure_modes.md): Documents where each format fails
- ANAL-03 (manifold_comparison.md): Quantifies manifold-aware optimizer benefit

Example:
    # Online mode (requires W&B credentials)
    python scripts/generate_reports.py --project altgrad

    # Offline mode (generates placeholder reports)
    python scripts/generate_reports.py --offline

    # Custom output directory
    python scripts/generate_reports.py --output-dir analysis_reports
"""
import argparse
import os
import sys
from datetime import datetime
from pathlib import Path

# Add project root to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import pandas as pd


def generate_placeholder_reports(output_dir: str) -> dict:
    """Generate placeholder reports when no W&B data is available.

    Creates reports with proper structure indicating experiments are pending.

    Args:
        output_dir: Directory for output reports

    Returns:
        Dictionary mapping report ID to file path
    """
    reports_dir = Path(output_dir)
    reports_dir.mkdir(parents=True, exist_ok=True)

    paths = {}
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # ANAL-01: Format Comparison (placeholder)
    format_comparison = f"""# ANAL-01: FP8 Format Comparison Report

**Generated:** {timestamp}
**Status:** Pending Experiments

## Executive Summary

This report will compare FP8 format performance across training experiments,
analyzing convergence quality, stability metrics, and completion rates.

**Sweet-spot format:** Pending - experiments not yet run

## Pending Experiments

The following experiments need to be run to generate this report:

| Format | Config File | Purpose |
|--------|-------------|---------|
| BF16 | experiments/configs/bf16_baseline.yaml | Baseline (no quantization) |
| E5M2 | experiments/configs/e5m2_short.yaml | IEEE FP8 standard format |
| E3M4 | experiments/configs/e3m4_uniform.yaml | Balanced exponent/mantissa |
| E1M6 | experiments/configs/e1m6_uniform.yaml | High precision, low range |
| E0M7 | experiments/configs/e0m7_uniform.yaml | Fixed-point (no exponent) |
| E7M0 | experiments/configs/e7m0_uniform.yaml | Maximum range, no precision |

## How to Run Experiments

```bash
# Run all format comparison experiments
python experiments/run_experiment.py experiments/configs/bf16_baseline.yaml
python experiments/run_experiment.py experiments/configs/e5m2_short.yaml
python experiments/run_experiment.py experiments/configs/e3m4_uniform.yaml
python experiments/run_experiment.py experiments/configs/e1m6_uniform.yaml
python experiments/run_experiment.py experiments/configs/e0m7_uniform.yaml
python experiments/run_experiment.py experiments/configs/e7m0_uniform.yaml

# After experiments complete, regenerate reports
python scripts/generate_reports.py --project <your-wandb-project>
```

## Expected Sections (after experiments)

1. Format Rankings by Loss
2. Format Rankings by Stability
3. Completion Rates by Format
4. Detailed Format Statistics
5. Improvement vs Baseline
6. Conclusions

## W&B Integration

Reports will be linked to W&B runs for full traceability once experiments are run.

---
*Generated by altgrad.analysis.ReportGenerator (placeholder mode)*
"""

    path = reports_dir / "format_comparison.md"
    path.write_text(format_comparison)
    paths["ANAL-01"] = str(path)
    print(f"  Created: {path}")

    # ANAL-02: Failure Modes (placeholder)
    failure_modes = f"""# ANAL-02: Failure Mode Analysis Report

**Generated:** {timestamp}
**Status:** Pending Experiments

## Executive Summary

This report will analyze training failures across FP8 formats, classifying
failure modes and providing recommendations for mitigation.

## Failure Mode Definitions

| Mode | Description | Indicators |
|------|-------------|------------|
| nan_loss | Training loss became NaN/Inf | Loss = NaN, Inf |
| bit_stall | Gradients quantized to zero | Bit stall rate > 50% |
| overflow | Gradient overflow | Overflow rate > 10% |
| early_stop | Stopped early | < 10% of max steps |
| gradient_vanishing | Gradients vanished | > 70% near-zero gradients |
| unknown | Could not classify | None of above |

## Expected Failure Patterns (Hypothesis)

Based on format characteristics, we expect:

### E7M0 (Maximum range, no precision)
- **Expected failures:** nan_loss, bit_stall
- **Reason:** 0 mantissa bits means all values are powers of 2
- **Failure Rate Hypothesis:** >90%

### E0M7 (Fixed-point)
- **Expected failures:** overflow (on unnormalized tensors)
- **Reason:** Range limited to [-1, 1) with constant 1/128 spacing
- **Failure Rate Hypothesis:** 40-60%

### E1M6 (High precision, low range)
- **Expected failures:** overflow on large gradients
- **Reason:** Limited exponent range

### E3M4, E5M2 (Balanced formats)
- **Expected failures:** Occasional bit_stall in extreme cases
- **Reason:** Good balance of range and precision

## Pending Experiments

Run experiments with each format to collect failure data:

```bash
python experiments/run_experiment.py experiments/configs/e7m0_uniform.yaml
python experiments/run_experiment.py experiments/configs/e0m7_uniform.yaml
# ... other formats
```

## How Failures Are Classified

1. Training runs are collected from W&B
2. Runs with state != "finished" are analyzed
3. Failure mode is determined by examining:
   - Final loss (NaN check)
   - Bit stall rate (>50% threshold)
   - Overflow rate (>10% threshold)
   - Steps completed vs max_steps

---
*Generated by altgrad.analysis.ReportGenerator (placeholder mode)*
"""

    path = reports_dir / "failure_modes.md"
    path.write_text(failure_modes)
    paths["ANAL-02"] = str(path)
    print(f"  Created: {path}")

    # ANAL-03: Manifold Comparison (placeholder)
    manifold_comparison = f"""# ANAL-03: Manifold-Aware Optimizer Comparison Report

**Generated:** {timestamp}
**Status:** Pending Experiments

## Executive Summary

This report will compare ManifoldAdamW (geometry-aware optimizer) versus
standard AdamW on E5M2 format experiments.

**ManifoldAdamW benefit:** Pending - experiments not yet run

## Hypothesis

ManifoldAdamW should improve training stability and convergence on quantized
formats by:
1. Scaling gradients by local stiffness (inverse of ULP spacing)
2. Preventing updates smaller than quantization resolution
3. Adapting step sizes to the non-uniform FP8 grid

## Pending Experiments

Run both standard and ManifoldAdamW variants on E5M2:

| Config | Optimizer | use_manifold_aware |
|--------|-----------|-------------------|
| e5m2_short.yaml | AdamW | false |
| e5m2_manifold.yaml | ManifoldAdamW | true |

```bash
# Standard AdamW on E5M2
python experiments/run_experiment.py experiments/configs/e5m2_short.yaml

# ManifoldAdamW on E5M2
python experiments/run_experiment.py experiments/configs/e5m2_manifold.yaml

# Regenerate reports after experiments
python scripts/generate_reports.py --project <your-wandb-project>
```

## Expected Comparison Metrics

| Metric | Standard AdamW | ManifoldAdamW | Expected Improvement |
|--------|---------------|---------------|---------------------|
| best_loss | TBD | TBD | 5-15% lower |
| bit_stall_rate | TBD | TBD | 10-30% lower |
| completion_rate | TBD | TBD | Higher |

## Stiffness Preconditioning

ManifoldAdamW modifies the gradient update:

```python
# Standard AdamW
param = param - lr * grad

# ManifoldAdamW
stiffness = 1.0 / ulp_spacing(param)  # Inverse of quantization grid spacing
preconditioned_grad = grad * stiffness
param = param - lr * preconditioned_grad
```

This scales gradients inversely to the local grid spacing, giving larger
updates in dense regions (small ULPs) and smaller updates in sparse regions.

## W&B Run Links (after experiments)

Runs will be linked here for full traceability:
- Standard AdamW run: `wandb.ai/<entity>/<project>/runs/<run-id>`
- ManifoldAdamW run: `wandb.ai/<entity>/<project>/runs/<run-id>`

---
*Generated by altgrad.analysis.ReportGenerator (placeholder mode)*
"""

    path = reports_dir / "manifold_comparison.md"
    path.write_text(manifold_comparison)
    paths["ANAL-03"] = str(path)
    print(f"  Created: {path}")

    return paths


def generate_online_reports(project: str, output_dir: str, entity: str = None) -> dict:
    """Generate reports from W&B experiment data.

    Args:
        project: W&B project name
        output_dir: Directory for output reports
        entity: W&B entity (username or team)

    Returns:
        Dictionary mapping report ID to file path
    """
    from altgrad.analysis import (
        ExperimentDataLoader,
        FormatComparator,
        FailureAnalyzer,
        ReportGenerator,
    )

    print(f"Loading data from W&B project: {project}")

    loader = ExperimentDataLoader(project=project, entity=entity)
    generator = ReportGenerator(reports_dir=output_dir)

    paths = {}

    # ANAL-01: Format Comparison
    print("\nGenerating ANAL-01: Format Comparison Report...")
    try:
        format_runs = loader.get_format_comparison_runs()
        if format_runs.empty:
            print("  No format comparison runs found - generating placeholder")
            return generate_placeholder_reports(output_dir)

        path = generator.generate_format_comparison(
            format_runs,
            output_path=str(Path(output_dir) / "format_comparison.md"),
        )
        paths["ANAL-01"] = path
        print(f"  Created: {path}")
    except Exception as e:
        print(f"  Error generating format comparison: {e}")
        print("  Falling back to placeholder report")
        return generate_placeholder_reports(output_dir)

    # ANAL-02: Failure Modes
    print("\nGenerating ANAL-02: Failure Modes Report...")
    try:
        analyzer = FailureAnalyzer()
        # Get failed runs (not completed)
        failed_runs = format_runs[~format_runs["completed"]].copy()
        if not failed_runs.empty:
            classified = analyzer.classify_failure_mode(failed_runs)
        else:
            classified = pd.DataFrame()

        path = generator.generate_failure_modes(
            classified,
            output_path=str(Path(output_dir) / "failure_modes.md"),
        )
        paths["ANAL-02"] = path
        print(f"  Created: {path}")
    except Exception as e:
        print(f"  Error generating failure modes: {e}")

    # ANAL-03: Manifold Comparison
    print("\nGenerating ANAL-03: Manifold Comparison Report...")
    try:
        manifold_runs = loader.get_manifold_comparison_runs()
        path = generator.generate_manifold_comparison(
            manifold_runs,
            output_path=str(Path(output_dir) / "manifold_comparison.md"),
        )
        paths["ANAL-03"] = path
        print(f"  Created: {path}")
    except Exception as e:
        print(f"  Error generating manifold comparison: {e}")

    return paths


def main() -> None:
    """Main entry point for report generation."""
    parser = argparse.ArgumentParser(
        description="Generate analysis reports from W&B experiment data"
    )
    parser.add_argument(
        "--project",
        default="altgrad",
        help="W&B project name (default: altgrad)",
    )
    parser.add_argument(
        "--entity",
        default=None,
        help="W&B entity (username or team). Uses default if not specified.",
    )
    parser.add_argument(
        "--output-dir",
        default="reports",
        help="Output directory for reports (default: reports)",
    )
    parser.add_argument(
        "--offline",
        action="store_true",
        help="Generate placeholder reports without W&B connection",
    )
    args = parser.parse_args()

    print("=" * 60)
    print("AltGrad Analysis Report Generator")
    print("=" * 60)

    if args.offline:
        print("\nRunning in OFFLINE mode (placeholder reports)")
        paths = generate_placeholder_reports(args.output_dir)
    else:
        print(f"\nRunning in ONLINE mode (W&B project: {args.project})")
        try:
            paths = generate_online_reports(
                args.project, args.output_dir, args.entity
            )
        except ImportError as e:
            print(f"\nW&B not available: {e}")
            print("Falling back to offline mode...")
            paths = generate_placeholder_reports(args.output_dir)
        except Exception as e:
            print(f"\nError connecting to W&B: {e}")
            print("Falling back to offline mode...")
            paths = generate_placeholder_reports(args.output_dir)

    print("\n" + "=" * 60)
    print("Report Generation Complete")
    print("=" * 60)
    print("\nGenerated reports:")
    for report_id, path in paths.items():
        print(f"  {report_id}: {path}")

    print("\nNext steps:")
    print("  1. Run experiments to collect W&B data")
    print("  2. Re-run this script without --offline to populate reports")
    print("  3. Review reports for research insights")


if __name__ == "__main__":
    main()
