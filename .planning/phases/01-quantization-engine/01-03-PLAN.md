---
phase: 01-quantization-engine
plan: 03
type: execute
wave: 3
depends_on: ["01-02"]
files_modified:
  - altgrad/quantization/scaling.py
  - altgrad/quantization/diagnostics.py
  - altgrad/quantization/__init__.py
  - tests/test_scaling.py
  - tests/test_diagnostics.py
autonomous: true

must_haves:
  truths:
    - "Per-tensor scaling computes scale from tensor's dynamic range"
    - "Amax history buffer smooths scale updates across batches"
    - "Bit-stall detection identifies when quantized updates round to zero"
    - "All quantization components are accessible from package root"
  artifacts:
    - path: "altgrad/quantization/scaling.py"
      provides: "Dynamic scaling with amax history"
      exports: ["compute_scale", "AmaxHistory", "ScalingConfig"]
    - path: "altgrad/quantization/diagnostics.py"
      provides: "Bit-stall detection and counters"
      exports: ["BitStallDetector", "detect_bit_stall"]
    - path: "altgrad/quantization/__init__.py"
      provides: "Complete package exports"
      exports: ["quantize", "dequantize", "FP8Format", "E0M7", "E1M6", "E3M4", "E5M2", "E7M0", "compute_scale", "AmaxHistory", "BitStallDetector"]
    - path: "tests/test_scaling.py"
      provides: "Scaling correctness tests"
      min_lines: 60
    - path: "tests/test_diagnostics.py"
      provides: "Bit-stall detection tests"
      min_lines: 40
  key_links:
    - from: "altgrad/quantization/scaling.py"
      to: "altgrad/quantization/formats.py"
      via: "format max value lookup"
      pattern: "format\\..*max|FP8Format"
    - from: "altgrad/quantization/diagnostics.py"
      to: "altgrad/quantization/ops.py"
      via: "quantization for stall detection"
      pattern: "quantize|to_bits"
---

<objective>
Implement per-tensor dynamic scaling with amax history and bit-stall detection.

Purpose: Dynamic scaling ensures tensors use the full FP8 range. Bit-stall detection identifies when gradients are too small to change quantized weights -- a critical diagnostic for understanding format limitations.

Output: Complete `altgrad/quantization/` package with scaling, diagnostics, and unified exports.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-quantization-engine/01-01-SUMMARY.md
@.planning/phases/01-quantization-engine/01-02-SUMMARY.md

# Per-Tensor Scaling

Scale factor maps tensor range to FP8 representable range:
  scale = amax(tensor) / max_fp8_value

Where max_fp8_value depends on format (e.g., 448 for E5M2, 240 for E4M3).

# Delayed Amax Tracking

Rather than recomputing scale every forward pass (expensive, noisy):
1. Maintain history buffer of recent amax values (length 16-32)
2. Use moving average or max-of-history for stable scaling
3. Update history each batch, scale lags slightly

Pattern from NVIDIA's FP8 training:
```python
class AmaxHistory:
    def __init__(self, history_len=16):
        self.history = deque(maxlen=history_len)

    def update(self, tensor):
        self.history.append(tensor.abs().max().item())

    def get_scale(self, format):
        amax = max(self.history) if self.history else 1.0
        return amax / format.max_value
```

# Bit-Stall Detection (STAB-04)

Bit-stall: when round(b + delta_b) == b despite non-zero gradient.

This happens when:
- Weight is in a region where ULP > gradient magnitude * learning_rate
- The update is too small to change the quantized representation

Detection:
```python
def detect_bit_stall(weight, grad, lr, format, scale):
    # Compute what the update would be
    update = -lr * grad
    # Quantize current weight
    w_q = quantize(weight, format, scale)
    # Quantize updated weight
    w_new_q = quantize(weight + update, format, scale)
    # Stall where quantized values didn't change
    stall_mask = (w_q == w_new_q) & (grad != 0)
    return stall_mask.sum(), stall_mask.numel()
```
</context>

<tasks>

<task type="auto">
  <name>Task 1: Per-tensor scaling with amax history</name>
  <files>altgrad/quantization/scaling.py, tests/test_scaling.py</files>
  <action>
    Create scaling.py with:

    1. ScalingConfig dataclass:
       - history_len: int = 16
       - scale_min: float = 1e-10 (prevent division by zero)

    2. AmaxHistory class:
       - __init__(history_len): initialize deque
       - update(tensor): append tensor.abs().amax().item()
       - get_amax(): return max of history (or 1.0 if empty)
       - reset(): clear history

    3. compute_scale(amax, format) function:
       - Returns amax / format.max_representable_value
       - Clamps to scale_min

    4. Format.max_representable_value property:
       - E5M2: 57344 (largest finite value)
       - E3M4: 240
       - E1M6: 1.96875
       - E0M7: 0.9921875 (127/128)
       - E7M0: 2^63 (largest power of 2 with bias=63)

    Create test_scaling.py:
    - Test amax history tracks correctly over multiple updates
    - Test scale computation for each format
    - Test history reset
    - Test edge cases (empty history, all zeros)
  </action>
  <verify>python -m pytest tests/test_scaling.py -v</verify>
  <done>AmaxHistory correctly tracks tensor ranges, compute_scale produces valid scales for all formats</done>
</task>

<task type="auto">
  <name>Task 2: Bit-stall detection</name>
  <files>altgrad/quantization/diagnostics.py, tests/test_diagnostics.py</files>
  <action>
    Create diagnostics.py with:

    1. detect_bit_stall(weight, grad, lr, format, scale) function:
       - Computes proposed update: update = -lr * grad
       - Quantizes current weight: w_q = quantize(weight, format, scale)
       - Quantizes updated weight: w_new_q = quantize(weight + update, format, scale)
       - Returns (stall_count, total_count) where stall = (w_q == w_new_q) & (grad.abs() > 1e-10)

    2. BitStallDetector class:
       - __init__(): initialize counters (stall_count, total_count, step_count)
       - update(weight, grad, lr, format, scale): call detect_bit_stall, accumulate
       - get_stall_rate(): return stall_count / total_count
       - reset(): clear counters
       - get_stats(): return dict with stall_rate, stall_count, total_count, steps

    Create test_diagnostics.py:
    - Test that small gradients cause stalls (grad < ULP / lr)
    - Test that large gradients don't cause stalls
    - Test E7M0 has high stall rate with normal-sized gradients (large ULPs)
    - Test accumulation over multiple updates
  </action>
  <verify>python -m pytest tests/test_diagnostics.py -v</verify>
  <done>BitStallDetector correctly identifies when quantized updates round to zero</done>
</task>

<task type="auto">
  <name>Task 3: Package integration and exports</name>
  <files>altgrad/quantization/__init__.py, altgrad/__init__.py</files>
  <action>
    Update altgrad/quantization/__init__.py:
    - Import and re-export from formats.py: FP8Format, E0M7, E1M6, E3M4, E5M2, E7M0, FORMAT_REGISTRY
    - Import and re-export from ops.py: quantize, dequantize
    - Import and re-export from scaling.py: compute_scale, AmaxHistory, ScalingConfig
    - Import and re-export from diagnostics.py: BitStallDetector, detect_bit_stall
    - Define __all__ with all public exports

    Create altgrad/__init__.py:
    - Import quantization subpackage
    - Add version: __version__ = "0.1.0"

    Ensure package structure:
    ```
    altgrad/
      __init__.py
      quantization/
        __init__.py
        formats.py
        ops.py
        scaling.py
        diagnostics.py
    tests/
      test_formats.py
      test_ops.py
      test_scaling.py
      test_diagnostics.py
    ```
  </action>
  <verify>
    python -c "from altgrad.quantization import quantize, FP8Format, E5M2, compute_scale, AmaxHistory, BitStallDetector; print('All exports available')"
    python -m pytest tests/ -v
  </verify>
  <done>Complete altgrad.quantization package with all exports, all tests pass</done>
</task>

</tasks>

<verification>
```bash
# Run all quantization tests
python -m pytest tests/ -v --tb=short

# Verify complete package exports
python -c "
from altgrad.quantization import (
    FP8Format, E0M7, E1M6, E3M4, E5M2, E7M0, FORMAT_REGISTRY,
    quantize, dequantize,
    compute_scale, AmaxHistory, ScalingConfig,
    BitStallDetector, detect_bit_stall
)
print('All exports verified')
"

# Quick integration test: quantize with dynamic scaling
python -c "
import torch
from altgrad.quantization import quantize, E5M2, AmaxHistory, compute_scale

x = torch.randn(100)
history = AmaxHistory()
history.update(x)
scale = compute_scale(history.get_amax(), E5M2)
x_q = quantize(x, E5M2, torch.tensor(scale))
print(f'Quantized tensor range: [{x_q.min():.4f}, {x_q.max():.4f}]')
"
```
</verification>

<success_criteria>
1. All tests pass (formats, ops, scaling, diagnostics)
2. AmaxHistory correctly tracks dynamic range across batches
3. BitStallDetector correctly identifies zero-update situations
4. Complete package exports all required symbols
5. Integration test shows quantization + scaling working together
</success_criteria>

<output>
After completion, create `.planning/phases/01-quantization-engine/01-03-SUMMARY.md`
</output>
