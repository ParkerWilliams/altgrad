---
phase: 01-quantization-engine
plan: 02
type: tdd
wave: 2
depends_on: ["01-01"]
files_modified:
  - altgrad/quantization/ops.py
  - tests/test_ops.py
autonomous: true

must_haves:
  truths:
    - "quantize() converts FP32 tensor to simulated FP8 representation"
    - "dequantize() converts simulated FP8 back to FP32"
    - "STE passes gradients through quantize/dequantize unchanged"
    - "Quantization respects format's representable range"
  artifacts:
    - path: "altgrad/quantization/ops.py"
      provides: "Quantize/dequantize autograd functions with STE"
      exports: ["quantize", "dequantize", "QuantizeFunc", "DequantizeFunc"]
    - path: "tests/test_ops.py"
      provides: "Quantization operation tests including gradient flow"
      min_lines: 80
  key_links:
    - from: "altgrad/quantization/ops.py"
      to: "altgrad/quantization/formats.py"
      via: "FP8Format usage"
      pattern: "from.*formats import.*FP8Format"
    - from: "altgrad/quantization/ops.py"
      to: "torch.autograd.Function"
      via: "STE implementation"
      pattern: "class.*Function.*backward.*grad_output"
---

<objective>
Implement quantize/dequantize operations with Straight-Through Estimator gradient override using TDD.

Purpose: The STE is the critical mechanism that allows gradients to flow through non-differentiable quantization. This must be correct before any training can occur.

Output: `altgrad/quantization/ops.py` with autograd-compatible quantize/dequantize functions, validated by gradient flow tests.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-quantization-engine/01-01-SUMMARY.md

# Straight-Through Estimator (STE)

Quantization is non-differentiable (step function). STE bypasses this:

Forward: y = quantize(x)  # actual quantization
Backward: dx = dy         # gradient passes through unchanged

Implementation via torch.autograd.Function:
```python
class QuantizeFunc(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x, format, scale):
        # Actual quantization: scale -> to_bits -> clamp -> to_real -> unscale
        return quantized_x

    @staticmethod
    def backward(ctx, grad_output):
        # STE: pass gradient unchanged
        return grad_output, None, None  # grad for x, None for format/scale
```

# Simulated Quantization Pattern

Store in FP32, simulate FP8 at boundaries:
1. Scale input: x_scaled = x / scale
2. Quantize: x_q = to_real(to_bits(x_scaled))  # via format's transfer functions
3. Unscale: x_out = x_q * scale

The "fake quantization" happens in to_bits/to_real round-trip.
</context>

<feature>
  <name>STE Quantize/Dequantize</name>
  <files>altgrad/quantization/ops.py, tests/test_ops.py</files>
  <behavior>
    quantize(x: Tensor, format: FP8Format, scale: Tensor) -> Tensor:
      - Forward: x / scale -> to_bits -> to_real -> * scale
      - Backward: gradient passes through unchanged (STE)

    dequantize(x_q: Tensor, scale: Tensor) -> Tensor:
      - Forward: x_q * scale (already dequantized by quantize, this is identity or scale-only)
      - Backward: gradient passes through unchanged

    Test cases:
      - quantize([0.5, 1.0, 2.0], E5M2, scale=1.0) produces valid FP8 values
      - Gradient of sum(quantize(x)).backward() equals ones (STE)
      - Gradient magnitude preserved through quantize/dequantize chain
      - Values outside representable range clamp correctly
  </behavior>
  <implementation>
    1. Create QuantizeFunc(torch.autograd.Function) with STE backward
    2. Create wrapper quantize() that applies scaling and calls QuantizeFunc
    3. Implement vectorized to_bits/to_real for tensor operations
    4. Handle edge cases: zeros, infinities, values outside range
    5. Create dequantize() (may be simple scale multiplication)
  </implementation>
</feature>

<verification>
```bash
# Run ops tests
python -m pytest tests/test_ops.py -v

# Verify gradient flow
python -c "
import torch
from altgrad.quantization import quantize, E5M2
x = torch.randn(10, requires_grad=True)
y = quantize(x, E5M2, torch.tensor(1.0))
y.sum().backward()
print('Gradient shape:', x.grad.shape)
print('Gradient matches ones:', torch.allclose(x.grad, torch.ones_like(x)))
"
```
</verification>

<success_criteria>
1. All ops tests pass (quantization correctness, gradient flow)
2. STE correctly passes gradients through unchanged
3. Quantize/dequantize work with all 5 FP8 formats
4. Tensor operations are vectorized (no Python loops over elements)
5. Edge cases handled: zeros, near-zero, large values, infinities
</success_criteria>

<output>
After completion, create `.planning/phases/01-quantization-engine/01-02-SUMMARY.md`
</output>
