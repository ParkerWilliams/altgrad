---
phase: 01-quantization-engine
plan: 02
type: tdd
wave: 2
depends_on: ["01-01"]
files_modified:
  - altgrad/quantization/ops.py
  - tests/test_ops.py
autonomous: true

must_haves:
  truths:
    - "quantize() converts FP32 tensor to simulated FP8 representation"
    - "dequantize() converts simulated FP8 back to FP32"
    - "STE passes gradients through quantize/dequantize unchanged"
    - "Quantization respects format's representable range"
  artifacts:
    - path: "altgrad/quantization/ops.py"
      provides: "Quantize/dequantize autograd functions with STE"
      exports: ["quantize", "dequantize", "QuantizeFunc", "DequantizeFunc"]
      min_lines: 100
    - path: "tests/test_ops.py"
      provides: "Quantization operation tests including gradient flow"
      min_lines: 80
  key_links:
    - from: "altgrad/quantization/ops.py"
      to: "altgrad/quantization/formats.py"
      via: "FP8Format usage"
      pattern: "from.*formats import.*FP8Format"
    - from: "altgrad/quantization/ops.py"
      to: "torch.autograd.Function"
      via: "STE implementation"
      pattern: "class.*Function.*backward.*grad_output"
---

<objective>
Implement quantize/dequantize operations with Straight-Through Estimator gradient override using TDD.

Purpose: The STE is the critical mechanism that allows gradients to flow through non-differentiable quantization. This must be correct before any training can occur.

Output: `altgrad/quantization/ops.py` with autograd-compatible quantize/dequantize functions, validated by gradient flow tests.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-quantization-engine/01-01-SUMMARY.md

# Straight-Through Estimator (STE)

Quantization is non-differentiable (step function). STE bypasses this:

Forward: y = quantize(x)  # actual quantization
Backward: dx = dy         # gradient passes through unchanged

Implementation via torch.autograd.Function:
```python
class QuantizeFunc(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x, format, scale):
        # Actual quantization: scale -> to_bits -> clamp -> to_real -> unscale
        return quantized_x

    @staticmethod
    def backward(ctx, grad_output):
        # STE: pass gradient unchanged
        return grad_output, None, None  # grad for x, None for format/scale
```

# Simulated Quantization Pattern

Store in FP32, simulate FP8 at boundaries:
1. Scale input: x_scaled = x / scale
2. Quantize: x_q = to_real(to_bits(x_scaled))  # via format's transfer functions
3. Unscale: x_out = x_q * scale

The "fake quantization" happens in to_bits/to_real round-trip.
</context>

<tasks>

<task type="tdd">
  <name>Task 1: Write failing tests for quantize/dequantize ops</name>
  <files>tests/test_ops.py</files>
  <action>
    Create tests/test_ops.py with comprehensive test cases BEFORE implementation.

    Test structure:
    1. test_quantize_returns_tensor():
       - quantize(x, E5M2, scale) returns a tensor of same shape
       - Output dtype is float32 (simulated quantization)

    2. test_quantize_values_are_fp8_representable():
       - For E5M2: output values match format's discrete levels
       - quantize([1.0, 2.0, 3.0], E5M2, scale=1.0) returns values that are exactly representable in E5M2

    3. test_quantize_respects_scale():
       - scale=2.0 doubles effective range
       - quantize([2.0], E5M2, scale=2.0) should handle value as if it were 1.0

    4. test_ste_gradient_passthrough():
       - x = torch.randn(10, requires_grad=True)
       - y = quantize(x, E5M2, scale)
       - y.sum().backward()
       - x.grad should equal torch.ones_like(x) exactly

    5. test_gradient_shape_preserved():
       - Gradient has same shape as input regardless of quantization

    6. test_quantize_clamps_out_of_range():
       - Values exceeding format max clamp to max
       - quantize([1e10], E5M2, scale=1.0) returns max representable

    7. test_quantize_handles_zeros():
       - quantize([0.0], E5M2, scale) returns 0.0

    8. test_quantize_all_formats():
       - quantize works with E0M7, E1M6, E3M4, E5M2, E7M0

    9. test_dequantize_with_scale():
       - dequantize applies scale multiplication correctly

    10. test_roundtrip_gradient_flow():
        - x -> quantize -> dequantize -> loss.backward()
        - Gradient flows through entire chain

    Tests MUST fail initially (ops.py doesn't exist yet).
  </action>
  <verify>python -m pytest tests/test_ops.py -v 2>&1 | grep -E "(FAILED|ERROR|passed)" (expect failures)</verify>
  <done>Test file exists with comprehensive gradient and quantization tests, all tests fail due to missing implementation</done>
</task>

<task type="tdd">
  <name>Task 2: Implement quantize/dequantize ops to pass tests</name>
  <files>altgrad/quantization/ops.py, altgrad/quantization/__init__.py</files>
  <action>
    Create altgrad/quantization/ops.py with:

    1. QuantizeFunc(torch.autograd.Function):
       ```python
       @staticmethod
       def forward(ctx, x, format, scale):
           # Vectorized quantization using format's transfer functions
           x_scaled = x / scale
           # Apply to_bits then to_real for each element
           # Use torch.where and vectorized ops, avoid Python loops
           x_quantized = _vectorized_quantize(x_scaled, format)
           return x_quantized * scale

       @staticmethod
       def backward(ctx, grad_output):
           # STE: pass gradient unchanged
           return grad_output, None, None
       ```

    2. _vectorized_quantize(x, format) helper:
       - Implement vectorized to_bits/to_real for tensors
       - Handle sign, exponent, mantissa extraction in parallel
       - Use torch.clamp, torch.floor, torch.round
       - No Python loops over tensor elements

    3. quantize(x, format, scale) wrapper:
       - Calls QuantizeFunc.apply(x, format, scale)
       - Handles type conversion if needed

    4. DequantizeFunc(torch.autograd.Function):
       - Forward: return x * scale (simple scaling)
       - Backward: return grad_output, None (STE)

    5. dequantize(x, scale) wrapper:
       - Calls DequantizeFunc.apply(x, scale)

    Update altgrad/quantization/__init__.py:
       - Add imports: from .ops import quantize, dequantize, QuantizeFunc, DequantizeFunc
       - Update __all__

    Key implementation details:
    - Use torch.no_grad() in forward where appropriate
    - Handle edge cases: inf, nan, very small values
    - Ensure gradient is detached in backward (don't create computation graph)

    Run tests iteratively until all pass.
  </action>
  <verify>python -m pytest tests/test_ops.py -v (all tests must pass)</verify>
  <done>All ops tests pass, STE correctly passes gradients through quantization</done>
</task>

</tasks>

<verification>
```bash
# Run ops tests
python -m pytest tests/test_ops.py -v

# Verify gradient flow
python -c "
import torch
from altgrad.quantization import quantize, E5M2
x = torch.randn(10, requires_grad=True)
y = quantize(x, E5M2, torch.tensor(1.0))
y.sum().backward()
print('Gradient shape:', x.grad.shape)
print('Gradient matches ones:', torch.allclose(x.grad, torch.ones_like(x)))
"
```
</verification>

<success_criteria>
1. All ops tests pass (quantization correctness, gradient flow)
2. STE correctly passes gradients through unchanged
3. Quantize/dequantize work with all 5 FP8 formats
4. Tensor operations are vectorized (no Python loops over elements)
5. Edge cases handled: zeros, near-zero, large values, infinities
</success_criteria>

<output>
After completion, create `.planning/phases/01-quantization-engine/01-02-SUMMARY.md`
</output>
