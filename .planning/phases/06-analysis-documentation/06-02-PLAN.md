---
phase: 06-analysis-documentation
plan: 02
type: execute
wave: 2
depends_on: [06-01]
files_modified:
  - scripts/generate_reports.py
  - reports/format_comparison.md
  - reports/failure_modes.md
  - reports/manifold_comparison.md
autonomous: true

must_haves:
  truths:
    - "format_comparison.md identifies sweet-spot format per layer type (ANAL-01)"
    - "failure_modes.md documents where each format fails (ANAL-02)"
    - "manifold_comparison.md quantifies manifold-aware benefit (ANAL-03)"
    - "All reports link back to W&B runs for traceability"
  artifacts:
    - path: "scripts/generate_reports.py"
      provides: "Entry point for report generation"
      min_lines: 50
    - path: "reports/format_comparison.md"
      provides: "ANAL-01: Sweet-spot format analysis"
      contains: "Sweet-spot format"
    - path: "reports/failure_modes.md"
      provides: "ANAL-02: Failure mode documentation"
      contains: "Failure Mode"
    - path: "reports/manifold_comparison.md"
      provides: "ANAL-03: Manifold-aware comparison"
      contains: "ManifoldAdamW"
  key_links:
    - from: "scripts/generate_reports.py"
      to: "altgrad/analysis/"
      via: "import and orchestration"
      pattern: "from altgrad\\.analysis import"
    - from: "reports/*.md"
      to: "W&B runs"
      via: "hyperlinks"
      pattern: "wandb\\.ai"
---

<objective>
Generate the three analysis reports answering the core research question.

Purpose: Execute the analysis pipeline to produce ANAL-01 (format comparison), ANAL-02 (failure modes), and ANAL-03 (manifold comparison) reports from W&B experiment data.

Output: Three markdown reports in `reports/` directory plus a reusable generation script.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-analysis-documentation/06-RESEARCH.md
@.planning/phases/06-analysis-documentation/06-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create generate_reports.py entry point</name>
  <files>scripts/generate_reports.py</files>
  <action>
Create scripts/ directory and generate_reports.py entry point:

1. Create `scripts/generate_reports.py`:
   ```python
   #!/usr/bin/env python
   """Generate AltGrad analysis reports from W&B experiment data.

   This script aggregates experiment results from Phases 4-5 and generates
   three analysis reports answering the core research question.

   Reports generated:
   - format_comparison.md (ANAL-01): Sweet-spot format per layer type
   - failure_modes.md (ANAL-02): Failure mode documentation
   - manifold_comparison.md (ANAL-03): Manifold-aware vs standard comparison

   Example:
       python scripts/generate_reports.py
       python scripts/generate_reports.py --project altgrad --output-dir reports
       python scripts/generate_reports.py --offline  # Use local failure reports only
   """
   ```

2. Implement main():
   - Parse arguments: --project (default "altgrad"), --output-dir (default "reports"), --offline (flag for no W&B API calls)
   - If not offline:
     - Create ExperimentDataLoader
     - Fetch format comparison data
     - Fetch manifold comparison data
   - If offline or W&B fails gracefully:
     - Create placeholder DataFrames with note about pending experiments
     - Parse local failure reports if they exist
   - Create FormatComparator, FailureAnalyzer, ReportGenerator
   - Call each generate_* method
   - Print summary of generated reports

3. Handle W&B not configured:
   - Wrap API calls in try/except
   - If wandb.errors.CommError or similar, fall back to offline mode
   - Print warning: "W&B not configured or no runs found. Generating placeholder reports."

4. Handle missing data gracefully:
   - If no format runs found, create report noting "Experiments pending - run on RunPod"
   - If no manifold runs found, same approach
   - Reports should still be valid markdown, just with "No data yet" sections
  </action>
  <verify>
```bash
python scripts/generate_reports.py --offline
ls reports/
```
  </verify>
  <done>generate_reports.py exists and runs in offline mode without errors, creating placeholder reports.</done>
</task>

<task type="auto">
  <name>Task 2: Generate reports (online or placeholder)</name>
  <files>reports/format_comparison.md, reports/failure_modes.md, reports/manifold_comparison.md</files>
  <action>
Run the report generation script and verify outputs:

1. Run `python scripts/generate_reports.py` (will use --offline if W&B not configured)

2. Verify format_comparison.md contains:
   - Title and timestamp
   - Executive summary with sweet-spot identification (or "pending")
   - Results table (with data or "experiments pending" note)
   - Conclusions section

3. Verify failure_modes.md contains:
   - Title and timestamp
   - Failure summary table (or placeholder)
   - E7M0 section (expected failure hypothesis)
   - Prevention recommendations

4. Verify manifold_comparison.md contains:
   - Title and timestamp
   - Hypothesis statement about stiffness preconditioning
   - Comparison table (or placeholder)
   - Analysis sections

5. If W&B is configured and has runs:
   - Reports will have actual data
   - Run links will be hyperlinks to W&B

6. If offline/no W&B:
   - Reports will have structure but note experiments pending
   - Include RunPod deployment reminder:
     ```markdown
     ## Pending Experiments

     Experiments have not yet been run on RunPod. Execute:
     1. Upload code to H100 RunPod instance
     2. Run: `python experiments/run_experiment.py experiments/configs/<config>.yaml`
     3. Re-run: `python scripts/generate_reports.py`
     ```

Note: This is analysis phase - we generate the reports structure NOW. The actual data populates when experiments complete on RunPod. The reports are designed to be regenerated.
  </action>
  <verify>
```bash
# Verify all three reports exist
ls -la reports/*.md

# Verify each has content
wc -l reports/format_comparison.md reports/failure_modes.md reports/manifold_comparison.md

# Verify key sections exist
grep -l "Sweet-spot\|sweet-spot\|Executive Summary" reports/format_comparison.md
grep -l "Failure\|E7M0" reports/failure_modes.md
grep -l "ManifoldAdamW\|manifold" reports/manifold_comparison.md
```
  </verify>
  <done>All three reports exist in reports/ directory with proper structure, ready for data population after RunPod experiments.</done>
</task>

<task type="auto">
  <name>Task 3: Add report regeneration to experiment workflow</name>
  <files>experiments/run_experiment.py</files>
  <action>
Update run_experiment.py to remind about report regeneration:

1. At the end of main(), after "Training complete!", add:
   ```python
   print("\nTo update analysis reports:")
   print("  python scripts/generate_reports.py")
   ```

2. This is a light touch - just a reminder, not automatic regeneration.
   Automatic regeneration would require W&B sync which may not be immediate.

3. The workflow becomes:
   - Run experiments on RunPod
   - W&B syncs automatically
   - User runs `python scripts/generate_reports.py` to pull latest data
   - Reports update with actual experiment results
  </action>
  <verify>
```bash
grep -A2 "Training complete" experiments/run_experiment.py
```
  </verify>
  <done>run_experiment.py reminds users to regenerate reports after training completes.</done>
</task>

</tasks>

<verification>
After all tasks complete:
```bash
# Verify script runs
python scripts/generate_reports.py --offline

# Verify all three reports exist with content
for f in reports/format_comparison.md reports/failure_modes.md reports/manifold_comparison.md; do
  echo "=== $f ==="
  head -20 "$f"
  echo ""
done

# Verify experiment runner reminder
grep "generate_reports" experiments/run_experiment.py
```
</verification>

<success_criteria>
- scripts/generate_reports.py exists and runs without errors
- reports/format_comparison.md exists with ANAL-01 structure
- reports/failure_modes.md exists with ANAL-02 structure
- reports/manifold_comparison.md exists with ANAL-03 structure
- All reports are valid markdown with proper sections
- Reports can be regenerated after RunPod experiments complete
</success_criteria>

<output>
After completion, create `.planning/phases/06-analysis-documentation/06-02-SUMMARY.md`
</output>
