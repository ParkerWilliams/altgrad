---
phase: 06-analysis-documentation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - altgrad/analysis/__init__.py
  - altgrad/analysis/data_loader.py
  - altgrad/analysis/comparisons.py
  - altgrad/analysis/failure_analysis.py
  - altgrad/analysis/report_generator.py
  - pyproject.toml
autonomous: true

must_haves:
  truths:
    - "W&B Public API can fetch experiment runs by format and optimizer type"
    - "Run summaries include format, state, final_loss, best_loss, bit_stall_rate"
    - "Failure data can be parsed from crashed runs and local failure reports"
    - "Markdown reports can be generated from DataFrames"
  artifacts:
    - path: "altgrad/analysis/__init__.py"
      provides: "Analysis module exports"
      exports: ["ExperimentDataLoader", "FormatComparator", "FailureAnalyzer", "ReportGenerator"]
    - path: "altgrad/analysis/data_loader.py"
      provides: "W&B API data fetching"
      min_lines: 80
    - path: "altgrad/analysis/comparisons.py"
      provides: "Format and optimizer comparison logic"
      min_lines: 60
    - path: "altgrad/analysis/failure_analysis.py"
      provides: "Failure mode summarization from W&B and local reports"
      min_lines: 50
    - path: "altgrad/analysis/report_generator.py"
      provides: "Markdown report generation"
      min_lines: 100
  key_links:
    - from: "altgrad/analysis/data_loader.py"
      to: "wandb.Api()"
      via: "W&B Public API"
      pattern: "wandb\\.Api"
    - from: "altgrad/analysis/report_generator.py"
      to: "pandas DataFrame"
      via: "to_markdown()"
      pattern: "to_markdown"
---

<objective>
Create the analysis module for synthesizing W&B experiment data into comparison reports.

Purpose: Provide programmatic tools to aggregate experiment results from Phases 4-5, compute format comparisons, analyze failure modes, and generate markdown reports that answer the core research question.

Output: `altgrad/analysis/` module with data loading, comparison logic, failure analysis, and report generation capabilities.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-analysis-documentation/06-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create data_loader.py with W&B API integration</name>
  <files>altgrad/analysis/__init__.py, altgrad/analysis/data_loader.py, pyproject.toml</files>
  <action>
Create the analysis module directory and data loader:

1. Create `altgrad/analysis/__init__.py` with exports for all analysis components.

2. Create `altgrad/analysis/data_loader.py` with `ExperimentDataLoader` class:
   - `__init__(self, project: str = "altgrad")` - stores W&B API client and project name
   - `get_format_comparison_runs(formats: List[str] = None) -> pd.DataFrame` - fetches runs filtered by fp8_format config, returns DataFrame with columns: format, run_id, run_url, state, steps, final_loss, best_loss, bit_stall_final, overflow_final
   - `get_manifold_comparison_runs() -> pd.DataFrame` - fetches E5M2 runs, filters by use_manifold_aware config (True/False), returns DataFrame with: optimizer, manifold_aware, run_id, final_loss, best_loss, bit_stall_final
   - `get_run_history(run_id: str, keys: List[str] = None) -> pd.DataFrame` - fetches per-step metrics for a single run
   - Handle missing metrics gracefully with `.get(key, float("nan"))`
   - Filter for finished/crashed runs: `{"state": {"$in": ["finished", "crashed"]}}`

3. Update `pyproject.toml` dependencies to include: wandb, pandas, matplotlib, tabulate, scipy (in project.dependencies, not optional-dependencies).

Key patterns from research:
- Use `api.runs(path, filters)` for filtered queries
- Use `run.summary.get(key, default)` for safe metric access
- Use `run.config.get(key)` for config values
- Include run_url as `run.url` for traceability
  </action>
  <verify>
```bash
python -c "from altgrad.analysis import ExperimentDataLoader; print('Import OK')"
```
  </verify>
  <done>ExperimentDataLoader class exists with methods for format comparison and manifold comparison data fetching.</done>
</task>

<task type="auto">
  <name>Task 2: Create comparisons.py and failure_analysis.py</name>
  <files>altgrad/analysis/comparisons.py, altgrad/analysis/failure_analysis.py</files>
  <action>
Create comparison and failure analysis modules:

1. Create `altgrad/analysis/comparisons.py` with `FormatComparator` class:
   - `__init__(self, df: pd.DataFrame)` - stores comparison DataFrame
   - `rank_by_metric(metric: str, ascending: bool = True) -> pd.DataFrame` - ranks formats by given metric
   - `identify_sweet_spot(exclude_crashed: bool = True) -> str` - returns format name with best final_loss among finished runs
   - `compute_improvement(baseline_format: str, test_format: str, metric: str = "final_loss") -> float` - percentage improvement
   - `compare_by_layer_type(layer_metrics_df: pd.DataFrame) -> pd.DataFrame` - groups metrics by layer type (attention, mlp, embedding, classifier)

   Layer type patterns (from nanoGPT naming):
   - attention: contains "attn" or "c_attn" or patterns like "h.N.attn"
   - mlp: contains "mlp" or "c_fc"
   - embedding: contains "wte" or "wpe"
   - classifier: contains "lm_head"

2. Create `altgrad/analysis/failure_analysis.py` with `FailureAnalyzer` class:
   - `__init__(self)` - no state needed
   - `extract_failures(df: pd.DataFrame) -> pd.DataFrame` - filters to crashed runs only
   - `parse_local_failure_reports(checkpoint_dir: str = "checkpoints") -> List[Dict]` - parses markdown failure reports from checkpoints/*/failure_reports/*.md
   - `classify_failure_mode(reason: str, grad_sparsity: float = None) -> str` - returns one of: "overflow", "gradient_vanishing", "underflow", "unknown"
   - `merge_failure_data(wandb_failures: pd.DataFrame, local_reports: List[Dict]) -> pd.DataFrame` - combines W&B crash data with parsed local reports

   Failure classification heuristic:
   - "NaN" or "Inf" in reason -> "overflow"
   - grad_sparsity > 0.5 -> "gradient_vanishing"
   - "underflow" in reason -> "underflow"
   - else -> "unknown"
  </action>
  <verify>
```bash
python -c "from altgrad.analysis import FormatComparator, FailureAnalyzer; print('Imports OK')"
```
  </verify>
  <done>FormatComparator and FailureAnalyzer classes exist with comparison ranking and failure classification methods.</done>
</task>

<task type="auto">
  <name>Task 3: Create report_generator.py</name>
  <files>altgrad/analysis/report_generator.py</files>
  <action>
Create markdown report generator:

1. Create `altgrad/analysis/report_generator.py` with `ReportGenerator` class:
   - `__init__(self, output_dir: str = "reports")` - creates output directory if needed
   - `generate_format_comparison(df: pd.DataFrame, sweet_spot: str, output_name: str = "format_comparison.md") -> str` - generates ANAL-01 report
   - `generate_failure_modes(failure_df: pd.DataFrame, output_name: str = "failure_modes.md") -> str` - generates ANAL-02 report
   - `generate_manifold_comparison(df: pd.DataFrame, output_name: str = "manifold_comparison.md") -> str` - generates ANAL-03 report

2. Report templates (use f-strings with DataFrame.to_markdown()):

   **format_comparison.md:**
   - Title: "FP8 Format Comparison Report"
   - Generated timestamp
   - Executive summary with sweet-spot format
   - Results table: format, state, steps, final_loss, best_loss, bit_stall_final
   - Rankings by final_loss and bit_stall_rate (finished runs only)
   - Failure analysis section (crashed runs)
   - Conclusions section
   - Run links section (hyperlinks to W&B runs)

   **failure_modes.md:**
   - Title: "FP8 Failure Mode Documentation"
   - Summary table: format, collapse_step, reason, last_good_loss
   - Detailed analysis per format that failed
   - E7M0 special section (expected failure - powers-of-two only)
   - Failure mode classification table
   - Prevention recommendations

   **manifold_comparison.md:**
   - Title: "Manifold-Aware vs Standard Optimizer Comparison"
   - Hypothesis statement
   - Results table: optimizer, final_loss, best_loss, bit_stall_final
   - Statistical comparison (mean difference, improvement %)
   - Analysis sections: when manifold-aware helps, when it doesn't
   - Conclusions

3. Key implementation details:
   - Use `DataFrame.to_markdown(index=False)` for tables
   - Include generation timestamp: `datetime.now().isoformat()`
   - Handle empty DataFrames gracefully (report "No data")
   - Return absolute path to generated file
  </action>
  <verify>
```bash
python -c "from altgrad.analysis import ReportGenerator; rg = ReportGenerator(); print('ReportGenerator OK')"
ls reports/ 2>/dev/null || echo "reports dir created on first use"
```
  </verify>
  <done>ReportGenerator class exists with methods for all three required reports (ANAL-01, ANAL-02, ANAL-03).</done>
</task>

</tasks>

<verification>
After all tasks complete:
```bash
# Verify all imports
python -c "from altgrad.analysis import ExperimentDataLoader, FormatComparator, FailureAnalyzer, ReportGenerator; print('All imports OK')"

# Verify dependencies in pyproject.toml
grep -E "wandb|pandas|matplotlib|tabulate" pyproject.toml

# Verify module structure
ls altgrad/analysis/
```
</verification>

<success_criteria>
- altgrad/analysis/ module exists with all four files
- ExperimentDataLoader, FormatComparator, FailureAnalyzer, ReportGenerator all import without errors
- pyproject.toml includes wandb, pandas, matplotlib, tabulate, scipy dependencies
- ReportGenerator creates reports/ directory on init
</success_criteria>

<output>
After completion, create `.planning/phases/06-analysis-documentation/06-01-SUMMARY.md`
</output>
