---
phase: 04-custom-format-testing
plan: 03
type: execute
wave: 2
depends_on: ["04-01", "04-02"]
files_modified:
  - experiments/configs/e0m7_uniform.yaml
  - experiments/configs/e1m6_uniform.yaml
  - experiments/configs/e3m4_uniform.yaml
  - experiments/configs/e7m0_uniform.yaml
  - altgrad/training/format_runner.py
  - altgrad/training/__init__.py
  - altgrad/training/config.py
autonomous: true

must_haves:
  truths:
    - "Each format config uses same seed (42) for fair comparison"
    - "Each format config uses 500 steps for short experiment runs"
    - "Format runner stops on first NaN and saves checkpoint"
    - "Format runner integrates PartitionRelativeClipper and EmergencyMantissaShift"
    - "Format runner logs stiffness, grid alignment, and ULP metrics to W&B"
  artifacts:
    - path: "experiments/configs/e0m7_uniform.yaml"
      provides: "E0M7 format experiment config"
      contains: "fp8_format: \"E0M7\""
    - path: "experiments/configs/e1m6_uniform.yaml"
      provides: "E1M6 format experiment config"
      contains: "fp8_format: \"E1M6\""
    - path: "experiments/configs/e3m4_uniform.yaml"
      provides: "E3M4 format experiment config"
      contains: "fp8_format: \"E3M4\""
    - path: "experiments/configs/e7m0_uniform.yaml"
      provides: "E7M0 format experiment config"
      contains: "fp8_format: \"E7M0\""
    - path: "altgrad/training/format_runner.py"
      provides: "Format experiment orchestration with stability and diagnostics"
      exports: ["FormatExperimentRunner", "run_format_experiment"]
  key_links:
    - from: "altgrad/training/format_runner.py"
      to: "altgrad/quantization/stability.py"
      via: "PartitionRelativeClipper, EmergencyMantissaShift"
      pattern: "from altgrad\\.quantization\\.stability import"
    - from: "altgrad/training/format_runner.py"
      to: "altgrad/quantization/advanced_diagnostics.py"
      via: "compute_stiffness_field, ulp_statistics"
      pattern: "from altgrad\\.quantization\\.advanced_diagnostics import"
---

<objective>
Create format experiment configs and runner infrastructure for Phase 4 experiments.

Purpose: Provide ready-to-run configurations for all FP8 formats (E0M7, E1M6, E3M4, E7M0) with experiment orchestration that integrates stability interventions and advanced diagnostics.

Output:
- Four format-specific YAML configs matching existing e5m2_fp8.yaml pattern
- FormatExperimentRunner class with stability/diagnostics integration
- All infrastructure needed for H100 RunPod execution (experiments run there, not locally)
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-custom-format-testing/04-CONTEXT.md
@.planning/phases/04-custom-format-testing/04-RESEARCH.md

# From 04-01: Stability interventions
@.planning/phases/04-custom-format-testing/04-01-PLAN.md

# From 04-02: Advanced diagnostics
@.planning/phases/04-custom-format-testing/04-02-PLAN.md

# Existing patterns
@experiments/configs/e5m2_fp8.yaml
@altgrad/training/config.py
@altgrad/training/trainer.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create format-specific experiment configs</name>
  <files>
    experiments/configs/e0m7_uniform.yaml
    experiments/configs/e1m6_uniform.yaml
    experiments/configs/e3m4_uniform.yaml
    experiments/configs/e7m0_uniform.yaml
  </files>
  <action>
Create four YAML configs following e5m2_fp8.yaml pattern. All configs MUST have:
- Same architecture as baseline (n_layer: 6, n_head: 6, n_embd: 384, etc.)
- Same seed: 42 (for fair comparison)
- max_steps: 500 (short runs per CONTEXT.md)
- use_fp8: true
- use_shadow: true (FP32 comparison)
- Format-specific fp8_format and run_name

**e0m7_uniform.yaml:**
```yaml
# E0M7 Fixed-Point Experiment
# Pure fixed-point format in [-1, 1) - expected challenge: narrow range

# Model architecture (IDENTICAL to baseline)
n_layer: 6
n_head: 6
n_embd: 384
block_size: 256
vocab_size: 50257
dropout: 0.0

# Training (SHORT run for format testing)
batch_size: 12
learning_rate: 6e-4
max_steps: 500  # Short run per CONTEXT.md
warmup_steps: 100
grad_clip: 1.0

# Precision - E0M7 fixed-point
use_fp8: true
fp8_format: "E0M7"
use_shadow: true

# Checkpointing
checkpoint_interval: 100
checkpoint_dir: "checkpoints/e0m7_uniform"
max_checkpoints: 3

# Stability thresholds
nan_patience: 3  # Stop early on NaN (capture failure point)
bit_stall_threshold: 0.5
overflow_threshold: 0.01
dead_neuron_threshold: 1.0e-8
dead_neuron_window: 100

# Logging
log_interval: 1
eval_interval: 100

# W&B
project: "altgrad"
run_name: "e0m7-uniform"
tags: ["fp8", "e0m7", "eurlex", "format-test", "uniform"]

# Reproducibility
seed: 42
```

**e1m6_uniform.yaml:**
- fp8_format: "E1M6"
- run_name: "e1m6-uniform"
- checkpoint_dir: "checkpoints/e1m6_uniform"
- tags: ["fp8", "e1m6", "eurlex", "format-test", "uniform"]
- Comment: "E1M6 narrow range format - expected challenge: overflow in attention"

**e3m4_uniform.yaml:**
- fp8_format: "E3M4"
- run_name: "e3m4-uniform"
- checkpoint_dir: "checkpoints/e3m4_uniform"
- tags: ["fp8", "e3m4", "eurlex", "format-test", "uniform"]
- Comment: "E3M4 moderate range format - expected to work similar to E5M2"

**e7m0_uniform.yaml:**
- fp8_format: "E7M0"
- run_name: "e7m0-uniform"
- checkpoint_dir: "checkpoints/e7m0_uniform"
- tags: ["fp8", "e7m0", "eurlex", "format-test", "uniform"]
- Comment: "E7M0 powers-of-2 only - expected failure: near-100% bit-stall"

Commit: `feat(04-03): add format-specific experiment configs`
  </action>
  <verify>ls experiments/configs/*.yaml shows 6 config files (bf16_baseline, e5m2_fp8, and 4 new)</verify>
  <done>Four format configs created with consistent settings, ready for H100 execution</done>
</task>

<task type="auto">
  <name>Task 2: Extend TrainConfig for stability settings</name>
  <files>altgrad/training/config.py</files>
  <action>
Add new fields to TrainConfig dataclass for stability interventions:

```python
# Add to TrainConfig dataclass after existing stability thresholds:

    # Stability interventions (Phase 4)
    enable_partition_clipping: bool = False  # STAB-05: format-aware clipping
    partition_clip_base: float = 1.0  # Base clip threshold before scaling
    enable_emergency_shift: bool = False  # STAB-06: format fallback
    emergency_shift_nan_patience: int = 3  # NaN batches before shift
    emergency_shift_stall_threshold: float = 0.5  # Bit-stall rate trigger

    # Diagnostic sampling (Phase 4)
    diagnostic_interval: int = 50  # Steps between advanced diagnostics
    log_stiffness: bool = False  # Log stiffness field statistics
    log_grid_alignment: bool = False  # Log grid alignment stats
    log_ulp: bool = False  # Log ULP movement stats
```

Add these fields to the float_fields list in load_config if needed:
```python
float_fields = [
    # ... existing fields ...
    "partition_clip_base",
    "emergency_shift_stall_threshold",
]
```

Commit: `feat(04-03): extend TrainConfig with stability and diagnostic settings`
  </action>
  <verify>python -c "from altgrad.training import TrainConfig; c = TrainConfig(enable_partition_clipping=True); print(c.enable_partition_clipping)"</verify>
  <done>TrainConfig has new stability and diagnostic fields with sensible defaults</done>
</task>

<task type="auto">
  <name>Task 3: Implement FormatExperimentRunner</name>
  <files>altgrad/training/format_runner.py, altgrad/training/__init__.py</files>
  <action>
Create format_runner.py with experiment orchestration:

```python
"""Format experiment runner with stability interventions and diagnostics.

Orchestrates FP8 format experiments with:
- Partition-relative gradient clipping (STAB-05)
- Emergency mantissa shift (STAB-06)
- Advanced diagnostics (DIAG-01 to DIAG-04)
- Failure capture and checkpoint on NaN

Usage:
    >>> runner = FormatExperimentRunner(config, model, data_dir, device)
    >>> results = runner.run(max_steps=500)
"""

from __future__ import annotations

import math
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional

import torch
import torch.nn as nn

from altgrad.quantization.formats import FORMAT_REGISTRY, FP8Format
from altgrad.quantization.stability import (
    PartitionRelativeClipper,
    EmergencyMantissaShift,
)
from altgrad.quantization.advanced_diagnostics import (
    compute_stiffness_field,
    grid_alignment_statistics,
    ulp_statistics,
    gradient_stiffness_correlation,
)
from altgrad.training.config import TrainConfig


@dataclass
class ExperimentResult:
    """Results from a format experiment run."""

    format_name: str
    steps_completed: int
    final_loss: Optional[float] = None
    failure_step: Optional[int] = None
    failure_reason: Optional[str] = None
    format_shifted_to: Optional[str] = None
    shift_step: Optional[int] = None
    metrics_history: List[Dict[str, Any]] = field(default_factory=list)
    diagnostics_history: List[Dict[str, Any]] = field(default_factory=list)


class FormatExperimentRunner:
    """Runs format experiments with stability interventions and diagnostics.

    Integrates:
    - PartitionRelativeClipper for format-aware gradient clipping
    - EmergencyMantissaShift for format fallback on instability
    - Advanced diagnostics sampling at configurable intervals

    Example:
        >>> from altgrad.training import TrainConfig, FormatExperimentRunner
        >>> config = TrainConfig(use_fp8=True, fp8_format="E7M0")
        >>> runner = FormatExperimentRunner(config, model, "data/eurlex", "cuda")
        >>> result = runner.run(max_steps=500)
        >>> print(f"Completed {result.steps_completed} steps")
    """

    def __init__(
        self,
        config: TrainConfig,
        model: nn.Module,
        data_dir: str,
        device: str,
        tracker: Optional[Any] = None,  # WandbTracker
        checkpoint_manager: Optional[Any] = None,
    ):
        self.config = config
        self.model = model
        self.data_dir = data_dir
        self.device = device
        self.tracker = tracker
        self.checkpoint_manager = checkpoint_manager

        # Current format (may change via emergency shift)
        self.current_format_name = config.fp8_format
        self.current_format = FORMAT_REGISTRY.get(config.fp8_format)

        # Initialize stability interventions
        self.clipper: Optional[PartitionRelativeClipper] = None
        self.shifter: Optional[EmergencyMantissaShift] = None

        if config.enable_partition_clipping and self.current_format:
            self.clipper = PartitionRelativeClipper(
                self.current_format,
                base_clip=config.partition_clip_base,
                overflow_threshold=config.overflow_threshold,
            )

        if config.enable_emergency_shift:
            self.shifter = EmergencyMantissaShift(
                nan_patience=config.emergency_shift_nan_patience,
                stall_threshold=config.emergency_shift_stall_threshold,
            )

    def _collect_diagnostics(
        self,
        step: int,
        weights_before: Dict[str, torch.Tensor],
        weights_after: Dict[str, torch.Tensor],
        gradients: Dict[str, torch.Tensor],
    ) -> Dict[str, Any]:
        """Collect advanced diagnostics for this step."""
        if self.current_format is None:
            return {}

        diagnostics = {"step": step, "format": self.current_format_name}
        mantissa_bits = self.current_format.mantissa_bits

        # Aggregate across layers
        all_stiffness = []
        all_grid_errors = []
        all_ulp_means = []
        all_grad_ratios = []

        for name, w_after in weights_after.items():
            w_before = weights_before.get(name)
            grad = gradients.get(name)

            if w_before is None or grad is None:
                continue

            # Stiffness statistics
            if self.config.log_stiffness:
                stiffness = compute_stiffness_field(w_after, mantissa_bits)
                valid_stiffness = stiffness[~torch.isnan(stiffness)]
                if len(valid_stiffness) > 0:
                    all_stiffness.append(valid_stiffness.mean().item())

            # Grid alignment
            if self.config.log_grid_alignment:
                scale = torch.tensor(1.0, device=w_after.device)
                stats = grid_alignment_statistics(w_after, self.current_format, scale)
                all_grid_errors.append(stats["grid_error_mean"])

            # ULP movement
            if self.config.log_ulp:
                ulp_stats = ulp_statistics(w_before, w_after)
                all_ulp_means.append(ulp_stats["ulp_mean"])

            # Gradient-stiffness correlation
            corr_stats = gradient_stiffness_correlation(w_after, grad, mantissa_bits)
            all_grad_ratios.append(corr_stats["grad_below_stiffness_frac"])

        # Aggregate
        if all_stiffness:
            diagnostics["stiffness_mean"] = sum(all_stiffness) / len(all_stiffness)
        if all_grid_errors:
            diagnostics["grid_error_mean"] = sum(all_grid_errors) / len(all_grid_errors)
        if all_ulp_means:
            diagnostics["ulp_mean"] = sum(all_ulp_means) / len(all_ulp_means)
        if all_grad_ratios:
            diagnostics["grad_below_stiffness_frac"] = (
                sum(all_grad_ratios) / len(all_grad_ratios)
            )

        return diagnostics

    def _handle_format_shift(
        self,
        step: int,
        has_nan: bool,
        stall_rate: float,
        result: ExperimentResult,
    ) -> bool:
        """Check for and handle emergency format shift.

        Returns True if training should continue, False if should stop.
        """
        if self.shifter is None:
            return True

        new_format = self.shifter.check_and_shift(
            self.current_format_name, has_nan, stall_rate
        )

        if new_format is not None:
            result.format_shifted_to = new_format
            result.shift_step = step
            self.current_format_name = new_format
            self.current_format = FORMAT_REGISTRY.get(new_format)

            # Update clipper for new format
            if self.clipper and self.current_format:
                self.clipper = PartitionRelativeClipper(
                    self.current_format,
                    base_clip=self.config.partition_clip_base,
                    overflow_threshold=self.config.overflow_threshold,
                )

            # Log shift
            if self.tracker:
                self.tracker.log(
                    {
                        "format_shift": 1,
                        "new_format": new_format,
                        "shift_reason": "nan" if has_nan else "stall",
                    },
                    step=step,
                )

            return True  # Continue with new format

        # Check if we hit NaN patience without fallback option
        if has_nan and self.shifter.consecutive_nans >= self.shifter.nan_patience:
            # No fallback available (e.g., E5M2)
            return False

        return True

    def run(self, max_steps: Optional[int] = None) -> ExperimentResult:
        """Run the format experiment.

        Args:
            max_steps: Override config max_steps if provided

        Returns:
            ExperimentResult with metrics, diagnostics, and failure info
        """
        steps = max_steps or self.config.max_steps

        result = ExperimentResult(
            format_name=self.current_format_name,
            steps_completed=0,
        )

        # Note: Actual training loop integration happens in trainer.py
        # This runner provides the infrastructure; the Trainer calls
        # runner methods during training.
        #
        # Key integration points:
        # 1. After backward(): call clipper.clip_if_needed(model, overflow_rate)
        # 2. After step: call _handle_format_shift(step, has_nan, stall_rate)
        # 3. Every diagnostic_interval: call _collect_diagnostics()
        #
        # This design keeps the runner modular and trainer modifications minimal.

        return result

    def on_step_end(
        self,
        step: int,
        loss: float,
        overflow_rate: float,
        stall_rate: float,
        weights_before: Optional[Dict[str, torch.Tensor]] = None,
        weights_after: Optional[Dict[str, torch.Tensor]] = None,
        gradients: Optional[Dict[str, torch.Tensor]] = None,
        result: Optional[ExperimentResult] = None,
    ) -> tuple[bool, Optional[Dict[str, Any]]]:
        """Called after each training step.

        Args:
            step: Current step number
            loss: Training loss (may be NaN)
            overflow_rate: Fraction of overflowed values
            stall_rate: Fraction of bit-stalled updates
            weights_before: Weight snapshots before step (for diagnostics)
            weights_after: Weight snapshots after step
            gradients: Gradient snapshots
            result: ExperimentResult to update

        Returns:
            Tuple of (should_continue, diagnostics_dict)
        """
        if result is None:
            result = ExperimentResult(format_name=self.current_format_name, steps_completed=0)

        has_nan = math.isnan(loss)

        # Apply clipping if needed
        if self.clipper:
            self.clipper.clip_if_needed(self.model, overflow_rate)

        # Check for format shift
        should_continue = self._handle_format_shift(step, has_nan, stall_rate, result)

        # Collect diagnostics at interval
        diagnostics = None
        if (
            step % self.config.diagnostic_interval == 0
            and weights_before is not None
            and weights_after is not None
            and gradients is not None
        ):
            diagnostics = self._collect_diagnostics(
                step, weights_before, weights_after, gradients
            )
            result.diagnostics_history.append(diagnostics)

            # Log to tracker
            if self.tracker and diagnostics:
                self.tracker.log(diagnostics, step=step)

        result.steps_completed = step + 1
        if not has_nan:
            result.final_loss = loss
        else:
            result.failure_step = step
            result.failure_reason = "NaN loss"

        return should_continue, diagnostics


def run_format_experiment(
    config_path: str,
    data_dir: str,
    device: str = "cuda",
) -> ExperimentResult:
    """Convenience function to run format experiment from config file.

    Args:
        config_path: Path to YAML config
        data_dir: Path to training data
        device: Device to run on

    Returns:
        ExperimentResult with all metrics and diagnostics

    Example:
        >>> result = run_format_experiment(
        ...     "experiments/configs/e7m0_uniform.yaml",
        ...     "data/eurlex",
        ...     "cuda"
        ... )
    """
    from altgrad.training.config import load_config

    config = load_config(config_path)
    # Note: Full integration requires model and trainer setup
    # This is a placeholder for the experiment script pattern
    raise NotImplementedError(
        "Full experiment runner requires trainer integration. "
        "Use FormatExperimentRunner with existing Trainer."
    )


__all__ = [
    "ExperimentResult",
    "FormatExperimentRunner",
    "run_format_experiment",
]
```

Update altgrad/training/__init__.py to export FormatExperimentRunner and ExperimentResult.

Commit: `feat(04-03): implement FormatExperimentRunner with stability and diagnostics`
  </action>
  <verify>python -c "from altgrad.training import FormatExperimentRunner, ExperimentResult; print('OK')"</verify>
  <done>FormatExperimentRunner provides modular stability/diagnostics infrastructure for trainer integration</done>
</task>

</tasks>

<verification>
```bash
# All configs exist
ls -la experiments/configs/*.yaml

# Configs have correct format settings
grep "fp8_format" experiments/configs/e*.yaml

# All configs use seed 42
grep "seed: 42" experiments/configs/e*.yaml

# TrainConfig has new fields
python -c "
from altgrad.training import TrainConfig
c = TrainConfig(
    enable_partition_clipping=True,
    enable_emergency_shift=True,
    log_stiffness=True,
)
print(f'Partition clipping: {c.enable_partition_clipping}')
print(f'Emergency shift: {c.enable_emergency_shift}')
print(f'Log stiffness: {c.log_stiffness}')
"

# FormatExperimentRunner imports correctly
python -c "
from altgrad.training import FormatExperimentRunner, ExperimentResult
from altgrad.training import TrainConfig
import torch.nn as nn

config = TrainConfig(
    use_fp8=True,
    fp8_format='E7M0',
    enable_partition_clipping=True,
    enable_emergency_shift=True,
)
model = nn.Linear(10, 10)
runner = FormatExperimentRunner(config, model, 'data/eurlex', 'cpu')
print(f'Runner format: {runner.current_format_name}')
print(f'Clipper initialized: {runner.clipper is not None}')
print(f'Shifter initialized: {runner.shifter is not None}')
"
```
</verification>

<success_criteria>
1. Four format configs created (e0m7, e1m6, e3m4, e7m0) all with seed 42, 500 steps
2. TrainConfig extended with stability and diagnostic settings
3. FormatExperimentRunner integrates PartitionRelativeClipper and EmergencyMantissaShift
4. FormatExperimentRunner collects and logs advanced diagnostics
5. All imports work cleanly from altgrad.training
6. Infrastructure ready for H100 RunPod experiment execution
</success_criteria>

<output>
After completion, create `.planning/phases/04-custom-format-testing/04-03-SUMMARY.md`
</output>
