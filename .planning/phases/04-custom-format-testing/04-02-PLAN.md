---
phase: 04-custom-format-testing
plan: 02
type: tdd
wave: 1
depends_on: []
files_modified:
  - altgrad/quantization/advanced_diagnostics.py
  - altgrad/quantization/__init__.py
  - tests/test_advanced_diagnostics.py
autonomous: true

must_haves:
  truths:
    - "Stiffness S = 2^(floor(log2|w|) - M) computes correctly for each weight"
    - "E0M7 stiffness is constant 1/128 (uniform grid spacing)"
    - "Zero weights return NaN stiffness (undefined)"
    - "Grid alignment measures |w - quantize(w)|"
    - "ULP distance uses torch.nextafter for IEEE 754 compliance"
    - "Gradient-stiffness correlation computes Pearson coefficient"
  artifacts:
    - path: "altgrad/quantization/advanced_diagnostics.py"
      provides: "compute_stiffness_field, grid_alignment_error, compute_ulp_distance, gradient_stiffness_correlation"
      exports: ["compute_stiffness_field", "grid_alignment_statistics", "compute_ulp_distance", "ulp_statistics", "gradient_stiffness_correlation"]
    - path: "tests/test_advanced_diagnostics.py"
      provides: "TDD tests for DIAG-01 to DIAG-04"
      min_lines: 150
  key_links:
    - from: "altgrad/quantization/advanced_diagnostics.py"
      to: "altgrad/quantization/ops.py"
      via: "quantize function for grid alignment"
      pattern: "from altgrad\\.quantization\\.ops import quantize"
    - from: "altgrad/quantization/advanced_diagnostics.py"
      to: "torch.nextafter"
      via: "ULP computation"
      pattern: "torch\\.nextafter"
---

<objective>
Implement advanced diagnostics (DIAG-01 to DIAG-04) for quantization analysis.

Purpose: Provide deep insight into how weights interact with quantization grids, enabling scientific understanding of why formats succeed or fail.

Output:
- DIAG-01: Stiffness field computation (S = 2^(floor(log2|w|) - M))
- DIAG-02: Grid alignment measurement (distance to nearest FP8 value)
- DIAG-03: Gradient-stiffness correlation analysis
- DIAG-04: ULP statistics (how many bit-positions each update moves)
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-custom-format-testing/04-RESEARCH.md

# Existing code patterns
@altgrad/quantization/formats.py
@altgrad/quantization/ops.py
@altgrad/quantization/diagnostics.py
</context>

<tasks>

<task type="auto">
  <name>Task 1 (RED): Write failing tests for advanced diagnostics</name>
  <files>tests/test_advanced_diagnostics.py</files>
  <action>
Create comprehensive tests for DIAG-01 through DIAG-04:

**DIAG-01: Stiffness Field tests:**
- `test_stiffness_formula_basic`: For w=1.0, M=2 (E5M2): S = 2^(0-2) = 0.25
- `test_stiffness_scales_with_magnitude`: Larger |w| -> larger S (less precision)
- `test_stiffness_zero_weight_is_nan`: Zero weights have undefined stiffness
- `test_stiffness_e0m7_constant`: E0M7 (M=7) should return constant 1/128 regardless of weight value
- `test_stiffness_negative_weights`: Sign doesn't affect stiffness (use abs)
- `test_stiffness_tensor_batch`: Works on full tensor, returns same shape

**DIAG-02: Grid Alignment tests:**
- `test_grid_alignment_on_grid_zero`: Value exactly on grid has zero error
- `test_grid_alignment_between_grid_points`: Mid-point has positive error
- `test_grid_alignment_statistics_returns_dict`: Returns mean, max, std, on_grid_frac
- `test_grid_alignment_clamped_values`: Values outside range clamp to boundary

**DIAG-03: Gradient-Stiffness Correlation tests:**
- `test_correlation_returns_pearson`: Returns correlation coefficient in [-1, 1]
- `test_correlation_positive_when_aligned`: High gradients at high stiffness -> positive
- `test_correlation_handles_zero_weights`: Masks out zero-weight positions
- `test_correlation_ratio_below_one_means_stall`: grad/stiffness < 1 indicates likely stall
- `test_grad_below_stiffness_frac_computed`: Returns fraction of gradients below stiffness

**DIAG-04: ULP Statistics tests:**
- `test_ulp_distance_identical_weights`: Before==After -> distance 0
- `test_ulp_distance_one_ulp_move`: Moving by exactly one ULP -> distance 1
- `test_ulp_statistics_returns_dict`: Returns mean, median, max, zero_frac
- `test_ulp_zero_frac_matches_stall`: Zero ULP movement = bit stall
- `test_ulp_uses_nextafter`: Implementation uses torch.nextafter (not manual)

Run: `pytest tests/test_advanced_diagnostics.py -v` - all tests MUST FAIL

Commit: `test(04-02): add failing tests for advanced diagnostics`
  </action>
  <verify>pytest tests/test_advanced_diagnostics.py -v shows all tests fail with ImportError</verify>
  <done>18+ tests written covering DIAG-01 to DIAG-04, all failing</done>
</task>

<task type="auto">
  <name>Task 2 (GREEN): Implement stiffness field and grid alignment (DIAG-01, DIAG-02)</name>
  <files>altgrad/quantization/advanced_diagnostics.py, altgrad/quantization/__init__.py</files>
  <action>
Create advanced_diagnostics.py with stiffness and grid alignment functions:

```python
"""Advanced quantization diagnostics for format analysis.

DIAG-01: Stiffness field - minimum meaningful update size at each weight magnitude
DIAG-02: Grid alignment - distance from weights to nearest quantization levels
DIAG-03: Gradient-stiffness correlation - alignment between gradients and precision
DIAG-04: ULP statistics - bit-position movement per update

These diagnostics enable scientific analysis of format suitability.
"""

from __future__ import annotations

from typing import Dict

import torch
from torch import Tensor

from altgrad.quantization.formats import FP8Format
from altgrad.quantization.ops import quantize


def compute_stiffness_field(weights: Tensor, mantissa_bits: int) -> Tensor:
    """Compute per-weight stiffness factor (DIAG-01).

    Stiffness S = 2^(floor(log2|w|) - M) represents the minimum
    meaningful update size at each weight's magnitude. Updates smaller
    than S will round to zero (bit-stall).

    Special case: E0M7 (mantissa_bits=7, fixed-point) has constant
    stiffness of 1/128 since the grid is uniformly spaced.

    Args:
        weights: Weight tensor
        mantissa_bits: Number of mantissa bits in format (M)

    Returns:
        Tensor of stiffness values, same shape as weights.
        Zero weights return NaN (undefined stiffness).

    Example:
        >>> w = torch.tensor([1.0, 0.5, 0.25])
        >>> S = compute_stiffness_field(w, mantissa_bits=2)  # E5M2
        >>> # S[0] = 2^(0-2) = 0.25, S[1] = 2^(-1-2) = 0.125, ...
    """
    # E0M7 special case: fixed-point with uniform grid spacing
    if mantissa_bits == 7:
        # Constant stiffness = 1/128 for all non-zero weights
        stiffness = torch.full_like(weights, 1.0 / 128.0)
        stiffness = torch.where(weights == 0, torch.tensor(float('nan')), stiffness)
        return stiffness

    # Standard floating-point: S = 2^(floor(log2|w|) - M)
    abs_w = weights.abs()
    # Handle zero weights: set to small positive to avoid log2(0)
    safe_abs_w = abs_w.clamp(min=1e-45)
    log2_w = torch.floor(torch.log2(safe_abs_w))

    stiffness = torch.pow(2.0, log2_w - mantissa_bits)

    # Zero weights have undefined stiffness
    stiffness = torch.where(weights == 0, torch.tensor(float('nan')), stiffness)

    return stiffness


def grid_alignment_error(
    weights: Tensor,
    format: FP8Format,
    scale: Tensor
) -> Tensor:
    """Compute distance to nearest quantization grid point (DIAG-02).

    Measures how far each weight is from its nearest representable value.
    Weights exactly on the grid have zero error.

    Args:
        weights: Weight tensor
        format: FP8 format specification
        scale: Scale factor for quantization

    Returns:
        Tensor of absolute errors, same shape as weights.

    Example:
        >>> from altgrad.quantization import E5M2
        >>> error = grid_alignment_error(weights, E5M2, torch.tensor(1.0))
    """
    quantized = quantize(weights, format, scale)
    return torch.abs(weights - quantized)


def grid_alignment_statistics(
    weights: Tensor,
    format: FP8Format,
    scale: Tensor
) -> Dict[str, float]:
    """Return summary statistics for grid alignment (DIAG-02).

    Args:
        weights: Weight tensor
        format: FP8 format specification
        scale: Scale factor for quantization

    Returns:
        Dictionary with keys:
          - grid_error_mean: Mean alignment error
          - grid_error_max: Maximum alignment error
          - grid_error_std: Standard deviation of error
          - on_grid_frac: Fraction of weights exactly on grid
    """
    error = grid_alignment_error(weights, format, scale)
    return {
        "grid_error_mean": error.mean().item(),
        "grid_error_max": error.max().item(),
        "grid_error_std": error.std().item(),
        "on_grid_frac": (error < 1e-10).float().mean().item(),
    }
```

Update altgrad/quantization/__init__.py to export stiffness and grid functions.

Run: `pytest tests/test_advanced_diagnostics.py -k "stiffness or grid" -v`

Commit: `feat(04-02): implement stiffness field and grid alignment diagnostics`
  </action>
  <verify>pytest tests/test_advanced_diagnostics.py -k "stiffness or grid" -v shows all stiffness/grid tests pass</verify>
  <done>DIAG-01 and DIAG-02 implemented, 10 tests passing</done>
</task>

<task type="auto">
  <name>Task 3 (GREEN): Implement ULP and correlation diagnostics (DIAG-03, DIAG-04)</name>
  <files>altgrad/quantization/advanced_diagnostics.py, altgrad/quantization/__init__.py</files>
  <action>
Add ULP and correlation functions to advanced_diagnostics.py:

```python
def compute_ulp_distance(before: Tensor, after: Tensor) -> Tensor:
    """Compute how many ULPs each weight moved (DIAG-04).

    ULP (Unit in Last Place) is the distance to the next representable value.
    This measures updates in terms of bit-positions, not real value.

    Uses torch.nextafter for IEEE 754 compliant ULP computation.

    Args:
        before: Weights before update
        after: Weights after update

    Returns:
        Tensor of ULP distances (0 = no movement = bit-stall)

    Example:
        >>> before = torch.tensor([1.0, 2.0])
        >>> after = torch.tensor([1.0, 2.5])
        >>> dist = compute_ulp_distance(before, after)
    """
    # ULP at each position = distance to next representable value
    inf_tensor = torch.full_like(before, float('inf'))
    ulp = torch.abs(torch.nextafter(before, inf_tensor) - before)

    # Avoid division by zero for very small values
    safe_ulp = ulp.clamp(min=1e-45)

    # Distance in ULP units
    distance = torch.abs(after - before) / safe_ulp
    return distance


def ulp_statistics(before: Tensor, after: Tensor) -> Dict[str, float]:
    """Return ULP movement statistics (DIAG-04).

    Args:
        before: Weights before update
        after: Weights after update

    Returns:
        Dictionary with keys:
          - ulp_mean: Mean ULP movement
          - ulp_median: Median ULP movement
          - ulp_max: Maximum ULP movement
          - ulp_zero_frac: Fraction with zero movement (stalled)
    """
    ulp_dist = compute_ulp_distance(before, after)
    return {
        "ulp_mean": ulp_dist.mean().item(),
        "ulp_median": ulp_dist.median().item(),
        "ulp_max": ulp_dist.max().item(),
        "ulp_zero_frac": (ulp_dist == 0).float().mean().item(),
    }


def gradient_stiffness_correlation(
    weights: Tensor,
    gradients: Tensor,
    mantissa_bits: int
) -> Dict[str, float]:
    """Analyze correlation between gradient magnitude and stiffness (DIAG-03).

    High positive correlation indicates large gradients appear where
    quantization is coarse (large stiffness). This is problematic.

    The grad/stiffness ratio indicates update effectiveness:
    - ratio > 1: gradient large enough to change quantized value
    - ratio < 1: gradient likely too small (bit-stall risk)

    Args:
        weights: Weight tensor
        gradients: Gradient tensor (same shape)
        mantissa_bits: Number of mantissa bits in format

    Returns:
        Dictionary with keys:
          - grad_stiff_correlation: Pearson correlation [-1, 1]
          - grad_stiff_ratio_mean: Mean of gradient/stiffness ratio
          - grad_below_stiffness_frac: Fraction where |grad| < stiffness
    """
    stiffness = compute_stiffness_field(weights, mantissa_bits)
    grad_mag = gradients.abs()

    # Flatten and remove NaN stiffness (zero weights)
    valid_mask = ~torch.isnan(stiffness)
    s_flat = stiffness[valid_mask].flatten()
    g_flat = grad_mag[valid_mask].flatten()

    if len(s_flat) == 0:
        return {
            "grad_stiff_correlation": 0.0,
            "grad_stiff_ratio_mean": 0.0,
            "grad_below_stiffness_frac": 0.0,
        }

    # Compute Pearson correlation
    s_centered = s_flat - s_flat.mean()
    g_centered = g_flat - g_flat.mean()

    numerator = (s_centered * g_centered).sum()
    denominator = s_centered.norm() * g_centered.norm() + 1e-10
    correlation = numerator / denominator

    # Ratio of gradient to stiffness
    ratio = g_flat / s_flat.clamp(min=1e-10)

    return {
        "grad_stiff_correlation": correlation.item(),
        "grad_stiff_ratio_mean": ratio.mean().item(),
        "grad_below_stiffness_frac": (g_flat < s_flat).float().mean().item(),
    }


__all__ = [
    "compute_stiffness_field",
    "grid_alignment_error",
    "grid_alignment_statistics",
    "compute_ulp_distance",
    "ulp_statistics",
    "gradient_stiffness_correlation",
]
```

Update altgrad/quantization/__init__.py exports.

Run: `pytest tests/test_advanced_diagnostics.py -v`

Commit: `feat(04-02): implement ULP and gradient-stiffness correlation diagnostics`
  </action>
  <verify>pytest tests/test_advanced_diagnostics.py -v shows all 18+ tests pass</verify>
  <done>All four DIAG requirements implemented, all tests passing</done>
</task>

</tasks>

<verification>
```bash
# All diagnostic tests pass
pytest tests/test_advanced_diagnostics.py -v

# Imports work from package
python -c "
from altgrad.quantization import (
    compute_stiffness_field,
    grid_alignment_statistics,
    compute_ulp_distance,
    ulp_statistics,
    gradient_stiffness_correlation
)
print('All imports OK')
"

# Quick sanity check
python -c "
import torch
from altgrad.quantization import (
    compute_stiffness_field,
    grid_alignment_statistics,
    compute_ulp_distance,
    gradient_stiffness_correlation,
    E5M2
)

# Stiffness
w = torch.tensor([1.0, 0.5, 0.25, 0.0])
S = compute_stiffness_field(w, mantissa_bits=2)
print(f'Stiffness for w=1.0 with M=2: {S[0].item():.4f} (expected 0.25)')
print(f'Stiffness for w=0.0: {S[3].item()} (expected nan)')

# Grid alignment
scale = torch.tensor(1.0)
stats = grid_alignment_statistics(w[:3], E5M2, scale)
print(f'Grid alignment mean error: {stats[\"grid_error_mean\"]:.6f}')

# ULP
before = torch.tensor([1.0, 2.0])
after = torch.tensor([1.0, 2.5])
ulp_dist = compute_ulp_distance(before, after)
print(f'ULP distance for unchanged: {ulp_dist[0].item():.0f} (expected 0)')

# Correlation
grad = torch.randn(100) * 0.01
weights = torch.randn(100)
corr = gradient_stiffness_correlation(weights, grad, mantissa_bits=2)
print(f'Grad-stiffness correlation: {corr[\"grad_stiff_correlation\"]:.4f}')
"
```
</verification>

<success_criteria>
1. Stiffness formula S = 2^(floor(log2|w|) - M) implemented correctly
2. E0M7 returns constant 1/128 stiffness
3. Zero weights return NaN stiffness
4. Grid alignment uses quantize() and measures |w - quantize(w)|
5. ULP computation uses torch.nextafter (IEEE 754 compliant)
6. Gradient-stiffness correlation returns Pearson coefficient in [-1, 1]
7. grad_below_stiffness_frac identifies bit-stall risk
8. All 18+ tests pass, clean exports from altgrad.quantization
</success_criteria>

<output>
After completion, create `.planning/phases/04-custom-format-testing/04-02-SUMMARY.md`
</output>
