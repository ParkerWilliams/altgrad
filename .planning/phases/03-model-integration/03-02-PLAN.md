---
phase: 03-model-integration
plan: 02
type: tdd
wave: 2
depends_on: ["03-01"]
files_modified:
  - altgrad/integration/config.py
  - altgrad/integration/surgery.py
  - altgrad/integration/__init__.py
  - altgrad/utils/reproducibility.py
  - altgrad/utils/__init__.py
  - tests/test_integration.py
  - tests/test_reproducibility.py
autonomous: true

must_haves:
  truths:
    - "QuantizationConfig specifies per-layer FP8 formats via pattern matching (attention BF16, MLP FP8)"
    - "quantize_model() respects QuantizationConfig to apply different formats per layer"
    - "Layers matching None format remain as nn.Linear (BF16 precision)"
    - "set_seed_for_reproducibility() sets all RNG sources (Python, NumPy, PyTorch, CUDA, cuDNN)"
    - "Format ablation runs with same seed produce identical initial states before quantization diverges"
  artifacts:
    - path: "altgrad/integration/config.py"
      provides: "Per-layer precision configuration"
      exports: ["LayerPrecisionRule", "QuantizationConfig"]
      min_lines: 80
    - path: "altgrad/utils/reproducibility.py"
      provides: "Comprehensive seed setup for ablations"
      exports: ["set_seed_for_reproducibility", "seed_worker", "create_reproducible_dataloader"]
      min_lines: 50
    - path: "tests/test_reproducibility.py"
      provides: "Reproducibility verification tests"
      min_lines: 60
  key_links:
    - from: "altgrad/integration/config.py"
      to: "altgrad/quantization/formats.py"
      via: "FORMAT_REGISTRY lookup"
      pattern: "FORMAT_REGISTRY"
    - from: "altgrad/integration/surgery.py"
      to: "altgrad/integration/config.py"
      via: "QuantizationConfig.get_format_for_layer"
      pattern: "config.get_format_for_layer"
    - from: "altgrad/utils/reproducibility.py"
      to: "torch.backends.cudnn"
      via: "deterministic settings"
      pattern: "torch.backends.cudnn.deterministic"
---

<objective>
Implement per-layer mixed precision configuration and comprehensive seed setup for reproducible ablation runs.

Purpose: Enable fine-grained control over which layers use which FP8 format (or remain in BF16), and ensure ablation runs produce identical random sequences so the only variable is the quantization format.

Output: `altgrad/integration/config.py` with LayerPrecisionRule/QuantizationConfig, updated surgery to use config, and `altgrad/utils/reproducibility.py` for seed management.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-model-integration/03-RESEARCH.md

# From Plan 01 (dependency)
# - QuantizedLinear wrapper implemented
# - quantize_model() with skip_patterns implemented
# - Surgery functions working on GPT model

# GPT model layer naming convention (for pattern matching):
# transformer.wte - token embedding
# transformer.wpe - position embedding
# transformer.h.0.ln_1 - LayerNorm
# transformer.h.0.attn.c_attn - QKV projection (attention)
# transformer.h.0.attn.c_proj - attention output projection
# transformer.h.0.ln_2 - LayerNorm
# transformer.h.0.mlp.c_fc - MLP up projection
# transformer.h.0.mlp.c_proj - MLP down projection
# transformer.ln_f - final LayerNorm
# lm_head - language model head (weight-tied with wte)

# Pattern examples:
# r"\.attn\." matches attention layers
# r"\.mlp\." matches MLP layers
# r"lm_head" matches output head
</context>

<tasks>

<task type="tdd">
  <name>Task 1: Write failing tests for QuantizationConfig and reproducibility</name>
  <files>tests/test_integration.py, tests/test_reproducibility.py</files>
  <action>
    Add tests to tests/test_integration.py for QuantizationConfig:

    **Config tests:**
    1. test_layer_precision_rule_matches_pattern():
       - Create rule with pattern=r"\.attn\.", format="E5M2"
       - Assert matches "transformer.h.0.attn.c_attn"
       - Assert doesn't match "transformer.h.0.mlp.c_fc"

    2. test_quantization_config_get_format_for_layer():
       - Create config with rules: attn -> None (BF16), mlp -> E5M2
       - Assert get_format_for_layer("transformer.h.0.attn.c_attn") is None
       - Assert get_format_for_layer("transformer.h.0.mlp.c_fc") == E5M2

    3. test_quantization_config_default_format():
       - Create config with default_format="E3M4", no rules
       - Assert get_format_for_layer("any.layer.name") == E3M4

    4. test_quantization_config_first_match_wins():
       - Create config with rules: c_proj -> E3M4, mlp -> E5M2 (overlapping)
       - Assert get_format_for_layer("mlp.c_proj") == E3M4 (first rule wins)

    5. test_quantize_model_with_config_mixed_precision():
       - Create GPT model
       - Create config: attn -> None (BF16), mlp -> E5M2, lm_head -> None
       - Call quantize_model(model, config=config)
       - Assert attn layers are still nn.Linear
       - Assert mlp layers are QuantizedLinear with E5M2

    6. test_quantize_model_config_different_formats():
       - Create config: mlp.c_fc -> E5M2, mlp.c_proj -> E3M4
       - Verify different layers get different formats

    Create tests/test_reproducibility.py:

    **Reproducibility tests:**
    7. test_set_seed_produces_same_torch_randn():
       - Call set_seed_for_reproducibility(42)
       - Generate x1 = torch.randn(100)
       - Call set_seed_for_reproducibility(42) again
       - Generate x2 = torch.randn(100)
       - Assert torch.allclose(x1, x2)

    8. test_set_seed_produces_same_numpy_random():
       - Same pattern with np.random.randn

    9. test_set_seed_produces_same_python_random():
       - Same pattern with random.random()

    10. test_different_seeds_produce_different_results():
        - set_seed(42) -> x1
        - set_seed(43) -> x2
        - Assert not torch.allclose(x1, x2)

    11. test_gpt_model_init_reproducible():
        - set_seed(42) -> create GPT -> weights1
        - set_seed(42) -> create GPT -> weights2
        - Assert torch.allclose(weights1, weights2)

    12. test_ablation_same_seed_different_format():
        - set_seed(42) -> create GPT -> quantize with E5M2 -> run forward
        - set_seed(42) -> create GPT -> quantize with E3M4 -> run forward
        - Initial weights identical (before forward)
        - Outputs differ only due to quantization (not random variation)

    Tests MUST fail initially (config.py and reproducibility.py don't exist yet).
  </action>
  <verify>cd /Users/prwilliams/Repos/altgrad && source .venv/bin/activate && python -m pytest tests/test_integration.py tests/test_reproducibility.py -v -k "config or reproducib" 2>&1 | grep -E "(FAILED|ERROR|ModuleNotFoundError)" (expect failures)</verify>
  <done>Tests for config and reproducibility exist, all fail due to missing implementation</done>
</task>

<task type="tdd">
  <name>Task 2: Implement QuantizationConfig and update surgery</name>
  <files>altgrad/integration/config.py, altgrad/integration/surgery.py, altgrad/integration/__init__.py</files>
  <action>
    Create altgrad/integration/config.py:

    ```python
    """Per-layer precision configuration for mixed-precision quantization.

    Provides LayerPrecisionRule and QuantizationConfig for specifying
    which layers use which FP8 format (or remain in BF16).
    """

    from dataclasses import dataclass, field
    import re
    from typing import Dict, List, Optional

    from altgrad.quantization import FP8Format, FORMAT_REGISTRY


    @dataclass
    class LayerPrecisionRule:
        """Rule for matching layer names to FP8 formats.

        Args:
            pattern: Regex pattern for matching layer names
            format: FP8 format name (e.g., "E5M2") or None for BF16

        Example:
            >>> rule = LayerPrecisionRule(r"\\.attn\\.", None)  # Attention in BF16
            >>> rule = LayerPrecisionRule(r"\\.mlp\\.", "E5M2")  # MLP in FP8
        """
        pattern: str
        format: Optional[str]

        def matches(self, layer_name: str) -> bool:
            """Check if layer name matches this rule's pattern."""
            return re.search(self.pattern, layer_name) is not None

        def get_format(self) -> Optional[FP8Format]:
            """Get the FP8Format for this rule, or None for BF16."""
            if self.format is None:
                return None
            return FORMAT_REGISTRY[self.format]


    @dataclass
    class QuantizationConfig:
        """Configuration for model quantization with per-layer precision.

        Attributes:
            default_format: Default FP8 format for unmatched layers (None = BF16)
            layer_rules: Ordered list of rules (first match wins)
            history_len: Amax history length for dynamic scaling
            quantize_input: Whether to quantize input activations

        Example:
            >>> config = QuantizationConfig(
            ...     default_format="E5M2",
            ...     layer_rules=[
            ...         LayerPrecisionRule(r"\\.attn\\.", None),     # Attention BF16
            ...         LayerPrecisionRule(r"\\.mlp\\.", "E5M2"),    # MLP FP8
            ...         LayerPrecisionRule(r"lm_head", None),        # LM head BF16
            ...     ],
            ... )
        """
        default_format: Optional[str] = "E5M2"
        layer_rules: List[LayerPrecisionRule] = field(default_factory=list)
        history_len: int = 16
        quantize_input: bool = False

        def get_format_for_layer(self, layer_name: str) -> Optional[FP8Format]:
            """Get FP8 format for a layer based on rules.

            First matching rule wins. Returns None if layer should stay in BF16.

            Args:
                layer_name: Full layer name (e.g., "transformer.h.0.attn.c_attn")

            Returns:
                FP8Format for quantization, or None for BF16
            """
            for rule in self.layer_rules:
                if rule.matches(layer_name):
                    return rule.get_format()

            # Use default format
            if self.default_format is None:
                return None
            return FORMAT_REGISTRY[self.default_format]


    def create_mixed_precision_config(
        attention_format: Optional[str] = None,
        mlp_format: str = "E5M2",
        lm_head_format: Optional[str] = None,
        default_format: str = "E5M2",
        history_len: int = 16,
    ) -> QuantizationConfig:
        """Create a standard mixed-precision config for GPT models.

        Args:
            attention_format: Format for attention layers (None = BF16)
            mlp_format: Format for MLP layers
            lm_head_format: Format for LM head (None = BF16, recommended)
            default_format: Default format for unmatched layers
            history_len: Amax history length

        Returns:
            QuantizationConfig with typical GPT layer rules
        """
        rules = [
            LayerPrecisionRule(r"\.attn\.", attention_format),
            LayerPrecisionRule(r"\.c_attn", attention_format),
            LayerPrecisionRule(r"\.mlp\.", mlp_format),
            LayerPrecisionRule(r"lm_head", lm_head_format),
        ]
        return QuantizationConfig(
            default_format=default_format,
            layer_rules=rules,
            history_len=history_len,
        )
    ```

    Update altgrad/integration/surgery.py to support QuantizationConfig:

    Add a new overload to quantize_model:

    ```python
    from typing import List, Optional, Union
    from altgrad.quantization import FP8Format
    from altgrad.integration.config import QuantizationConfig

    def quantize_model(
        model: nn.Module,
        fp8_format: Optional[FP8Format] = None,
        config: Optional[QuantizationConfig] = None,
        skip_patterns: Optional[List[str]] = None,
        history_len: int = 16,
        quantize_input: bool = False,
        inplace: bool = True,
    ) -> nn.Module:
        """Replace nn.Linear layers with QuantizedLinear wrappers.

        Can be called with either:
        1. fp8_format - Apply same format to all layers (except skip_patterns)
        2. config - Apply per-layer formats based on QuantizationConfig rules

        Args:
            model: Model to quantize
            fp8_format: Single FP8 format for all layers (mutually exclusive with config)
            config: QuantizationConfig for per-layer precision (mutually exclusive with fp8_format)
            skip_patterns: Layer name patterns to skip (only used with fp8_format)
            history_len: Amax history length (only used with fp8_format)
            quantize_input: Whether to quantize inputs (only used with fp8_format)
            inplace: Modify model in place (False creates deepcopy)

        Returns:
            Model with Linear layers replaced by QuantizedLinear

        Raises:
            ValueError: If neither or both fp8_format and config are provided
        """
        if (fp8_format is None) == (config is None):
            raise ValueError("Provide exactly one of fp8_format or config")

        if not inplace:
            model = copy.deepcopy(model)

        # Collect replacements first (avoid mutating during iteration)
        replacements = []

        for name, module in model.named_modules():
            if isinstance(module, nn.Linear):
                if config is not None:
                    # Use config for per-layer format
                    layer_format = config.get_format_for_layer(name)
                    if layer_format is not None:
                        replacements.append((name, module, layer_format, config.history_len, config.quantize_input))
                else:
                    # Use single format with skip patterns
                    skip_patterns = skip_patterns or []
                    should_skip = any(pattern in name for pattern in skip_patterns)
                    if not should_skip:
                        replacements.append((name, module, fp8_format, history_len, quantize_input))

        # Apply replacements
        for name, module, layer_format, hist_len, quant_input in replacements:
            q_module = QuantizedLinear(module, layer_format, hist_len, quant_input)
            _set_module_by_name(model, name, q_module)

        return model
    ```

    Update altgrad/integration/__init__.py:
    ```python
    from altgrad.integration.wrapper import QuantizedLinear
    from altgrad.integration.surgery import quantize_model, dequantize_model
    from altgrad.integration.config import (
        LayerPrecisionRule,
        QuantizationConfig,
        create_mixed_precision_config,
    )

    __all__ = [
        "QuantizedLinear",
        "quantize_model",
        "dequantize_model",
        "LayerPrecisionRule",
        "QuantizationConfig",
        "create_mixed_precision_config",
    ]
    ```

    Run config tests to verify.
  </action>
  <verify>cd /Users/prwilliams/Repos/altgrad && source .venv/bin/activate && python -m pytest tests/test_integration.py -v -k "config" (config tests should pass)</verify>
  <done>QuantizationConfig tests pass, per-layer mixed precision verified on GPT model</done>
</task>

<task type="tdd">
  <name>Task 3: Implement reproducibility utilities</name>
  <files>altgrad/utils/__init__.py, altgrad/utils/reproducibility.py</files>
  <action>
    Create directory: mkdir -p altgrad/utils

    Create altgrad/utils/reproducibility.py:

    ```python
    """Comprehensive seed setup for reproducible ablation experiments.

    Provides utilities for setting all random number generator sources
    to ensure reproducible experiments where the only variable is the
    quantization format.

    Based on PyTorch Reproducibility Guide:
    https://pytorch.org/docs/stable/notes/randomness.html
    """

    import os
    import random
    from typing import Optional

    import numpy as np
    import torch
    from torch.utils.data import DataLoader, Dataset


    def set_seed_for_reproducibility(seed: int) -> None:
        """Set all random seeds for reproducible experiments.

        Sets seeds for:
        - Python's random module
        - NumPy's random generator
        - PyTorch's random generator (CPU and CUDA)
        - cuDNN deterministic mode
        - CUBLAS workspace config (for CUDA >= 10.2)

        Args:
            seed: Random seed to use

        Example:
            >>> set_seed_for_reproducibility(42)
            >>> x1 = torch.randn(10)
            >>> set_seed_for_reproducibility(42)
            >>> x2 = torch.randn(10)
            >>> assert torch.allclose(x1, x2)
        """
        # Python random
        random.seed(seed)

        # NumPy
        np.random.seed(seed)

        # PyTorch CPU
        torch.manual_seed(seed)

        # PyTorch CUDA (all devices)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(seed)

        # cuDNN deterministic mode
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

        # CUBLAS workspace config (for reproducibility on CUDA >= 10.2)
        os.environ["CUBLAS_WORKSPACE_CONFIG"] = ":4096:8"

        # PyTorch 1.8+ use_deterministic_algorithms
        if hasattr(torch, "use_deterministic_algorithms"):
            try:
                torch.use_deterministic_algorithms(True)
            except RuntimeError:
                # Some operations don't have deterministic implementations
                pass


    def seed_worker(worker_id: int) -> None:
        """Seed function for DataLoader workers.

        Ensures each worker has reproducible random state based on
        the initial seed and worker ID.

        Args:
            worker_id: DataLoader worker ID (0 to num_workers-1)

        Example:
            >>> loader = DataLoader(dataset, worker_init_fn=seed_worker)
        """
        worker_seed = torch.initial_seed() % 2**32
        np.random.seed(worker_seed)
        random.seed(worker_seed)


    def create_reproducible_dataloader(
        dataset: Dataset,
        batch_size: int,
        seed: int,
        shuffle: bool = True,
        num_workers: int = 0,
        **kwargs,
    ) -> DataLoader:
        """Create a DataLoader with reproducible shuffling and worker seeding.

        Args:
            dataset: Dataset to load
            batch_size: Batch size
            seed: Random seed for shuffling
            shuffle: Whether to shuffle (default True)
            num_workers: Number of worker processes
            **kwargs: Additional DataLoader arguments

        Returns:
            Configured DataLoader with reproducible behavior

        Example:
            >>> loader = create_reproducible_dataloader(dataset, 32, seed=42)
            >>> for batch in loader:
            ...     # Batches will be identical across runs with same seed
        """
        generator = torch.Generator()
        generator.manual_seed(seed)

        return DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=shuffle,
            num_workers=num_workers,
            worker_init_fn=seed_worker if num_workers > 0 else None,
            generator=generator if shuffle else None,
            **kwargs,
        )


    def get_rng_state() -> dict:
        """Capture current RNG state for all sources.

        Returns:
            Dictionary with RNG states that can be restored with set_rng_state()
        """
        state = {
            "python": random.getstate(),
            "numpy": np.random.get_state(),
            "torch_cpu": torch.get_rng_state(),
        }
        if torch.cuda.is_available():
            state["torch_cuda"] = torch.cuda.get_rng_state_all()
        return state


    def set_rng_state(state: dict) -> None:
        """Restore RNG state from captured state.

        Args:
            state: State dictionary from get_rng_state()
        """
        random.setstate(state["python"])
        np.random.set_state(state["numpy"])
        torch.set_rng_state(state["torch_cpu"])
        if "torch_cuda" in state and torch.cuda.is_available():
            torch.cuda.set_rng_state_all(state["torch_cuda"])
    ```

    Create altgrad/utils/__init__.py:
    ```python
    """Utility functions for AltGrad experiments."""

    from altgrad.utils.reproducibility import (
        set_seed_for_reproducibility,
        seed_worker,
        create_reproducible_dataloader,
        get_rng_state,
        set_rng_state,
    )

    __all__ = [
        "set_seed_for_reproducibility",
        "seed_worker",
        "create_reproducible_dataloader",
        "get_rng_state",
        "set_rng_state",
    ]
    ```

    Run reproducibility tests to verify.
  </action>
  <verify>cd /Users/prwilliams/Repos/altgrad && source .venv/bin/activate && python -m pytest tests/test_reproducibility.py -v (all reproducibility tests should pass)</verify>
  <done>All reproducibility tests pass, seed setup produces identical random sequences</done>
</task>

</tasks>

<verification>
```bash
# Run all tests
cd /Users/prwilliams/Repos/altgrad && source .venv/bin/activate
python -m pytest tests/test_integration.py tests/test_reproducibility.py -v

# Verify module imports
python -c "
from altgrad.integration import (
    QuantizedLinear,
    quantize_model,
    dequantize_model,
    LayerPrecisionRule,
    QuantizationConfig,
    create_mixed_precision_config,
)
from altgrad.utils import set_seed_for_reproducibility, seed_worker
print('All imports OK')
"

# Verify mixed precision on GPT
python -c "
from altgrad.training.model import GPT, GPTConfig
from altgrad.integration import quantize_model, create_mixed_precision_config, QuantizedLinear
import torch.nn as nn

config = GPTConfig(n_layer=2, n_head=2, n_embd=64, vocab_size=100)
model = GPT(config)

# Mixed precision: attention BF16, MLP FP8
quant_config = create_mixed_precision_config(
    attention_format=None,  # BF16
    mlp_format='E5M2',
    lm_head_format=None,  # BF16
)
quantize_model(model, config=quant_config)

# Check results
attn_layer = model.transformer.h[0].attn.c_attn
mlp_layer = model.transformer.h[0].mlp.c_fc

assert isinstance(attn_layer, nn.Linear), 'Attention should stay nn.Linear'
assert isinstance(mlp_layer, QuantizedLinear), 'MLP should be QuantizedLinear'
assert mlp_layer.fp8_format.name == 'E5M2', 'MLP format should be E5M2'
print('Mixed precision GPT: OK')
"

# Verify ablation reproducibility
python -c "
import torch
from altgrad.training.model import GPT, GPTConfig
from altgrad.integration import quantize_model
from altgrad.quantization import E5M2, E3M4
from altgrad.utils import set_seed_for_reproducibility

# Run 1: E5M2
set_seed_for_reproducibility(42)
config = GPTConfig(n_layer=2, n_head=2, n_embd=64, vocab_size=100)
model1 = GPT(config)
weights1_before = model1.transformer.h[0].mlp.c_fc.weight.clone()

# Run 2: E3M4 (same seed)
set_seed_for_reproducibility(42)
model2 = GPT(config)
weights2_before = model2.transformer.h[0].mlp.c_fc.weight.clone()

# Weights should be identical before quantization
assert torch.allclose(weights1_before, weights2_before), 'Initial weights should match'
print('Ablation reproducibility: OK')
"
```
</verification>

<success_criteria>
1. QuantizationConfig correctly routes layers to different FP8 formats via pattern matching
2. quantize_model() works with both single format and QuantizationConfig modes
3. Attention layers remain nn.Linear when format=None (BF16)
4. MLP layers become QuantizedLinear with specified format
5. set_seed_for_reproducibility() produces identical torch/numpy/python random sequences
6. GPT model initialization is reproducible with same seed
7. Ablation runs with same seed have identical initial state
8. All tests pass (config + reproducibility)
</success_criteria>

<output>
After completion, create `.planning/phases/03-model-integration/03-02-SUMMARY.md`
</output>
