---
phase: 03-model-integration
plan: 01
type: tdd
wave: 1
depends_on: []
files_modified:
  - altgrad/integration/__init__.py
  - altgrad/integration/wrapper.py
  - altgrad/integration/surgery.py
  - tests/test_integration.py
autonomous: true

must_haves:
  truths:
    - "QuantizedLinear forward pass applies quantize/dequantize to weights using configured FP8 format"
    - "QuantizedLinear maintains gradient flow through STE (gradients reach original Linear weights)"
    - "quantize_model() replaces nn.Linear layers with QuantizedLinear without modifying source model class"
    - "dequantize_model() restores original nn.Linear layers from QuantizedLinear wrappers"
    - "Weight tying between wte and lm_head is preserved after surgery"
  artifacts:
    - path: "altgrad/integration/wrapper.py"
      provides: "QuantizedLinear wrapper module"
      exports: ["QuantizedLinear"]
      min_lines: 80
    - path: "altgrad/integration/surgery.py"
      provides: "Model surgery functions"
      exports: ["quantize_model", "dequantize_model"]
      min_lines: 60
    - path: "altgrad/integration/__init__.py"
      provides: "Package exports"
      exports: ["QuantizedLinear", "quantize_model", "dequantize_model"]
    - path: "tests/test_integration.py"
      provides: "Integration tests for wrapper and surgery"
      min_lines: 150
  key_links:
    - from: "altgrad/integration/wrapper.py"
      to: "altgrad/quantization/ops.py"
      via: "quantize function"
      pattern: "from altgrad.quantization import quantize"
    - from: "altgrad/integration/wrapper.py"
      to: "altgrad/quantization/scaling.py"
      via: "AmaxHistory and compute_scale"
      pattern: "from altgrad.quantization import.*AmaxHistory.*compute_scale"
    - from: "altgrad/integration/surgery.py"
      to: "altgrad/integration/wrapper.py"
      via: "QuantizedLinear creation"
      pattern: "QuantizedLinear"
    - from: "tests/test_integration.py"
      to: "altgrad/training/model.py"
      via: "GPT model for surgery tests"
      pattern: "from altgrad.training.model import GPT"
---

<objective>
Implement QuantizedLinear wrapper and model surgery functions for injecting FP8 quantization into GPT models.

Purpose: Enable simulated FP8 quantization at layer boundaries without modifying the nanoGPT source code. The wrapper encapsulates quantization logic while surgery functions handle post-init layer replacement.

Output: `altgrad/integration/` module with QuantizedLinear wrapper and quantize_model/dequantize_model surgery functions, validated by comprehensive tests.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-model-integration/03-RESEARCH.md

# Existing quantization infrastructure
@altgrad/quantization/__init__.py
@altgrad/quantization/ops.py
@altgrad/quantization/scaling.py

# GPT model structure (surgery target)
@altgrad/training/model.py

# Key patterns from research:
# 1. QuantizedLinear wraps nn.Linear, applies quantize() on forward
# 2. Surgery uses list(model.named_modules()) then setattr pattern
# 3. Weight tying: model.transformer.wte.weight == model.lm_head.weight
# 4. Never mutate named_modules() while iterating
</context>

<tasks>

<task type="tdd">
  <name>Task 1: Write failing tests for QuantizedLinear and surgery</name>
  <files>tests/test_integration.py</files>
  <action>
    Create tests/test_integration.py with comprehensive test cases BEFORE implementation.

    Test structure:

    **QuantizedLinear tests:**
    1. test_quantized_linear_forward_produces_quantized_output():
       - Create nn.Linear(10, 5), wrap with QuantizedLinear using E5M2
       - Forward pass with torch.randn input
       - Output shape matches expected (batch, 5)
       - Output values differ from unwrapped linear (quantization effect)

    2. test_quantized_linear_gradient_flow():
       - Create QuantizedLinear wrapping nn.Linear
       - Forward pass with requires_grad=True input
       - Call .sum().backward()
       - Assert input.grad is not None
       - Assert wrapped_linear.linear.weight.grad is not None (STE passes gradients)

    3. test_quantized_linear_amax_history_updates():
       - Create QuantizedLinear
       - Run multiple forward passes with different inputs
       - Assert len(wrapper.weight_history) > 0
       - Assert len(wrapper.input_history) > 0

    4. test_quantized_linear_properties_expose_underlying():
       - Assert wrapper.weight is wrapper.linear.weight
       - Assert wrapper.bias is wrapper.linear.bias (or None)

    **Surgery tests:**
    5. test_quantize_model_replaces_linear_layers():
       - Create simple nn.Sequential with nn.Linear layers
       - Call quantize_model(model, format=E5M2)
       - Assert all nn.Linear replaced with QuantizedLinear
       - Assert isinstance(model[0], QuantizedLinear)

    6. test_quantize_model_preserves_weights():
       - Create model with known weights
       - Capture weights before surgery
       - Call quantize_model
       - Assert wrapper.linear.weight equals original weights

    7. test_quantize_model_gpt_weight_tying():
       - Create GPT model (from altgrad.training.model)
       - Assert model.transformer.wte.weight is model.lm_head.weight (before)
       - Call quantize_model with skip_patterns=["lm_head"] (or handle internally)
       - Assert weight tying still holds (wte.weight is lm_head.weight)

    8. test_dequantize_model_restores_linear():
       - Create model, quantize it
       - Call dequantize_model
       - Assert all QuantizedLinear replaced back with nn.Linear
       - Assert weights preserved

    9. test_quantize_model_skip_patterns():
       - Create model with multiple Linear layers named distinctly
       - Call quantize_model with skip_patterns=["layer2"]
       - Assert layer1 is QuantizedLinear
       - Assert layer2 is still nn.Linear

    10. test_quantize_model_nested_modules():
        - Create model with nested modules (like GPT's transformer.h[0].attn.c_attn)
        - Call quantize_model
        - Assert nested Linear layers are replaced

    Tests MUST fail initially (integration module doesn't exist yet).
  </action>
  <verify>cd /Users/prwilliams/Repos/altgrad && source .venv/bin/activate && python -m pytest tests/test_integration.py -v 2>&1 | grep -E "(FAILED|ERROR|ModuleNotFoundError)" (expect failures)</verify>
  <done>Test file exists with 10+ test cases, all tests fail due to missing implementation</done>
</task>

<task type="tdd">
  <name>Task 2: Implement QuantizedLinear wrapper</name>
  <files>altgrad/integration/__init__.py, altgrad/integration/wrapper.py</files>
  <action>
    Create directory: mkdir -p altgrad/integration

    Create altgrad/integration/wrapper.py:

    ```python
    """QuantizedLinear wrapper for simulated FP8 quantization.

    Wraps nn.Linear and applies quantize/dequantize on forward pass
    using the existing quantization infrastructure from Phase 1.
    """

    from typing import Optional

    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torch import Tensor

    from altgrad.quantization import FP8Format, quantize, compute_scale, AmaxHistory


    class QuantizedLinear(nn.Module):
        """Linear layer with simulated FP8 quantization.

        Wraps an existing nn.Linear and applies quantize/dequantize
        on forward pass. Maintains AmaxHistory for dynamic scaling.

        Args:
            linear: The nn.Linear module to wrap
            fp8_format: FP8 format for quantization (E5M2, E3M4, etc.)
            history_len: Length of amax history buffer (default: 16)
            quantize_input: Whether to quantize input activations (default: False)

        Example:
            >>> linear = nn.Linear(10, 5)
            >>> q_linear = QuantizedLinear(linear, E5M2)
            >>> out = q_linear(torch.randn(4, 10))
        """

        def __init__(
            self,
            linear: nn.Linear,
            fp8_format: FP8Format,
            history_len: int = 16,
            quantize_input: bool = False,
        ):
            super().__init__()
            self.linear = linear  # Keep original linear (not a copy)
            self.fp8_format = fp8_format
            self.quantize_input = quantize_input
            self.weight_history = AmaxHistory(history_len)
            self.input_history = AmaxHistory(history_len)

        def forward(self, x: Tensor) -> Tensor:
            # Update amax histories (detach to avoid affecting graph)
            self.weight_history.update(self.linear.weight.detach())

            # Compute weight scale and quantize weight
            w_scale = compute_scale(self.weight_history.get_amax(), self.fp8_format)
            w_scale_t = torch.tensor(w_scale, device=x.device, dtype=x.dtype)
            w_q = quantize(self.linear.weight, self.fp8_format, w_scale_t)

            # Optionally quantize input
            if self.quantize_input:
                self.input_history.update(x.detach())
                x_scale = compute_scale(self.input_history.get_amax(), self.fp8_format)
                x_scale_t = torch.tensor(x_scale, device=x.device, dtype=x.dtype)
                x = quantize(x, self.fp8_format, x_scale_t)

            # Compute output with quantized weight
            return F.linear(x, w_q, self.linear.bias)

        @classmethod
        def from_linear(
            cls,
            linear: nn.Linear,
            fp8_format: FP8Format,
            history_len: int = 16,
            quantize_input: bool = False,
        ) -> "QuantizedLinear":
            """Create QuantizedLinear from existing nn.Linear."""
            return cls(linear, fp8_format, history_len, quantize_input)

        @property
        def weight(self) -> Tensor:
            """Access wrapped linear's weight for compatibility."""
            return self.linear.weight

        @property
        def bias(self) -> Optional[Tensor]:
            """Access wrapped linear's bias for compatibility."""
            return self.linear.bias

        def extra_repr(self) -> str:
            return (
                f"in_features={self.linear.in_features}, "
                f"out_features={self.linear.out_features}, "
                f"format={self.fp8_format.name}"
            )
    ```

    Create altgrad/integration/__init__.py (initially just wrapper):
    ```python
    """Model integration for FP8 quantization.

    Provides QuantizedLinear wrapper and model surgery functions
    for injecting quantization into existing models without forking.
    """

    from altgrad.integration.wrapper import QuantizedLinear

    __all__ = ["QuantizedLinear"]
    ```

    Run wrapper tests to verify.
  </action>
  <verify>cd /Users/prwilliams/Repos/altgrad && source .venv/bin/activate && python -m pytest tests/test_integration.py -v -k "quantized_linear" (wrapper tests should pass)</verify>
  <done>QuantizedLinear wrapper tests pass, gradient flow verified, amax history updates correctly</done>
</task>

<task type="tdd">
  <name>Task 3: Implement model surgery functions</name>
  <files>altgrad/integration/surgery.py, altgrad/integration/__init__.py</files>
  <action>
    Create altgrad/integration/surgery.py:

    ```python
    """Model surgery functions for post-init layer replacement.

    Provides quantize_model() and dequantize_model() for replacing
    nn.Linear layers with QuantizedLinear wrappers without modifying
    the original model class.
    """

    from typing import List, Optional
    import copy

    import torch.nn as nn

    from altgrad.quantization import FP8Format
    from altgrad.integration.wrapper import QuantizedLinear


    def quantize_model(
        model: nn.Module,
        fp8_format: FP8Format,
        skip_patterns: Optional[List[str]] = None,
        history_len: int = 16,
        quantize_input: bool = False,
        inplace: bool = True,
    ) -> nn.Module:
        """Replace nn.Linear layers with QuantizedLinear wrappers.

        Traverses the model tree and replaces matching nn.Linear layers
        with QuantizedLinear wrappers that apply FP8 quantization on forward.

        Args:
            model: Model to quantize
            fp8_format: FP8 format for quantization
            skip_patterns: List of substrings; layers whose names contain any
                          pattern will be skipped (remain as nn.Linear)
            history_len: Amax history length for dynamic scaling
            quantize_input: Whether to quantize input activations
            inplace: Modify model in place (False creates deepcopy)

        Returns:
            Model with Linear layers replaced by QuantizedLinear

        Example:
            >>> model = GPT(config)
            >>> quantize_model(model, E5M2, skip_patterns=["lm_head"])
        """
        if not inplace:
            model = copy.deepcopy(model)

        skip_patterns = skip_patterns or []

        # Collect replacements first (avoid mutating during iteration)
        replacements = []

        for name, module in model.named_modules():
            if isinstance(module, nn.Linear):
                # Check skip patterns
                should_skip = any(pattern in name for pattern in skip_patterns)
                if not should_skip:
                    replacements.append((name, module))

        # Apply replacements
        for name, module in replacements:
            q_module = QuantizedLinear(
                module, fp8_format, history_len, quantize_input
            )
            _set_module_by_name(model, name, q_module)

        return model


    def dequantize_model(
        model: nn.Module,
        inplace: bool = True,
    ) -> nn.Module:
        """Restore nn.Linear layers from QuantizedLinear wrappers.

        Traverses the model tree and replaces QuantizedLinear wrappers
        with their underlying nn.Linear modules.

        Args:
            model: Model to dequantize
            inplace: Modify model in place (False creates deepcopy)

        Returns:
            Model with QuantizedLinear wrappers replaced by nn.Linear
        """
        if not inplace:
            model = copy.deepcopy(model)

        # Collect replacements first
        replacements = []

        for name, module in model.named_modules():
            if isinstance(module, QuantizedLinear):
                replacements.append((name, module.linear))

        # Apply replacements
        for name, linear in replacements:
            _set_module_by_name(model, name, linear)

        return model


    def _set_module_by_name(model: nn.Module, name: str, new_module: nn.Module) -> None:
        """Set a module by its dot-separated name path.

        Args:
            model: Root model
            name: Dot-separated path to module (e.g., "transformer.h.0.attn.c_attn")
            new_module: Module to set at that path
        """
        parts = name.rsplit(".", 1)
        if len(parts) == 1:
            # Top-level module
            setattr(model, name, new_module)
        else:
            # Nested module - find parent and set child
            parent_name, child_name = parts
            parent = _get_module_by_name(model, parent_name)
            setattr(parent, child_name, new_module)


    def _get_module_by_name(model: nn.Module, name: str) -> nn.Module:
        """Get a module by its dot-separated name path."""
        parts = name.split(".")
        module = model
        for part in parts:
            if part.isdigit():
                module = module[int(part)]
            else:
                module = getattr(module, part)
        return module
    ```

    Update altgrad/integration/__init__.py to include surgery:
    ```python
    """Model integration for FP8 quantization.

    Provides QuantizedLinear wrapper and model surgery functions
    for injecting quantization into existing models without forking.
    """

    from altgrad.integration.wrapper import QuantizedLinear
    from altgrad.integration.surgery import quantize_model, dequantize_model

    __all__ = [
        "QuantizedLinear",
        "quantize_model",
        "dequantize_model",
    ]
    ```

    Run all integration tests to verify.
  </action>
  <verify>cd /Users/prwilliams/Repos/altgrad && source .venv/bin/activate && python -m pytest tests/test_integration.py -v (all tests must pass)</verify>
  <done>All 10+ integration tests pass, surgery works on GPT model, weight tying preserved</done>
</task>

</tasks>

<verification>
```bash
# Run all integration tests
cd /Users/prwilliams/Repos/altgrad && source .venv/bin/activate
python -m pytest tests/test_integration.py -v

# Verify module imports correctly
python -c "from altgrad.integration import QuantizedLinear, quantize_model, dequantize_model; print('Imports OK')"

# Verify surgery on GPT model
python -c "
from altgrad.training.model import GPT, GPTConfig
from altgrad.integration import quantize_model
from altgrad.quantization import E5M2

config = GPTConfig(n_layer=2, n_head=2, n_embd=64, vocab_size=100)
model = GPT(config)

# Check weight tying before
assert model.transformer.wte.weight is model.lm_head.weight, 'Weight tying before'

# Quantize (skip lm_head to preserve weight tying)
quantize_model(model, E5M2, skip_patterns=['lm_head'])

# Check weight tying after
assert model.transformer.wte.weight is model.lm_head.weight, 'Weight tying after'
print('GPT surgery with weight tying: OK')
"

# Verify gradient flow
python -c "
import torch
from altgrad.integration import QuantizedLinear
from altgrad.quantization import E5M2

linear = torch.nn.Linear(10, 5)
q_linear = QuantizedLinear(linear, E5M2)
x = torch.randn(4, 10, requires_grad=True)
y = q_linear(x)
y.sum().backward()
assert x.grad is not None, 'Input gradient'
assert linear.weight.grad is not None, 'Weight gradient'
print('Gradient flow: OK')
"
```
</verification>

<success_criteria>
1. QuantizedLinear correctly applies FP8 quantization on forward pass
2. Gradient flow via STE is verified (gradients reach Linear.weight)
3. quantize_model() replaces all nn.Linear (except skipped) with QuantizedLinear
4. dequantize_model() restores original nn.Linear modules
5. Weight tying between GPT's wte and lm_head is preserved after surgery
6. Nested module replacement works (transformer.h[N].attn.c_attn)
7. All 10+ tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/03-model-integration/03-01-SUMMARY.md`
</output>
