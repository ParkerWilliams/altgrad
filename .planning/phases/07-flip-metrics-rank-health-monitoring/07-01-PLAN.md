---
phase: 07-flip-metrics-rank-health-monitoring
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - altgrad/quantization/flip_metrics.py
  - altgrad/quantization/__init__.py
  - tests/test_flip_metrics.py
autonomous: true

must_haves:
  truths:
    - "Flip count tracks how many weights changed FP8 representation after optimizer step"
    - "Flip rate computes per-layer flip counts relative to layer size"
    - "Flip metrics can be logged to W&B with proper step alignment"
  artifacts:
    - path: "altgrad/quantization/flip_metrics.py"
      provides: "WeightFlipTracker class, compute_flip_rate function"
      exports: ["WeightFlipTracker", "compute_flip_rate"]
    - path: "tests/test_flip_metrics.py"
      provides: "Unit tests for flip metrics"
      contains: "def test_"
  key_links:
    - from: "altgrad/quantization/flip_metrics.py"
      to: "altgrad/quantization/ops.py"
      via: "quantize import"
      pattern: "from altgrad\\.quantization\\.ops import quantize"
    - from: "altgrad/quantization/flip_metrics.py"
      to: "altgrad/quantization/formats.py"
      via: "FP8Format import"
      pattern: "from altgrad\\.quantization\\.formats import FP8Format"
---

<objective>
Implement weight flip counting to track when quantized weights change FP8 representation during training.

Purpose: Enable analysis of discrete optimization dynamics - how often weights actually change their quantized representation vs. remaining stuck at the same value. This complements bit-stall detection (gradient too small to change) with actual transition counting (did it change?).

Output: `flip_metrics.py` module with WeightFlipTracker class and compute_flip_rate function, plus unit tests.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-flip-metrics-rank-health-monitoring/07-RESEARCH.md

# Existing patterns to follow
@altgrad/quantization/diagnostics.py
@altgrad/quantization/ops.py
@altgrad/quantization/formats.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create flip_metrics.py with WeightFlipTracker</name>
  <files>altgrad/quantization/flip_metrics.py</files>
  <action>
Create `altgrad/quantization/flip_metrics.py` with:

1. **WeightFlipTracker class** - tracks weight transitions between FP8 representations:
   - `__init__()`: Initialize empty prev_quantized dict and counters
   - `snapshot_pre_step(name: str, weight: Tensor, format: FP8Format, scale: Tensor)`: Quantize and store weight state before optimizer step
   - `compute_flips_post_step(name: str, weight: Tensor, format: FP8Format, scale: Tensor) -> int`: Count weights that changed FP8 representation after step
   - `get_flip_counts() -> Dict[str, int]`: Return per-layer flip counts
   - `get_flip_rates() -> Dict[str, float]`: Return per-layer flip rates (flips / total_weights)
   - `reset()`: Clear all state for new epoch

2. **compute_flip_rate function** - standalone utility:
   - `compute_flip_rate(prev_quantized: Tensor, curr_quantized: Tensor) -> float`: Given two quantized tensors, compute flip rate

Follow the pattern from `diagnostics.py` (BitStallDetector):
- Use `quantize()` from ops.py
- Store clone of quantized tensor (not reference)
- Track total_weights per layer for rate computation
- Use Dict[str, int] for layer-wise tracking

Include comprehensive docstrings with usage examples.
  </action>
  <verify>
```bash
cd /Users/prwilliams/Repos/altgrad && python -c "
from altgrad.quantization.flip_metrics import WeightFlipTracker, compute_flip_rate
import torch
# Basic instantiation
tracker = WeightFlipTracker()
print('WeightFlipTracker instantiated')
# Function exists
print(f'compute_flip_rate callable: {callable(compute_flip_rate)}')
"
```
  </verify>
  <done>WeightFlipTracker class instantiates without error, compute_flip_rate function is callable</done>
</task>

<task type="auto">
  <name>Task 2: Add flip_metrics exports and unit tests</name>
  <files>altgrad/quantization/__init__.py, tests/test_flip_metrics.py</files>
  <action>
1. **Update `altgrad/quantization/__init__.py`**:
   - Add imports: `from altgrad.quantization.flip_metrics import WeightFlipTracker, compute_flip_rate`
   - Add to `__all__`: `"WeightFlipTracker"`, `"compute_flip_rate"`

2. **Create `tests/test_flip_metrics.py`** with tests:

```python
"""Tests for weight flip metrics."""
import pytest
import torch
from altgrad.quantization.flip_metrics import WeightFlipTracker, compute_flip_rate
from altgrad.quantization.formats import E5M2, E3M4

class TestComputeFlipRate:
    def test_identical_tensors_zero_rate(self):
        """Identical quantized tensors have zero flip rate."""
        q = torch.tensor([1.0, 2.0, 3.0])
        assert compute_flip_rate(q, q.clone()) == 0.0

    def test_all_different_full_rate(self):
        """All different values have 100% flip rate."""
        q1 = torch.tensor([1.0, 2.0, 3.0])
        q2 = torch.tensor([4.0, 5.0, 6.0])
        assert compute_flip_rate(q1, q2) == 1.0

    def test_partial_flips(self):
        """Partial changes give correct rate."""
        q1 = torch.tensor([1.0, 2.0, 3.0, 4.0])
        q2 = torch.tensor([1.0, 2.0, 5.0, 6.0])  # 2 of 4 changed
        assert compute_flip_rate(q1, q2) == 0.5


class TestWeightFlipTracker:
    def test_no_change_zero_flips(self):
        """Unchanged weights produce zero flips."""
        tracker = WeightFlipTracker()
        w = torch.tensor([1.0, 2.0, 3.0])
        scale = torch.tensor(1.0)

        tracker.snapshot_pre_step("layer1", w, E5M2, scale)
        flips = tracker.compute_flips_post_step("layer1", w, E5M2, scale)
        assert flips == 0

    def test_changed_weights_count_flips(self):
        """Changed weights are counted as flips."""
        tracker = WeightFlipTracker()
        w1 = torch.tensor([1.0, 2.0, 3.0])
        w2 = torch.tensor([10.0, 20.0, 30.0])  # All different after quantization
        scale = torch.tensor(1.0)

        tracker.snapshot_pre_step("layer1", w1, E5M2, scale)
        flips = tracker.compute_flips_post_step("layer1", w2, E5M2, scale)
        assert flips == 3

    def test_flip_rates_correct(self):
        """get_flip_rates returns correct per-layer rates."""
        tracker = WeightFlipTracker()
        w = torch.randn(100)
        scale = torch.tensor(1.0)

        tracker.snapshot_pre_step("layer1", w, E5M2, scale)
        tracker.compute_flips_post_step("layer1", w + 0.5, E5M2, scale)

        rates = tracker.get_flip_rates()
        assert "layer1" in rates
        assert 0.0 <= rates["layer1"] <= 1.0

    def test_reset_clears_state(self):
        """reset() clears all tracking state."""
        tracker = WeightFlipTracker()
        w = torch.tensor([1.0, 2.0])
        scale = torch.tensor(1.0)

        tracker.snapshot_pre_step("layer1", w, E5M2, scale)
        tracker.compute_flips_post_step("layer1", w + 1.0, E5M2, scale)

        tracker.reset()
        assert tracker.get_flip_counts() == {}
        assert tracker.get_flip_rates() == {}
```

Run tests to verify.
  </action>
  <verify>
```bash
cd /Users/prwilliams/Repos/altgrad && python -m pytest tests/test_flip_metrics.py -v
```
  </verify>
  <done>All flip metrics tests pass, exports work from altgrad.quantization</done>
</task>

</tasks>

<verification>
1. Module imports: `from altgrad.quantization import WeightFlipTracker, compute_flip_rate`
2. All tests pass: `pytest tests/test_flip_metrics.py -v`
3. Tracker correctly counts flips between pre/post optimizer state
</verification>

<success_criteria>
- WeightFlipTracker class tracks per-layer flip counts across training steps
- compute_flip_rate function computes flip rate from two quantized tensors
- Unit tests verify correct counting for unchanged, fully changed, and partially changed weights
- Module exports available from altgrad.quantization
</success_criteria>

<output>
After completion, create `.planning/phases/07-flip-metrics-rank-health-monitoring/07-01-SUMMARY.md`
</output>
