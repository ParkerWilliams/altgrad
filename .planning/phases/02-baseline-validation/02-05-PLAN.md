---
phase: 02-baseline-validation
plan: 05
type: execute
wave: 4
depends_on: ["02-04"]
files_modified:
  - experiments/configs/e5m2_fp8.yaml
  - altgrad/training/trainer.py
autonomous: false

must_haves:
  truths:
    - "E5M2 FP8 training runs with identical seed to BF16 baseline"
    - "FP32 shadow tracks gradient cosine similarity every step"
    - "All stability metrics logged (overflow, underflow, bit-stall)"
    - "Comparison plots generated in W&B showing BF16 vs FP8 curves"
    - "Training completes without persistent NaN (recoverable instability OK)"
  artifacts:
    - path: "experiments/configs/e5m2_fp8.yaml"
      provides: "E5M2 FP8 experiment configuration"
  key_links:
    - from: "experiments/configs/e5m2_fp8.yaml"
      to: "altgrad/quantization"
      via: "use_fp8=true, fp8_format=E5M2"
      pattern: "use_fp8.*true|fp8_format.*E5M2"
    - from: "altgrad/training/trainer.py"
      to: "altgrad/training/shadow.py"
      via: "FP32ShadowModel for gradient comparison"
      pattern: "FP32ShadowModel|shadow.*forward"
---

<objective>
Run E5M2 FP8 experiment with FP32 shadow gradient comparison against BF16 baseline.

Purpose: Validate that standard FP8 (E5M2) training works and produces comparable results to BF16. This confirms the quantization infrastructure is correct before testing exotic formats in Phase 4. The FP32 shadow provides gradient fidelity metrics.

Output: Completed W&B run with FP8 training curves, gradient similarity metrics, and automatic comparison with BF16 baseline.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-baseline-validation/02-RESEARCH.md
@.planning/phases/02-baseline-validation/02-CONTEXT.md
@.planning/phases/02-baseline-validation/02-04-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create E5M2 FP8 config and integrate quantization into trainer</name>
  <files>
    experiments/configs/e5m2_fp8.yaml
    altgrad/training/trainer.py
  </files>
  <action>
Create E5M2 configuration and implement concrete quantization integration in trainer.

**experiments/configs/e5m2_fp8.yaml:**

```yaml
# E5M2 FP8 Experiment
# Standard FP8 format comparison to BF16 baseline

# Model architecture (IDENTICAL to bf16_baseline)
n_layer: 6
n_head: 6
n_embd: 384
block_size: 256
vocab_size: 50257
dropout: 0.0

# Training (IDENTICAL to bf16_baseline)
batch_size: 12
learning_rate: 6e-4
max_steps: 2000
warmup_steps: 100
grad_clip: 1.0

# Precision - FP8 with shadow for gradient comparison
use_fp8: true
fp8_format: "E5M2"
use_shadow: true  # FP32 shadow for gradient cosine similarity

# Checkpointing
checkpoint_interval: 100
checkpoint_dir: "checkpoints/e5m2_fp8"
max_checkpoints: 3

# Stability thresholds
nan_patience: 10
bit_stall_threshold: 0.5
overflow_threshold: 0.01
dead_neuron_threshold: 1.0e-8
dead_neuron_window: 100

# Logging
log_interval: 1
eval_interval: 100

# W&B
project: "altgrad"
run_name: "e5m2-fp8"
tags: ["fp8", "e5m2", "eurlex", "shadow"]

# Reproducibility (SAME SEED as baseline for comparison)
seed: 42
```

**Update altgrad/training/trainer.py with CONCRETE quantization integration:**

Import quantization at module level (guarded):
```python
# At top of trainer.py
from altgrad.quantization import (
    quantize,
    dequantize,
    compute_scale,
    AmaxHistory,
    BitStallDetector,
    FORMAT_REGISTRY,
)
```

**1. In `Trainer.__init__` - Initialize quantization components:**
```python
def __init__(self, config: TrainConfig, model: GPT, data_dir: str, device: str):
    # ... existing init ...

    # Quantization setup when use_fp8=True
    if config.use_fp8:
        # Get format from registry (E5M2, E3M4, etc.)
        self.fp8_format = FORMAT_REGISTRY[config.fp8_format]

        # Create per-layer amax history for dynamic scaling
        # Track: attention Q/K/V projections, MLP layers, output projection
        self.amax_histories = {}
        for name, param in model.named_parameters():
            if param.requires_grad and param.dim() >= 2:
                self.amax_histories[name] = AmaxHistory(window_size=16)

        # Create BitStallDetector for gradient stall monitoring
        self.bit_stall_detector = BitStallDetector(threshold=config.bit_stall_threshold)

        # Overflow/underflow counters
        self.overflow_count = 0
        self.underflow_count = 0
        self.total_quant_ops = 0
```

**2. In `Trainer.train_step` - Apply quantization at boundaries:**
```python
def train_step(self, x, y) -> dict:
    self.model.train()

    # Forward pass - quantize activations at layer boundaries
    with torch.autocast(device_type=self.device, dtype=torch.bfloat16, enabled=not self.config.use_fp8):
        if self.config.use_fp8:
            # Simulated FP8: quantize weights before forward, dequantize after
            # This simulates FP8 compute without actual FP8 storage
            with self._quantized_forward_context():
                logits, loss = self.model(x, y)
        else:
            logits, loss = self.model(x, y)

    # Backward pass
    self.scaler.scale(loss).backward()

    # If FP8: quantize gradients before optimizer, dequantize after
    if self.config.use_fp8:
        self._quantize_gradients()

    # Shadow comparison (if enabled)
    if self.shadow is not None:
        shadow_loss = self.shadow.forward_backward(x, y)
        grad_metrics = self.shadow.compute_gradient_similarity(self.model)
    else:
        grad_metrics = {}

    # Gradient clipping
    self.scaler.unscale_(self.optimizer)
    grad_norm = torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.grad_clip)

    # Optimizer step
    self.scaler.step(self.optimizer)
    self.scaler.update()

    # Post-optimizer: detect bit-stall on weight updates
    if self.config.use_fp8:
        self._update_bit_stall_detector()

    self.optimizer.zero_grad(set_to_none=True)

    return {
        'loss': loss.item(),
        'grad_norm': grad_norm.item(),
        **grad_metrics,
        **self._get_quantization_metrics() if self.config.use_fp8 else {},
    }

def _quantized_forward_context(self):
    """Context manager for simulated FP8 forward pass."""
    # Store original weights, apply quantized versions
    # On exit, restore originals
    # This simulates FP8 matmul precision
    class QuantizedContext:
        def __init__(ctx, trainer):
            ctx.trainer = trainer
            ctx.original_weights = {}

        def __enter__(ctx):
            for name, param in ctx.trainer.model.named_parameters():
                if name in ctx.trainer.amax_histories:
                    ctx.original_weights[name] = param.data.clone()
                    # Update amax history
                    ctx.trainer.amax_histories[name].update(param.data)
                    # Compute scale and quantize
                    amax = ctx.trainer.amax_histories[name].get_amax()
                    scale = compute_scale(amax, ctx.trainer.fp8_format)
                    # Apply simulated quantization (stays in fp32 but with fp8 precision)
                    quantized = quantize(param.data, ctx.trainer.fp8_format, torch.tensor(scale, device=param.device))
                    param.data.copy_(quantized)
            return ctx

        def __exit__(ctx, *args):
            # Restore original weights
            for name, orig in ctx.original_weights.items():
                ctx.trainer.model.get_parameter(name).data.copy_(orig)

    return QuantizedContext(self)

def _quantize_gradients(self):
    """Quantize gradients before optimizer step (simulated FP8)."""
    for name, param in self.model.named_parameters():
        if param.grad is not None and name in self.amax_histories:
            # Update amax from gradient
            grad_amax = param.grad.abs().max().item()
            scale = compute_scale(grad_amax, self.fp8_format)

            # Count overflow/underflow
            max_val = self.fp8_format.max_value
            min_val = self.fp8_format.min_subnormal
            overflow = (param.grad.abs() > max_val * scale).sum().item()
            underflow = ((param.grad.abs() < min_val * scale) & (param.grad != 0)).sum().item()
            self.overflow_count += overflow
            self.underflow_count += underflow
            self.total_quant_ops += param.grad.numel()

            # Quantize gradient
            param.grad.data = quantize(param.grad.data, self.fp8_format, torch.tensor(scale, device=param.device))

def _update_bit_stall_detector(self):
    """Check for bit-stall after optimizer step."""
    for name, param in self.model.named_parameters():
        if param.grad is not None and name in self.amax_histories:
            # Get previous and current weight
            # Detect if update rounded to zero in FP8 precision
            self.bit_stall_detector.update(name, param.grad, param.data)

def _get_quantization_metrics(self) -> dict:
    """Return quantization-specific metrics for logging."""
    total = max(self.total_quant_ops, 1)
    return {
        'quantization/overflow_rate': self.overflow_count / total,
        'quantization/underflow_rate': self.underflow_count / total,
        'quantization/bit_stall_rate': self.bit_stall_detector.get_stall_rate(),
        'quantization/dynamic_range_used': self.bit_stall_detector.get_dynamic_range_usage(),
    }
```

**3. In `_log_metrics` - Include quantization metrics:**
```python
def _log_metrics(self, step: int, train_metrics: dict, val_loss: float = None):
    metrics = {
        'train/loss': train_metrics['loss'],
        'train/perplexity': math.exp(train_metrics['loss']),
        'train/grad_norm': train_metrics['grad_norm'],
        'train/lr': self.optimizer.param_groups[0]['lr'],
    }

    # Gradient similarity from shadow (includes SNR metrics from 02-03)
    for k, v in train_metrics.items():
        if k.startswith('grad_cos_sim/') or k.startswith('grad_snr/'):
            metrics[k] = v

    # Quantization metrics when use_fp8
    if self.config.use_fp8:
        for k, v in train_metrics.items():
            if k.startswith('quantization/'):
                metrics[k] = v

    if val_loss is not None:
        metrics['val/loss'] = val_loss
        metrics['val/perplexity'] = math.exp(val_loss)

    self.tracker.log(metrics, step=step)
```
  </action>
  <verify>
```bash
source .venv/bin/activate
# Test 1: Config loads correctly
python -c "from altgrad.training import load_config; c = load_config('experiments/configs/e5m2_fp8.yaml'); print(f'use_fp8={c.use_fp8}, use_shadow={c.use_shadow}')"

# Test 2: Quantization components initialize when use_fp8=True
python -c "
from altgrad.training.model import GPT, GPTConfig
from altgrad.training.trainer import Trainer
from altgrad.training.config import TrainConfig
import torch

# Create tiny model and config with use_fp8=True
config = TrainConfig(
    n_layer=1, n_head=1, n_embd=32, block_size=16, vocab_size=100,
    batch_size=2, learning_rate=1e-3, max_steps=10,
    use_fp8=True, fp8_format='E5M2', use_shadow=True,
    checkpoint_dir='/tmp/test_ckpt', project='test', run_name='test'
)
model_config = GPTConfig(n_layer=1, n_head=1, n_embd=32, block_size=16, vocab_size=100)
model = GPT(model_config)

# This should NOT raise - quantization components must initialize
trainer = Trainer(config, model, data_dir='/tmp', device='cpu')

# Verify quantization components exist
assert hasattr(trainer, 'fp8_format'), 'fp8_format not initialized'
assert hasattr(trainer, 'amax_histories'), 'amax_histories not initialized'
assert hasattr(trainer, 'bit_stall_detector'), 'bit_stall_detector not initialized'
assert len(trainer.amax_histories) > 0, 'No amax histories created for model params'

print('SUCCESS: All quantization components initialized')
print(f'  - fp8_format: {trainer.fp8_format}')
print(f'  - amax_histories: {len(trainer.amax_histories)} tracked params')
print(f'  - bit_stall_detector: {trainer.bit_stall_detector}')
"
```
  </verify>
  <done>E5M2 config loads AND quantization components (fp8_format, amax_histories, bit_stall_detector) initialize when use_fp8=True</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
E5M2 FP8 training run with FP32 shadow gradient comparison.

This experiment:
- Trains same 10M parameter GPT model on EurLex
- Uses E5M2 FP8 format (standard 5 exponent, 2 mantissa bits)
- Maintains FP32 shadow model for per-step gradient comparison
- Uses IDENTICAL seed (42) to BF16 baseline
- Logs all quantization stability metrics
- Enables automatic W&B comparison against baseline
  </what-built>
  <how-to-verify>
1. Start the experiment:
   ```bash
   source .venv/bin/activate
   python experiments/run_experiment.py experiments/configs/e5m2_fp8.yaml
   ```

2. During training, verify in terminal:
   - Loss decreases (may be slightly worse than BF16)
   - Gradient similarity metrics logged
   - Bit-stall rate below 50% threshold (no alerts)
   - Overflow rate below 1% threshold (no alerts)

3. In W&B dashboard, create comparison view:
   - Select both "bf16-baseline" and "e5m2-fp8" runs
   - Compare loss curves (should track closely)
   - Compare perplexity curves
   - Check gradient cosine similarity (should be >0.9 for healthy training)
   - Check bit-stall rate over time

4. Key metrics to verify:
   - `grad_cos_sim/mean` > 0.9 (FP8 gradients align with FP32)
   - `quantization/bit_stall_rate` < 0.5
   - `quantization/overflow_rate` < 0.01
   - Loss within ~10% of BF16 baseline at same step

5. Check checkpoints:
   ```bash
   ls -la checkpoints/e5m2_fp8/
   ```

Expected behavior:
- Loss curve tracks BF16 closely (within 10%)
- Gradient similarity high (>0.9) early, may decrease slightly
- E5M2 should be stable (it's the standard FP8 format)
- Bit-stall rate typically 10-30% for E5M2

If FP8 shows >20% worse loss or gradient similarity <0.8, that's a signal to investigate.
  </how-to-verify>
  <resume-signal>Type "fp8 verified" if E5M2 training completes with acceptable loss and high gradient similarity, or describe stability issues</resume-signal>
</task>

<task type="auto">
  <name>Task 3: Generate baseline comparison summary</name>
  <files>None (W&B and terminal output only)</files>
  <action>
After both experiments complete, generate comparison summary.

Using W&B programmatic access or manual inspection:

1. Extract key metrics at step 2000:
   - BF16 final loss, final perplexity
   - E5M2 final loss, final perplexity
   - E5M2 mean gradient similarity
   - E5M2 mean bit-stall rate

2. Compute comparison:
   - Loss ratio: E5M2_loss / BF16_loss (should be ~1.0-1.1)
   - Perplexity ratio: E5M2_ppl / BF16_ppl

3. Report summary in console:
   ```
   === Phase 2 Baseline Comparison ===

   Metric            BF16        E5M2        Ratio
   ------------------------------------------------
   Final Loss        X.XXX       X.XXX       X.XX
   Final PPL         XXX.X       XXX.X       X.XX

   E5M2 Stability:
   - Mean Grad Cosine Sim: 0.XXX
   - Mean Bit-Stall Rate:  XX.X%
   - Max Overflow Rate:    XX.X%

   Verdict: [PASS/INVESTIGATE] - E5M2 is [within/outside] 10% of BF16
   ```

This confirms Phase 2 success criteria are met.
  </action>
  <verify>Visual inspection of comparison output</verify>
  <done>Comparison summary shows E5M2 within 10% of BF16 baseline</done>
</task>

</tasks>

<verification>
1. E5M2 FP8 training completes 2000 steps
2. Gradient cosine similarity > 0.9 (mean across training)
3. Loss within 10% of BF16 baseline
4. No persistent NaN or divergence
5. All stability metrics logged and within thresholds
6. W&B shows both runs with comparable curves
</verification>

<success_criteria>
- E5M2 FP8 validates as working FP8 format
- FP32 shadow provides meaningful gradient comparison
- Stability monitoring catches any issues
- BF16 vs E5M2 comparison establishes baseline for Phase 4
- Ready to test exotic formats (E1M6, E3M4, E7M0)
</success_criteria>

<output>
After completion, create `.planning/phases/02-baseline-validation/02-05-SUMMARY.md`
</output>
