---
phase: 02-baseline-validation
plan: 05
type: execute
wave: 4
depends_on: ["02-04"]
files_modified:
  - experiments/configs/e5m2_fp8.yaml
  - altgrad/training/trainer.py
autonomous: false

must_haves:
  truths:
    - "E5M2 FP8 training runs with identical seed to BF16 baseline"
    - "FP32 shadow tracks gradient cosine similarity every step"
    - "All stability metrics logged (overflow, underflow, bit-stall)"
    - "Comparison plots generated in W&B showing BF16 vs FP8 curves"
    - "Training completes without persistent NaN (recoverable instability OK)"
  artifacts:
    - path: "experiments/configs/e5m2_fp8.yaml"
      provides: "E5M2 FP8 experiment configuration"
  key_links:
    - from: "experiments/configs/e5m2_fp8.yaml"
      to: "altgrad/quantization"
      via: "use_fp8=true, fp8_format=E5M2"
      pattern: "use_fp8.*true|fp8_format.*E5M2"
    - from: "altgrad/training/trainer.py"
      to: "altgrad/training/shadow.py"
      via: "FP32ShadowModel for gradient comparison"
      pattern: "FP32ShadowModel|shadow.*forward"
---

<objective>
Run E5M2 FP8 experiment with FP32 shadow gradient comparison against BF16 baseline.

Purpose: Validate that standard FP8 (E5M2) training works and produces comparable results to BF16. This confirms the quantization infrastructure is correct before testing exotic formats in Phase 4. The FP32 shadow provides gradient fidelity metrics.

Output: Completed W&B run with FP8 training curves, gradient similarity metrics, and automatic comparison with BF16 baseline.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-baseline-validation/02-RESEARCH.md
@.planning/phases/02-baseline-validation/02-CONTEXT.md
@.planning/phases/02-baseline-validation/02-04-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create E5M2 FP8 config and integrate quantization</name>
  <files>
    experiments/configs/e5m2_fp8.yaml
    altgrad/training/trainer.py
  </files>
  <action>
Create E5M2 configuration and ensure trainer integrates quantization.

**experiments/configs/e5m2_fp8.yaml:**

```yaml
# E5M2 FP8 Experiment
# Standard FP8 format comparison to BF16 baseline

# Model architecture (IDENTICAL to bf16_baseline)
n_layer: 6
n_head: 6
n_embd: 384
block_size: 256
vocab_size: 50257
dropout: 0.0

# Training (IDENTICAL to bf16_baseline)
batch_size: 12
learning_rate: 6e-4
max_steps: 2000
warmup_steps: 100
grad_clip: 1.0

# Precision - FP8 with shadow for gradient comparison
use_fp8: true
fp8_format: "E5M2"
use_shadow: true  # FP32 shadow for gradient cosine similarity

# Checkpointing
checkpoint_interval: 100
checkpoint_dir: "checkpoints/e5m2_fp8"
max_checkpoints: 3

# Stability thresholds
nan_patience: 10
bit_stall_threshold: 0.5
overflow_threshold: 0.01
dead_neuron_threshold: 1.0e-8
dead_neuron_window: 100

# Logging
log_interval: 1
eval_interval: 100

# W&B
project: "altgrad"
run_name: "e5m2-fp8"
tags: ["fp8", "e5m2", "eurlex", "shadow"]

# Reproducibility (SAME SEED as baseline for comparison)
seed: 42
```

**Update altgrad/training/trainer.py (if needed):**

Ensure the Trainer class properly integrates altgrad.quantization when use_fp8=True:

1. In `__init__`:
   - If config.use_fp8: import quantization utilities
   - Create AmaxHistory for dynamic scaling
   - Create BitStallDetector for stall tracking

2. In `train_step`:
   - If use_fp8: Apply quantization to activations/weights at appropriate points
   - For Phase 2, use simulated quantization: store in float32/bf16, quantize at boundaries
   - Track overflow/underflow counts
   - Update amax history after forward pass
   - Update bit-stall detector after optimizer step

3. In `_log_metrics`:
   - If use_fp8: Add quantization-specific metrics
     - `quantization/overflow_rate`
     - `quantization/underflow_rate`
     - `quantization/bit_stall_rate`
     - `quantization/dynamic_range_used`
   - If use_shadow: Add gradient comparison metrics from shadow

Note: Full layer-by-layer quantization is Phase 3. Phase 2 focuses on infrastructure validation with simulated quantization at key points.
  </action>
  <verify>
```bash
source .venv/bin/activate
python -c "from altgrad.training import load_config; c = load_config('experiments/configs/e5m2_fp8.yaml'); print(f'use_fp8={c.use_fp8}, use_shadow={c.use_shadow}')"
```
  </verify>
  <done>E5M2 config loads with FP8 and shadow enabled</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
E5M2 FP8 training run with FP32 shadow gradient comparison.

This experiment:
- Trains same 10M parameter GPT model on EurLex
- Uses E5M2 FP8 format (standard 5 exponent, 2 mantissa bits)
- Maintains FP32 shadow model for per-step gradient comparison
- Uses IDENTICAL seed (42) to BF16 baseline
- Logs all quantization stability metrics
- Enables automatic W&B comparison against baseline
  </what-built>
  <how-to-verify>
1. Start the experiment:
   ```bash
   source .venv/bin/activate
   python experiments/run_experiment.py experiments/configs/e5m2_fp8.yaml
   ```

2. During training, verify in terminal:
   - Loss decreases (may be slightly worse than BF16)
   - Gradient similarity metrics logged
   - Bit-stall rate below 50% threshold (no alerts)
   - Overflow rate below 1% threshold (no alerts)

3. In W&B dashboard, create comparison view:
   - Select both "bf16-baseline" and "e5m2-fp8" runs
   - Compare loss curves (should track closely)
   - Compare perplexity curves
   - Check gradient cosine similarity (should be >0.9 for healthy training)
   - Check bit-stall rate over time

4. Key metrics to verify:
   - `grad_cos_sim/mean` > 0.9 (FP8 gradients align with FP32)
   - `quantization/bit_stall_rate` < 0.5
   - `quantization/overflow_rate` < 0.01
   - Loss within ~10% of BF16 baseline at same step

5. Check checkpoints:
   ```bash
   ls -la checkpoints/e5m2_fp8/
   ```

Expected behavior:
- Loss curve tracks BF16 closely (within 10%)
- Gradient similarity high (>0.9) early, may decrease slightly
- E5M2 should be stable (it's the standard FP8 format)
- Bit-stall rate typically 10-30% for E5M2

If FP8 shows >20% worse loss or gradient similarity <0.8, that's a signal to investigate.
  </how-to-verify>
  <resume-signal>Type "fp8 verified" if E5M2 training completes with acceptable loss and high gradient similarity, or describe stability issues</resume-signal>
</task>

<task type="auto">
  <name>Task 3: Generate baseline comparison summary</name>
  <files>None (W&B and terminal output only)</files>
  <action>
After both experiments complete, generate comparison summary.

Using W&B programmatic access or manual inspection:

1. Extract key metrics at step 2000:
   - BF16 final loss, final perplexity
   - E5M2 final loss, final perplexity
   - E5M2 mean gradient similarity
   - E5M2 mean bit-stall rate

2. Compute comparison:
   - Loss ratio: E5M2_loss / BF16_loss (should be ~1.0-1.1)
   - Perplexity ratio: E5M2_ppl / BF16_ppl

3. Report summary in console:
   ```
   === Phase 2 Baseline Comparison ===

   Metric            BF16        E5M2        Ratio
   ------------------------------------------------
   Final Loss        X.XXX       X.XXX       X.XX
   Final PPL         XXX.X       XXX.X       X.XX

   E5M2 Stability:
   - Mean Grad Cosine Sim: 0.XXX
   - Mean Bit-Stall Rate:  XX.X%
   - Max Overflow Rate:    XX.X%

   Verdict: [PASS/INVESTIGATE] - E5M2 is [within/outside] 10% of BF16
   ```

This confirms Phase 2 success criteria are met.
  </action>
  <verify>Visual inspection of comparison output</verify>
  <done>Comparison summary shows E5M2 within 10% of BF16 baseline</done>
</task>

</tasks>

<verification>
1. E5M2 FP8 training completes 2000 steps
2. Gradient cosine similarity > 0.9 (mean across training)
3. Loss within 10% of BF16 baseline
4. No persistent NaN or divergence
5. All stability metrics logged and within thresholds
6. W&B shows both runs with comparable curves
</verification>

<success_criteria>
- E5M2 FP8 validates as working FP8 format
- FP32 shadow provides meaningful gradient comparison
- Stability monitoring catches any issues
- BF16 vs E5M2 comparison establishes baseline for Phase 4
- Ready to test exotic formats (E1M6, E3M4, E7M0)
</success_criteria>

<output>
After completion, create `.planning/phases/02-baseline-validation/02-05-SUMMARY.md`
</output>
