---
phase: 02-baseline-validation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - altgrad/training/__init__.py
  - altgrad/training/data.py
  - data/eurlex/train.bin
  - data/eurlex/val.bin
  - tests/test_data.py
autonomous: true

must_haves:
  truths:
    - "EurLex documents are tokenized with GPT-2 BPE (tiktoken)"
    - "Tokens are stored as memory-mapped uint16 binary files"
    - "Train/val split exists and is loadable"
    - "get_batch() returns correct tensor shapes for training"
  artifacts:
    - path: "altgrad/training/data.py"
      provides: "EurLex data preparation and batch loading"
      exports: ["prepare_eurlex", "get_batch"]
    - path: "data/eurlex/train.bin"
      provides: "Tokenized training data"
    - path: "data/eurlex/val.bin"
      provides: "Tokenized validation data"
    - path: "tests/test_data.py"
      provides: "Data loading tests"
  key_links:
    - from: "altgrad/training/data.py"
      to: "data/eurlex/*.bin"
      via: "np.memmap file access"
      pattern: "memmap.*eurlex"
---

<objective>
EurLex dataset preparation in nanoGPT-style binary format with batch loading utilities.

Purpose: Provide tokenized training data for BF16 baseline and FP8 experiments. The nanoGPT-style binary format enables efficient memory-mapped loading without per-batch tokenization overhead.

Output: `data/eurlex/train.bin` and `data/eurlex/val.bin` containing GPT-2 BPE tokens as uint16, plus `get_batch()` function for training loop integration.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-baseline-validation/02-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: EurLex data preparation module</name>
  <files>
    altgrad/training/__init__.py
    altgrad/training/data.py
  </files>
  <action>
Create `altgrad/training/` package with data preparation module.

In `altgrad/training/data.py`:

1. `prepare_eurlex(data_dir: str = "data/eurlex", num_proc: int = 8) -> dict`:
   - Load `nlpaueb/multi_eurlex` with `lang='en'` from HuggingFace datasets
   - Use tiktoken `gpt2` encoding (50257 vocab)
   - Tokenize all documents, append EOT token after each
   - Concatenate tokens into single array per split
   - Write to `{data_dir}/train.bin` and `{data_dir}/val.bin` as uint16 memmap
   - Return dict with token counts per split

2. `get_batch(split: str, data_dir: str, block_size: int, batch_size: int, device: str) -> tuple[Tensor, Tensor]`:
   - Load memmap file for split
   - Random sample `batch_size` starting positions
   - Return (x, y) where y = x shifted by 1 token
   - Tensors on specified device as int64

Note: The multi_eurlex dataset uses "train", "validation", "test" splits. Map "val" to "validation" internally. Store as train.bin and val.bin for nanoGPT compatibility.
  </action>
  <verify>
```bash
source .venv/bin/activate
python -c "from altgrad.training.data import prepare_eurlex, get_batch; print('imports ok')"
```
  </verify>
  <done>Module imports successfully with prepare_eurlex and get_batch functions</done>
</task>

<task type="auto">
  <name>Task 2: Prepare EurLex binary files and test loading</name>
  <files>
    data/eurlex/train.bin
    data/eurlex/val.bin
    tests/test_data.py
  </files>
  <action>
1. Run data preparation to create binary files:
   ```python
   from altgrad.training.data import prepare_eurlex
   stats = prepare_eurlex("data/eurlex")
   print(f"Train tokens: {stats['train']}, Val tokens: {stats['validation']}")
   ```

2. Create `tests/test_data.py` with tests:
   - `test_binary_files_exist`: Verify train.bin and val.bin exist after prepare
   - `test_binary_file_dtype`: Verify files are uint16
   - `test_get_batch_shapes`: get_batch returns (batch_size, block_size) tensors
   - `test_get_batch_target_shift`: y[i] == x[i+1] for autoregressive training
   - `test_get_batch_device`: Tensors are on correct device (cpu for tests)
   - `test_get_batch_reproducibility`: Same random state gives same batch

Use small block_size=64 and batch_size=4 for tests to keep them fast.

Note: If EurLex download is slow or fails, tests should skip gracefully with @pytest.mark.skipif checking for data existence.
  </action>
  <verify>
```bash
source .venv/bin/activate
pytest tests/test_data.py -v
```
  </verify>
  <done>Binary files exist in data/eurlex/, all data loading tests pass</done>
</task>

</tasks>

<verification>
1. `data/eurlex/train.bin` exists and is non-empty
2. `data/eurlex/val.bin` exists and is non-empty
3. `pytest tests/test_data.py` passes all tests
4. `get_batch("train", ...)` returns correctly shaped tensors
</verification>

<success_criteria>
- EurLex tokenized and stored as memory-mapped uint16 binary
- get_batch() loads random batches efficiently
- Train/val splits accessible for training loop
- Tests validate data integrity and loading correctness
</success_criteria>

<output>
After completion, create `.planning/phases/02-baseline-validation/02-01-SUMMARY.md`
</output>
