---
phase: 02-baseline-validation
plan: 03
type: execute
wave: 2
depends_on: ["02-02"]
files_modified:
  - altgrad/training/model.py
  - altgrad/training/trainer.py
  - altgrad/training/shadow.py
  - tests/test_model.py
autonomous: true

must_haves:
  truths:
    - "GPT model forward pass produces logits and loss"
    - "Model supports BF16 autocast context"
    - "FP32 shadow model tracks gradients for comparison"
    - "Trainer integrates model, data, optimizer, and logging"
    - "Gradient SNR comparison between FP8 and FP32 is logged"
  artifacts:
    - path: "altgrad/training/model.py"
      provides: "nanoGPT-style GPT model"
      exports: ["GPT", "GPTConfig"]
    - path: "altgrad/training/trainer.py"
      provides: "Training loop with quantization hooks"
      exports: ["Trainer"]
    - path: "altgrad/training/shadow.py"
      provides: "FP32 shadow model for gradient comparison"
      exports: ["FP32ShadowModel"]
  key_links:
    - from: "altgrad/training/trainer.py"
      to: "altgrad/training/model.py"
      via: "GPT model instantiation"
      pattern: "GPT\\(|model\\.forward"
    - from: "altgrad/training/trainer.py"
      to: "altgrad/training/callbacks.py"
      via: "WandbTracker for logging"
      pattern: "WandbTracker|tracker\\.log"
    - from: "altgrad/training/shadow.py"
      to: "altgrad/training/metrics.py"
      via: "gradient_cosine_similarity"
      pattern: "gradient_cosine_similarity"
---

<objective>
nanoGPT-style model and training loop with FP32 shadow for gradient comparison.

Purpose: Provide the model architecture and training orchestration needed to run baseline experiments. The trainer integrates data loading, model forward/backward, gradient statistics, stability monitoring, checkpointing, and W&B logging into a single coherent loop.

Output: GPT model class, FP32 shadow model for gradient comparison, and Trainer class that runs the full training loop.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-baseline-validation/02-RESEARCH.md
@.planning/phases/02-baseline-validation/02-CONTEXT.md
@.planning/phases/02-baseline-validation/02-02-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: GPT model implementation</name>
  <files>altgrad/training/model.py</files>
  <action>
Implement nanoGPT-style GPT model. Fork the essential structure from nanoGPT/model.py (~300 lines).

**altgrad/training/model.py:**

1. `@dataclass GPTConfig`:
   - n_layer, n_head, n_embd, block_size, vocab_size
   - dropout (default 0.0), bias (default True)

2. `class CausalSelfAttention(nn.Module)`:
   - Multi-head attention with causal mask
   - Key, query, value projections
   - Output projection
   - Dropout on attention weights and output

3. `class MLP(nn.Module)`:
   - Two-layer feedforward: n_embd -> 4*n_embd -> n_embd
   - GELU activation
   - Dropout

4. `class Block(nn.Module)`:
   - LayerNorm -> Attention -> residual
   - LayerNorm -> MLP -> residual

5. `class GPT(nn.Module)`:
   - Token embedding + position embedding
   - Stack of Block layers
   - Final LayerNorm
   - Language model head (weight tied to token embedding)
   - `forward(idx, targets=None) -> (logits, loss)`:
     - If targets provided, compute cross-entropy loss
     - Otherwise, return (logits, None)
   - `configure_optimizers(weight_decay, learning_rate, betas, device_type) -> Optimizer`:
     - Separate weight decay for 2D params (matmul weights) vs no decay for 1D (biases, norms)
     - Return AdamW optimizer

Key differences from full nanoGPT:
- No Flash Attention (use standard scaled_dot_product_attention or manual)
- No generation methods (training only)
- Keep it under 200 lines

Use `torch.nn.functional.scaled_dot_product_attention` if available (PyTorch 2.0+), otherwise manual attention.
  </action>
  <verify>
```bash
source .venv/bin/activate
python -c "
from altgrad.training.model import GPT, GPTConfig
config = GPTConfig(n_layer=2, n_head=2, n_embd=64, block_size=32, vocab_size=100)
model = GPT(config)
import torch
x = torch.randint(0, 100, (2, 32))
logits, loss = model(x, x)
print(f'logits shape: {logits.shape}, loss: {loss.item():.4f}')
"
```
  </verify>
  <done>GPT model produces correct logits shape and computes loss</done>
</task>

<task type="auto">
  <name>Task 2: FP32 shadow model with SNR comparison</name>
  <files>
    altgrad/training/shadow.py
    altgrad/training/trainer.py
  </files>
  <action>
Implement FP32 shadow model with gradient SNR comparison and main training loop.

**altgrad/training/shadow.py:**

1. `class FP32ShadowModel`:
   - `__init__(model: nn.Module)`:
     - Deep copy model to FP32
     - Enable gradients on shadow parameters
   - `sync_weights(source_model: nn.Module)`:
     - Copy weights from source to shadow (dequantized/float32)
   - `forward_backward(x, y) -> loss`:
     - Zero gradients, forward pass, backward pass
     - Return loss for logging
   - `compute_gradient_similarity(quantized_model: nn.Module) -> dict`:
     - Use gradient_cosine_similarity from metrics.py for cosine similarity
     - **CRITICAL: Compute per-layer gradient SNR comparison:**
       ```python
       def _compute_snr(grad: torch.Tensor) -> float:
           """SNR = |mean| / std, handles zero std."""
           mean = grad.abs().mean().item()
           std = grad.std().item()
           return mean / std if std > 1e-8 else float('inf')
       ```
     - For each layer with gradients in both models:
       - `snr_fp8 = _compute_snr(quantized_model_grad)`
       - `snr_fp32 = _compute_snr(shadow_model_grad)`
       - `snr_diff = snr_fp8 - snr_fp32`  (positive = FP8 noisier)
       - `snr_ratio = snr_fp8 / snr_fp32 if snr_fp32 > 0 else 1.0`
     - Return dict with:
       - Per-layer cosine similarities (existing)
       - Aggregate mean cosine similarity (existing)
       - **NEW:** `grad_snr/{layer_name}_fp8`, `grad_snr/{layer_name}_fp32`, `grad_snr/{layer_name}_diff`
       - **NEW:** `grad_snr/mean_fp8`, `grad_snr/mean_fp32`, `grad_snr/mean_diff`

Memory note: Shadow doubles parameter memory (~400MB for 50M params) but NOT activation memory.

**altgrad/training/trainer.py:**

1. `class Trainer`:
   - `__init__(config: TrainConfig, model: GPT, data_dir: str, device: str)`:
     - Store config, model, device
     - Create optimizer via model.configure_optimizers()
     - Create GradScaler for mixed precision
     - Create CheckpointManager
     - Create WandbTracker (if not disabled)
     - Create FP32ShadowModel if config.use_shadow
     - Create BitStallDetector from altgrad.quantization if config.use_fp8
     - Initialize step counter, best_val_loss

   - `train_step(x, y) -> dict`:
     - Forward with autocast (bfloat16 if not use_fp8, else float32 for simulated quant)
     - Compute loss, backward with scaler
     - If shadow: run shadow forward/backward, compute gradient similarity (includes SNR)
     - Compute gradient stats
     - Gradient clipping
     - Optimizer step, scaler update
     - Return metrics dict

   - `eval_step() -> float`:
     - Evaluate on validation set (multiple batches)
     - Return mean validation loss

   - `train(max_steps: int = None)`:
     - Main loop over steps (or config.max_steps)
     - Get batch, train_step, log metrics
     - Check alerts (NaN, bit-stall, overflow)
     - Save checkpoint on interval or anomaly
     - Eval on eval_interval
     - Track best validation loss

   - `_log_metrics(step: int, train_metrics: dict, val_loss: float = None)`:
     - Combine all metrics: loss, perplexity, gradient stats, stability metrics
     - **Include SNR metrics from shadow.compute_gradient_similarity() when use_shadow**
     - Add learning rate, throughput (tokens/sec)
     - Log via tracker

   - `resume(checkpoint_path: str)`:
     - Load checkpoint, restore state
     - Continue W&B run with same run_id

Perplexity: exp(loss) for language modeling.
Throughput: batch_size * block_size / step_time tokens/sec.
  </action>
  <verify>
```bash
source .venv/bin/activate
python -c "
from altgrad.training.model import GPT, GPTConfig
from altgrad.training.trainer import Trainer
from altgrad.training.config import TrainConfig
print('Trainer imports successfully')
"
```
  </verify>
  <done>Trainer class imports and has train/eval methods; shadow computes both cosine similarity and SNR</done>
</task>

<task type="auto">
  <name>Task 3: Model and trainer tests</name>
  <files>
    tests/test_model.py
    altgrad/training/__init__.py
  </files>
  <action>
Create tests for model and trainer, update package exports.

**tests/test_model.py:**

GPT model tests:
- `test_gpt_forward_shape`: Output shape is (batch, seq_len, vocab_size)
- `test_gpt_forward_with_targets`: Returns loss when targets provided
- `test_gpt_forward_no_targets`: Returns None loss when no targets
- `test_gpt_causal_mask`: Future tokens don't affect current predictions
- `test_gpt_configure_optimizers`: Returns AdamW with correct param groups

Shadow model tests:
- `test_shadow_init_copies_weights`: Shadow has same initial weights
- `test_shadow_sync_weights`: sync_weights copies current weights
- `test_shadow_gradient_similarity_identical`: Same model gives similarity ~1.0
- `test_shadow_gradient_similarity_after_update`: Diverges after training steps
- **NEW:** `test_shadow_snr_comparison`: Verify SNR metrics returned
  ```python
  def test_shadow_snr_comparison():
      """SNR metrics computed for FP8 vs FP32 gradient comparison."""
      model = GPT(tiny_config)
      shadow = FP32ShadowModel(model)
      # Run forward/backward on both
      x, y = torch.randint(0, 100, (2, 16)), torch.randint(0, 100, (2, 16))
      model(x, y)[1].backward()
      shadow.forward_backward(x, y)
      metrics = shadow.compute_gradient_similarity(model)
      # Verify SNR keys present
      assert 'grad_snr/mean_fp8' in metrics
      assert 'grad_snr/mean_fp32' in metrics
      assert 'grad_snr/mean_diff' in metrics
      # SNR should be positive (mean/std)
      assert metrics['grad_snr/mean_fp8'] > 0
      assert metrics['grad_snr/mean_fp32'] > 0
  ```

Trainer tests (mock W&B, use tiny model):
- `test_trainer_init`: Creates all components
- `test_trainer_train_step`: Returns metrics dict with expected keys
- `test_trainer_checkpoint_save_restore`: Can resume from checkpoint
- Use small config: n_layer=1, n_head=1, n_embd=32, block_size=16, vocab_size=100, batch_size=2

Use @pytest.fixture for common setup (tiny model, mock data).

**Update altgrad/training/__init__.py:**
Add exports for GPT, GPTConfig, FP32ShadowModel, Trainer.
  </action>
  <verify>
```bash
source .venv/bin/activate
pytest tests/test_model.py -v
```
  </verify>
  <done>All model and trainer tests pass, including SNR comparison test</done>
</task>

</tasks>

<verification>
1. `GPT(config).forward(x, x)` returns (logits, loss) with correct shapes
2. `FP32ShadowModel` computes gradient similarity between models
3. `FP32ShadowModel.compute_gradient_similarity()` returns SNR metrics (grad_snr/mean_fp8, grad_snr/mean_fp32, grad_snr/mean_diff)
4. `Trainer` integrates all components and runs training steps
5. `pytest tests/test_model.py` passes all tests including SNR test
</verification>

<success_criteria>
- GPT model matches nanoGPT architecture (transformer blocks, causal attention)
- FP32 shadow enables per-step gradient comparison for FP8 runs
- Shadow computes both cosine similarity AND SNR comparison (GRAD-02 requirement)
- Trainer orchestrates training loop with logging, checkpointing, and alerts
- All components tested and working together
</success_criteria>

<output>
After completion, create `.planning/phases/02-baseline-validation/02-03-SUMMARY.md`
</output>
