---
phase: 02-baseline-validation
plan: 04
type: execute
wave: 3
depends_on: ["02-01", "02-03"]
files_modified:
  - experiments/configs/bf16_baseline.yaml
  - experiments/run_experiment.py
autonomous: false

must_haves:
  truths:
    - "BF16 baseline trains on EurLex without divergence"
    - "Loss decreases over training steps (convergence trend visible)"
    - "All metrics log to W&B every step"
    - "Checkpoint saved every 100 steps"
    - "Validation loss and perplexity tracked"
  artifacts:
    - path: "experiments/configs/bf16_baseline.yaml"
      provides: "BF16 baseline experiment configuration"
    - path: "experiments/run_experiment.py"
      provides: "Experiment runner script"
      exports: ["main"]
  key_links:
    - from: "experiments/run_experiment.py"
      to: "altgrad/training/trainer.py"
      via: "Trainer instantiation and train()"
      pattern: "Trainer\\(|trainer\\.train"
    - from: "experiments/run_experiment.py"
      to: "experiments/configs/*.yaml"
      via: "load_config"
      pattern: "load_config"
---

<objective>
Run BF16 baseline experiment on EurLex to establish reference metrics.

Purpose: Validate that the training infrastructure works end-to-end and establish baseline loss/perplexity curves that FP8 experiments will compare against. This is the "known good" reference.

Output: Completed W&B run with BF16 training curves, checkpoint files, and experiment configuration.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-baseline-validation/02-RESEARCH.md
@.planning/phases/02-baseline-validation/02-CONTEXT.md
@.planning/phases/02-baseline-validation/02-01-SUMMARY.md
@.planning/phases/02-baseline-validation/02-03-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create experiment runner and BF16 config</name>
  <files>
    experiments/configs/bf16_baseline.yaml
    experiments/run_experiment.py
  </files>
  <action>
Create experiment infrastructure and BF16 baseline configuration.

**experiments/configs/bf16_baseline.yaml:**

```yaml
# BF16 Baseline Experiment
# Reference run for comparing FP8 formats

# Model architecture (small for budget constraint)
n_layer: 6
n_head: 6
n_embd: 384
block_size: 256
vocab_size: 50257  # GPT-2 vocab
dropout: 0.0

# Training
batch_size: 12
learning_rate: 6e-4
max_steps: 2000  # Short run for convergence trend
warmup_steps: 100
grad_clip: 1.0

# Precision
use_fp8: false
use_shadow: false  # No shadow needed for BF16 baseline

# Checkpointing
checkpoint_interval: 100
checkpoint_dir: "checkpoints/bf16_baseline"
max_checkpoints: 3

# Stability thresholds
nan_patience: 10
bit_stall_threshold: 0.5
overflow_threshold: 0.01
dead_neuron_threshold: 1.0e-8
dead_neuron_window: 100

# Logging
log_interval: 1
eval_interval: 100

# W&B
project: "altgrad"
run_name: "bf16-baseline"
tags: ["baseline", "bf16", "eurlex"]

# Reproducibility
seed: 42
```

Model size: ~10M params (6 layers, 384 dim, 6 heads) - well under 50M limit.

**experiments/run_experiment.py:**

```python
#!/usr/bin/env python
"""Run AltGrad training experiment."""
import argparse
import os
import sys

# Add project root to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from altgrad.training import TrainConfig, load_config, GPT, GPTConfig, Trainer

def main():
    parser = argparse.ArgumentParser(description="Run AltGrad experiment")
    parser.add_argument("config", help="Path to experiment config YAML")
    parser.add_argument("--data-dir", default="data/eurlex", help="Data directory")
    parser.add_argument("--resume", help="Checkpoint to resume from")
    parser.add_argument("--device", default="cuda" if torch.cuda.is_available() else "cpu")
    args = parser.parse_args()

    # Load config
    config = load_config(args.config)

    # Set seed for reproducibility
    set_seed(config.seed)

    # Create model
    gpt_config = GPTConfig(
        n_layer=config.n_layer,
        n_head=config.n_head,
        n_embd=config.n_embd,
        block_size=config.block_size,
        vocab_size=config.vocab_size,
        dropout=config.dropout,
    )
    model = GPT(gpt_config).to(args.device)

    # Create trainer
    trainer = Trainer(config, model, args.data_dir, args.device)

    # Resume if specified
    if args.resume:
        trainer.resume(args.resume)

    # Train
    trainer.train()

def set_seed(seed: int):
    import random
    import numpy as np
    import torch
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

if __name__ == "__main__":
    import torch
    main()
```

Make script executable: `chmod +x experiments/run_experiment.py`
  </action>
  <verify>
```bash
source .venv/bin/activate
python experiments/run_experiment.py --help
python -c "from altgrad.training import load_config; c = load_config('experiments/configs/bf16_baseline.yaml'); print(f'Config loaded: {c.run_name}')"
```
  </verify>
  <done>Experiment runner and BF16 config created and loadable</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
BF16 baseline training run on EurLex dataset with full W&B logging.

This experiment:
- Trains a 10M parameter GPT model on EurLex legal documents
- Uses BF16 mixed precision (standard, no FP8)
- Logs all metrics to W&B every step
- Saves checkpoints every 100 steps
- Runs for 2000 steps to see convergence trend
  </what-built>
  <how-to-verify>
1. The experiment will be started with:
   ```bash
   source .venv/bin/activate
   python experiments/run_experiment.py experiments/configs/bf16_baseline.yaml
   ```

2. During training, verify in terminal:
   - Loss decreases over first few hundred steps
   - No NaN warnings
   - Checkpoints saved at step 100, 200, etc.

3. After training (or during), check W&B dashboard:
   - Go to wandb.ai and find the "altgrad" project
   - Open the "bf16-baseline" run
   - Verify loss curve shows downward trend
   - Verify perplexity decreases
   - Verify gradient norms are logged per layer
   - Verify no stability alerts fired

4. Check checkpoints exist:
   ```bash
   ls -la checkpoints/bf16_baseline/
   ```
   Should see step_100.pt, step_200.pt, etc.

Expected behavior:
- Initial loss ~10-11 (random initialization)
- Loss should drop to ~5-7 by step 2000
- Perplexity should decrease from ~20000+ to ~200-1000
- No NaN or divergence

Note: First run on H100 - ensure CUDA is available and W&B is logged in (`wandb login`).
  </how-to-verify>
  <resume-signal>Type "baseline verified" if training completes successfully with decreasing loss, or describe any issues observed</resume-signal>
</task>

</tasks>

<verification>
1. `experiments/run_experiment.py` runs without error
2. W&B run shows decreasing loss curve
3. Checkpoints saved in `checkpoints/bf16_baseline/`
4. No NaN or divergence during training
5. Perplexity metric tracked and decreasing
</verification>

<success_criteria>
- BF16 baseline completes 2000 steps without divergence
- Loss curve shows clear convergence trend
- All stability metrics logged to W&B
- Checkpoints enable restart capability
- Baseline established for FP8 comparison
</success_criteria>

<output>
After completion, create `.planning/phases/02-baseline-validation/02-04-SUMMARY.md`
</output>
