---
phase: 02-baseline-validation
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - altgrad/training/config.py
  - altgrad/training/metrics.py
  - altgrad/training/checkpoint.py
  - altgrad/training/callbacks.py
  - tests/test_training_infra.py
autonomous: true

must_haves:
  truths:
    - "Experiment config is loadable from YAML"
    - "Gradient statistics compute per-layer (norms, SNR, dead fraction)"
    - "Checkpoints save/load model + optimizer + quantization state + RNG"
    - "W&B logging tracks all metrics with explicit step numbers"
    - "W&B alerts fire on stability threshold breaches"
  artifacts:
    - path: "altgrad/training/config.py"
      provides: "Experiment configuration dataclasses"
      exports: ["TrainConfig", "load_config"]
    - path: "altgrad/training/metrics.py"
      provides: "Gradient and stability metrics computation"
      exports: ["compute_gradient_stats", "compute_stability_metrics"]
    - path: "altgrad/training/checkpoint.py"
      provides: "Checkpoint save/load with quantization state"
      exports: ["save_checkpoint", "load_checkpoint", "CheckpointManager"]
    - path: "altgrad/training/callbacks.py"
      provides: "W&B logging and alerts"
      exports: ["WandbTracker"]
  key_links:
    - from: "altgrad/training/callbacks.py"
      to: "wandb"
      via: "wandb.init and wandb.log"
      pattern: "wandb\\.(init|log|alert)"
    - from: "altgrad/training/checkpoint.py"
      to: "torch.save/load"
      via: "PyTorch checkpoint API"
      pattern: "torch\\.(save|load)"
---

<objective>
Training infrastructure: configuration, metrics, checkpointing, and W&B integration.

Purpose: Establish the monitoring and state management foundation that all training runs depend on. This enables per-step logging, stability alerts, and resumable training.

Output: Config dataclasses, gradient/stability metrics computation, checkpoint management, and W&B tracker with alerts.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-baseline-validation/02-RESEARCH.md
@.planning/phases/02-baseline-validation/02-CONTEXT.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Configuration and metrics modules</name>
  <files>
    altgrad/training/config.py
    altgrad/training/metrics.py
  </files>
  <action>
Create configuration and metrics modules.

**altgrad/training/config.py:**

1. `@dataclass TrainConfig`:
   - Model: n_layer, n_head, n_embd, block_size, vocab_size, dropout
   - Training: batch_size, learning_rate, max_steps, warmup_steps, grad_clip
   - Quantization: use_fp8, fp8_format (default "E5M2"), use_shadow (for FP32 comparison)
   - Checkpointing: checkpoint_interval (default 100), checkpoint_dir, max_checkpoints (default 3)
   - Stability thresholds: nan_patience (default 10), bit_stall_threshold (default 0.5), overflow_threshold (default 0.01), dead_neuron_threshold (default 1e-8), dead_neuron_window (default 100)
   - Logging: log_interval (default 1), eval_interval (default 100)
   - W&B: project, run_name, tags
   - Reproducibility: seed

2. `load_config(path: str) -> TrainConfig`: Load from YAML file
3. `save_config(config: TrainConfig, path: str)`: Save to YAML file

**altgrad/training/metrics.py:**

1. `compute_gradient_stats(model: nn.Module, threshold: float = 1e-8) -> dict`:
   - Per-layer L2 norm: `grad_norm_l2/{name}`
   - Per-layer Linf norm: `grad_norm_linf/{name}`
   - Per-layer dead neuron fraction (grad < threshold): `dead_neuron_frac/{name}`
   - Per-layer SNR (|mean| / std): `grad_snr/{name}`
   - Aggregate: mean/min across layers

2. `compute_stability_metrics(model: nn.Module, detector: BitStallDetector = None) -> dict`:
   - NaN count in parameters: `param_nan_count`
   - Inf count in parameters: `param_inf_count`
   - If detector provided: `bit_stall_rate` from detector.stall_rate()
   - Zero-update fraction (params unchanged): tracked separately in training loop

3. `gradient_cosine_similarity(model_a: nn.Module, model_b: nn.Module) -> dict`:
   - Per-layer cosine similarity: `grad_cos_sim/{name}`
   - Aggregate mean and min: `grad_cos_sim/mean`, `grad_cos_sim/min`
   - Uses F.cosine_similarity on flattened gradients
  </action>
  <verify>
```bash
source .venv/bin/activate
python -c "from altgrad.training.config import TrainConfig, load_config; from altgrad.training.metrics import compute_gradient_stats; print('imports ok')"
```
  </verify>
  <done>Config and metrics modules import with all expected exports</done>
</task>

<task type="auto">
  <name>Task 2: Checkpoint and W&B callback modules</name>
  <files>
    altgrad/training/checkpoint.py
    altgrad/training/callbacks.py
    altgrad/training/__init__.py
  </files>
  <action>
Create checkpoint management and W&B integration.

**altgrad/training/checkpoint.py:**

1. `save_checkpoint(filepath, model, optimizer, scaler, step, config, quantization_state=None)`:
   - Save model_state_dict, optimizer_state_dict, scaler_state_dict (if scaler)
   - Save step, config (as dict)
   - Save quantization_state (amax_history, scale_factors from altgrad.quantization)
   - Save RNG state: random, numpy, torch, cuda (all devices)

2. `load_checkpoint(filepath, model, optimizer, scaler=None) -> tuple[int, dict, dict]`:
   - Load and restore all state
   - Restore RNG state for reproducibility
   - Return (step, config, quantization_state)

3. `class CheckpointManager`:
   - `__init__(checkpoint_dir, max_checkpoints=3)`: Track checkpoints, retain best + last N-1
   - `save(step, model, optimizer, scaler, config, val_loss, quantization_state=None)`:
     - Save to `{checkpoint_dir}/step_{step}.pt`
     - Track if best (lowest val_loss), save as `best.pt`
     - Delete old checkpoints beyond max_checkpoints (keep best separately)
   - `save_on_anomaly(step, ...)`: Emergency save to `anomaly_{step}.pt`
   - `latest() -> str`: Return path to most recent checkpoint
   - `best() -> str`: Return path to best checkpoint

**altgrad/training/callbacks.py:**

1. `class WandbTracker`:
   - `__init__(config: TrainConfig, run_id: str = None)`:
     - wandb.init with project, config, resume="allow" if run_id provided
     - Initialize consecutive_nan_steps counter, loss_history
   - `log_step(step: int, metrics: dict)`:
     - wandb.log(metrics, step=step) - explicit step for alignment
   - `check_alerts(step: int, metrics: dict, config: TrainConfig) -> str`:
     - Check NaN: increment counter, alert at nan_patience, return 'stop' or 'save_checkpoint'
     - Check bit_stall_rate > threshold: AlertLevel.WARN, wait_duration=300
     - Check overflow_rate > threshold: AlertLevel.WARN, wait_duration=300
     - Return 'continue', 'save_checkpoint', or 'stop'
   - `finish()`: wandb.finish()
   - Property `run_id`: Return current run ID for checkpoint saving

Use AlertLevel from wandb for alert levels.

**altgrad/training/__init__.py:**
Export all public APIs from submodules.
  </action>
  <verify>
```bash
source .venv/bin/activate
python -c "from altgrad.training import TrainConfig, save_checkpoint, load_checkpoint, CheckpointManager, WandbTracker; print('imports ok')"
```
  </verify>
  <done>All training infrastructure modules import successfully</done>
</task>

<task type="auto">
  <name>Task 3: Infrastructure tests</name>
  <files>tests/test_training_infra.py</files>
  <action>
Create comprehensive tests for training infrastructure.

**tests/test_training_infra.py:**

Config tests:
- `test_config_defaults`: TrainConfig has sensible defaults
- `test_config_yaml_roundtrip`: save_config then load_config preserves values
- `test_config_fp8_options`: use_fp8 and fp8_format work correctly

Metrics tests (use simple nn.Linear model with fake gradients):
- `test_gradient_stats_shapes`: Returns dict with expected keys
- `test_gradient_l2_norm`: Correct L2 norm computation
- `test_gradient_dead_fraction`: Correctly identifies gradients below threshold
- `test_gradient_snr`: Signal-to-noise ratio computed correctly
- `test_cosine_similarity_identical`: Identical gradients give similarity ~1.0
- `test_cosine_similarity_opposite`: Opposite gradients give similarity ~-1.0

Checkpoint tests (use tmp_path fixture):
- `test_checkpoint_save_load`: Model state restored correctly
- `test_checkpoint_rng_state`: RNG state restored (same random sequence after load)
- `test_checkpoint_quantization_state`: Custom quantization_state preserved
- `test_checkpoint_manager_max_checkpoints`: Old checkpoints deleted
- `test_checkpoint_manager_best_tracking`: Best checkpoint updated on improvement

W&B tests (mock wandb to avoid actual API calls):
- `test_wandb_tracker_init`: Initializes with config
- `test_wandb_log_step`: Calls wandb.log with explicit step
- `test_wandb_alert_nan`: Triggers alert after nan_patience exceeded
- `test_wandb_alert_bit_stall`: Triggers alert on high stall rate
- Use unittest.mock.patch for wandb module
  </action>
  <verify>
```bash
source .venv/bin/activate
pytest tests/test_training_infra.py -v
```
  </verify>
  <done>All training infrastructure tests pass</done>
</task>

</tasks>

<verification>
1. `from altgrad.training import TrainConfig, load_config, save_config` works
2. `from altgrad.training import compute_gradient_stats, compute_stability_metrics` works
3. `from altgrad.training import save_checkpoint, load_checkpoint, CheckpointManager` works
4. `from altgrad.training import WandbTracker` works
5. `pytest tests/test_training_infra.py` passes all tests
</verification>

<success_criteria>
- Configuration loads from YAML and supports all required fields
- Gradient statistics compute per-layer metrics correctly
- Checkpoints save/restore full training state including RNG
- W&B tracker logs with explicit steps and fires alerts on thresholds
- All infrastructure tested without requiring actual W&B connection
</success_criteria>

<output>
After completion, create `.planning/phases/02-baseline-validation/02-02-SUMMARY.md`
</output>
