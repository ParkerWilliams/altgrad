---
phase: 08-update-metrics-test-matrix
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - altgrad/training/optimizer.py
  - tests/test_optimizer.py
autonomous: true

must_haves:
  truths:
    - "GridOptim maintains FP32 master weights separate from model FP8 weights"
    - "Grid is built from all representable FP8 values via torch.arange(-128,128).view(DTYPE)"
    - "Stochastic rounding uses floor(v_rungs + rand_like(v_rungs))"
    - "Rung clipping clamps v_rungs to [-10, 10] before rounding"
    - "step() returns flip count (number of FP8 values changed)"
  artifacts:
    - path: "altgrad/training/optimizer.py"
      provides: "GridOptim class with rung-based updates"
      contains: "class GridOptim"
    - path: "tests/test_optimizer.py"
      provides: "Tests for GridOptim"
      contains: "test_grid_optim"
  key_links:
    - from: "altgrad/training/optimizer.py"
      to: "torch.searchsorted"
      via: "Grid index lookup"
      pattern: "searchsorted.*grid"
    - from: "altgrad/training/optimizer.py"
      to: "FP8 dtype"
      via: "Grid construction"
      pattern: "arange.*view.*float8"
---

<objective>
Implement GridOptim: rung-based optimizer with FP32 master weights and stochastic rounding

Purpose: Provide reference implementation for grid-based FP8 optimization following the MasterOptim pattern - explicit grid, stochastic rounding in rung space, proper clipping
Output: GridOptim class in optimizer.py with grid construction, momentum, stochastic rounding, and flip counting
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08-update-metrics-test-matrix/08-RESEARCH.md
@.planning/phases/08-update-metrics-test-matrix/reference_optimizer.py

# Existing optimizer module
@altgrad/training/optimizer.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement GridOptim class</name>
  <files>altgrad/training/optimizer.py</files>
  <action>
Add GridOptim class to optimizer.py following the reference_optimizer.py pattern:

```python
class GridOptim:
    """Grid-based optimizer with FP32 master weights and stochastic rounding.

    Maintains FP32 master weights and projects to FP8 grid using stochastic
    rounding in rung space. This approach gives explicit control over discrete
    weight transitions.

    Args:
        params: Iterable of model parameters
        scale: Learning rate in rung units (default: 6.0)
        momentum: SGD momentum coefficient (default: 0.9)
        weight_decay: L2 penalty coefficient (default: 1e-4)
        rung_clip: Maximum rung movement per step (default: 10)
        fp8_dtype: FP8 dtype for grid construction (default: torch.float8_e4m3fn)
        device: Device for grid tensor (default: "cuda" if available)

    Example:
        >>> optimizer = GridOptim(model.parameters(), scale=6.0)
        >>> flips, updates = optimizer.step()
    """
```

Implementation details:

1. `__init__(self, params, scale=6.0, momentum=0.9, weight_decay=1e-4, rung_clip=10, fp8_dtype=None, device=None)`:
   - self.params = list(params)
   - self.scale, self.momentum, self.wd, self.rung_clip = scale, momentum, weight_decay, rung_clip
   - Create master weights: `self.master_p = [p.detach().clone().float().to(device) for p in self.params]`
   - Create velocity: `self.velocity = [torch.zeros_like(p) for p in self.master_p]`
   - Build grid using dtype (default torch.float8_e4m3fn if available, else skip grid):
     ```python
     fp8_dtype = fp8_dtype or getattr(torch, 'float8_e4m3fn', None)
     if fp8_dtype is not None:
         raw_bits = torch.arange(-128, 128, dtype=torch.int8)
         all_floats = raw_bits.view(fp8_dtype).to(torch.float32)
         clean = all_floats[~torch.isnan(all_floats) & ~torch.isinf(all_floats)]
         self.grid = torch.sort(torch.unique(clean))[0].to(device)
     else:
         self.grid = None  # Fallback for testing without FP8 support
     ```

2. `@torch.no_grad() step(self, current_scale=None) -> Tuple[int, int]`:
   - Returns (flips, updates) where:
     - flips = number of FP8 values that changed
     - updates = number of non-zero gradients applied

   Implementation:
   ```python
   flips, updates = 0, 0
   scale = current_scale if current_scale is not None else self.scale

   for i, p in enumerate(self.params):
       if p.grad is None:
           continue

       old_data = p.data.clone()
       grad = p.grad.to(torch.float32)

       # Count updates (non-zero gradient elements)
       updates += (grad.abs() > 1e-10).sum().item()

       # Weight decay
       if self.wd != 0:
           grad.add_(self.master_p[i], alpha=self.wd)

       # Momentum
       self.velocity[i] = self.momentum * self.velocity[i] + grad

       if self.grid is not None:
           # Grid-based update
           indices = torch.searchsorted(self.grid, self.master_p[i].contiguous())
           v_rungs = self.velocity[i] * scale

           # CRITICAL: Clip to prevent NaN at grid boundaries
           v_rungs = torch.clamp(v_rungs, -self.rung_clip, self.rung_clip)

           # Stochastic rounding
           v_rounded = torch.floor(v_rungs + torch.rand_like(v_rungs)).to(torch.int32)

           # New grid indices (subtract for descent)
           new_indices = torch.clamp(indices - v_rounded, 0, len(self.grid) - 1)
           new_floats = self.grid[new_indices.long()].view(p.shape)

           self.master_p[i].copy_(new_floats)
           p.data.copy_(new_floats)
       else:
           # Fallback: Euclidean update
           self.master_p[i].sub_(scale * self.velocity[i])
           p.data.copy_(self.master_p[i])

       flips += (p.data != old_data).sum().item()

   return flips, updates
   ```

3. `zero_grad(self)`:
   - For each p in self.params: if p.grad is not None: p.grad.zero_()

4. Update `__all__` to include "GridOptim"
  </action>
  <verify>
Run: `python -c "from altgrad.training.optimizer import GridOptim; print('GridOptim imported successfully')"`
Expected: GridOptim imported successfully
  </verify>
  <done>
GridOptim class exists with grid construction, FP32 master weights, stochastic rounding, rung clipping, and returns (flips, updates) tuple
  </done>
</task>

<task type="auto">
  <name>Task 2: Add GridOptim tests</name>
  <files>tests/test_optimizer.py</files>
  <action>
Add tests for GridOptim to test_optimizer.py (create file if doesn't exist):

1. `test_grid_optim_init()`:
   - Create simple model (nn.Linear)
   - Create GridOptim with default params
   - Verify master_p created and matches initial weights
   - Verify velocity initialized to zeros
   - Verify grid created (or None if no FP8 support)

2. `test_grid_optim_step_returns_counts()`:
   - Create model with parameters
   - Set fake gradients on parameters
   - Call optimizer.step()
   - Verify returns tuple (flips, updates)
   - Verify updates > 0 (since we set non-zero grads)

3. `test_grid_optim_momentum()`:
   - Create optimizer with momentum=0.9
   - Do two steps with same gradient
   - Verify velocity accumulates (second step moves more)

4. `test_grid_optim_zero_grad()`:
   - Set gradients on parameters
   - Call optimizer.zero_grad()
   - Verify all grads are zeroed

5. `test_grid_optim_scale_override()`:
   - Call step(current_scale=1.0) vs step() with default scale=6.0
   - Verify different scale produces different movement

6. `test_grid_optim_rung_clipping()`:
   - Create optimizer with rung_clip=5
   - Set very large gradient
   - Verify rung movement is clipped (no NaN in weights)

Use pytest.mark.skipif for tests requiring FP8 dtype support:
```python
import pytest
HAS_FP8 = hasattr(torch, 'float8_e4m3fn')

@pytest.mark.skipif(not HAS_FP8, reason="Requires FP8 dtype support")
def test_grid_optim_with_fp8_grid():
    ...
```
  </action>
  <verify>
Run: `pytest tests/test_optimizer.py -v --tb=short`
Expected: All tests pass (some may skip if no FP8 support)
  </verify>
  <done>
6 test cases for GridOptim covering initialization, step counts, momentum, zero_grad, scale override, and rung clipping
  </done>
</task>

</tasks>

<verification>
1. `pytest tests/test_optimizer.py -v --tb=short` - All tests pass
2. `python -c "from altgrad.training import GridOptim"` - Import works
3. `python -c "import torch; from altgrad.training.optimizer import GridOptim; m = torch.nn.Linear(4,4); opt = GridOptim(m.parameters()); print('grid:', opt.grid is not None)"` - Optimizer creates
</verification>

<success_criteria>
- GridOptim class exists in altgrad/training/optimizer.py
- FP32 master weights maintained separately from model parameters
- Grid constructed from FP8 representable values (when FP8 dtype available)
- Stochastic rounding via floor(v_rungs + rand_like(v_rungs))
- Rung clipping clamps v_rungs to [-rung_clip, rung_clip]
- step() returns (flips, updates) tuple
- All 6 tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/08-update-metrics-test-matrix/08-02-SUMMARY.md`
</output>
