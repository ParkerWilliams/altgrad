---
phase: 05-manifold-aware-optimizer
plan: 02
type: execute
wave: 2
depends_on: ["05-01"]
files_modified:
  - altgrad/training/config.py
  - altgrad/training/trainer.py
  - altgrad/training/model.py
  - experiments/configs/e5m2_manifold.yaml
autonomous: true

must_haves:
  truths:
    - "TrainConfig has use_manifold_aware toggle"
    - "Trainer uses ManifoldAdamW when use_manifold_aware=True"
    - "Experiment config can specify manifold_aware mode"
    - "Manifold mode logs bit-position statistics"
  artifacts:
    - path: "altgrad/training/config.py"
      provides: "Manifold config options"
      contains: "use_manifold_aware"
    - path: "altgrad/training/trainer.py"
      provides: "ManifoldAdamW integration"
      contains: "ManifoldAdamW"
    - path: "experiments/configs/e5m2_manifold.yaml"
      provides: "Example manifold experiment config"
  key_links:
    - from: "altgrad/training/trainer.py"
      to: "altgrad/training/optimizer.py"
      via: "ManifoldAdamW import"
      pattern: "from altgrad.training.optimizer import ManifoldAdamW"
    - from: "altgrad/training/config.py"
      to: "altgrad/training/trainer.py"
      via: "config.use_manifold_aware"
      pattern: "config\\.use_manifold_aware"
---

<objective>
Integrate ManifoldAdamW optimizer into training infrastructure with config support and diagnostics.

Purpose: Enable experiments to toggle between standard and manifold-aware training via config, with proper bit-position logging for analysis.

Output: Updated config, trainer, and example experiment config ready for manifold-aware experiments.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-manifold-aware-optimizer/05-RESEARCH.md

# From Plan 01
@altgrad/training/optimizer.py

# Files to modify
@altgrad/training/config.py
@altgrad/training/trainer.py
@altgrad/training/model.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Update TrainConfig with manifold options</name>
  <files>altgrad/training/config.py</files>
  <action>
Add manifold-aware optimizer configuration options to TrainConfig dataclass.

Add these fields after the diagnostic sampling section (around line 119):

```python
    # Manifold-aware optimizer (Phase 5)
    use_manifold_aware: bool = False  # MANI-03: Toggle manifold-aware mode
    manifold_mantissa_bits: int = 2  # M for S = 2^(floor(log2|w|) - M), default E5M2
    manifold_max_stiffness: float = 1e6  # Clamp to prevent explosion
    log_bit_position: bool = False  # Log bit-position statistics
```

Update the `float_fields` list in `load_config()` to include `manifold_max_stiffness`:
```python
    float_fields = [
        "dropout",
        "learning_rate",
        # ... existing fields ...
        "manifold_max_stiffness",
    ]
```

Verify config loads correctly:
```bash
cd /Users/prwilliams/Repos/altgrad && source .venv/bin/activate && python -c "
from altgrad.training.config import TrainConfig, save_config, load_config
import tempfile, os

# Test default values
config = TrainConfig()
assert config.use_manifold_aware == False
assert config.manifold_mantissa_bits == 2
assert config.manifold_max_stiffness == 1e6
assert config.log_bit_position == False

# Test save/load roundtrip
with tempfile.NamedTemporaryFile(mode='w', suffix='.yaml', delete=False) as f:
    save_config(TrainConfig(use_manifold_aware=True, manifold_mantissa_bits=4), f.name)
    loaded = load_config(f.name)
    assert loaded.use_manifold_aware == True
    assert loaded.manifold_mantissa_bits == 4
    os.unlink(f.name)

print('TrainConfig manifold options: OK')
"
```
  </action>
  <verify>TrainConfig accepts manifold options and roundtrips through YAML</verify>
  <done>TrainConfig has use_manifold_aware, manifold_mantissa_bits, manifold_max_stiffness, log_bit_position fields</done>
</task>

<task type="auto">
  <name>Task 2: Integrate ManifoldAdamW into Trainer</name>
  <files>altgrad/training/trainer.py, altgrad/training/model.py</files>
  <action>
Modify Trainer to use ManifoldAdamW when config.use_manifold_aware=True.

**In trainer.py:**

1. Add import at top (after existing imports):
```python
from altgrad.training.optimizer import ManifoldAdamW
```

2. Modify the optimizer configuration in `__init__` (around line 104). Replace:
```python
        # Configure optimizer
        self.optimizer = model.configure_optimizers(
            weight_decay=0.1,
            learning_rate=config.learning_rate,
            betas=(0.9, 0.95),
            device_type=self.device_type,
        )
```

With:
```python
        # Configure optimizer
        if config.use_manifold_aware:
            # Use ManifoldAdamW for stiffness-preconditioned updates
            self.optimizer = self._configure_manifold_optimizer(model)
        else:
            self.optimizer = model.configure_optimizers(
                weight_decay=0.1,
                learning_rate=config.learning_rate,
                betas=(0.9, 0.95),
                device_type=self.device_type,
            )
```

3. Add helper method to Trainer class (after `__init__`):
```python
    def _configure_manifold_optimizer(self, model: nn.Module) -> ManifoldAdamW:
        """Configure ManifoldAdamW with proper parameter groups.

        Separates parameters into decay and no-decay groups matching
        the standard AdamW configuration from model.configure_optimizers().

        Args:
            model: The model to optimize

        Returns:
            Configured ManifoldAdamW optimizer
        """
        # Separate parameters into decay and no-decay groups
        decay = set()
        no_decay = set()

        for pn, p in model.named_parameters():
            if not p.requires_grad:
                continue
            # 2D params (weights) get decay, 1D params (biases, norms) don't
            if p.dim() >= 2:
                decay.add(pn)
            else:
                no_decay.add(pn)

        param_dict = {pn: p for pn, p in model.named_parameters() if p.requires_grad}

        optim_groups = [
            {"params": [param_dict[pn] for pn in sorted(decay)], "weight_decay": 0.1},
            {"params": [param_dict[pn] for pn in sorted(no_decay)], "weight_decay": 0.0},
        ]

        return ManifoldAdamW(
            optim_groups,
            lr=self.config.learning_rate,
            betas=(0.9, 0.95),
            manifold_aware=True,
            mantissa_bits=self.config.manifold_mantissa_bits,
            max_stiffness=self.config.manifold_max_stiffness,
        )
```

4. Add bit-position logging to `_log_metrics` method. Before the console logging (around line 494), add:
```python
        # Bit-position statistics (if manifold-aware)
        if self.config.use_manifold_aware and self.config.log_bit_position:
            bit_pos_stats = self._get_bit_position_stats()
            train_metrics.update(bit_pos_stats)
```

5. Add helper method for bit-position stats:
```python
    def _get_bit_position_stats(self) -> Dict[str, float]:
        """Get bit-position statistics from ManifoldAdamW state.

        Returns:
            Dictionary with bit-position statistics for logging.
        """
        if not isinstance(self.optimizer, ManifoldAdamW):
            return {}

        total_ulp = 0.0
        total_abs_ulp = 0.0
        count = 0

        for group in self.optimizer.param_groups:
            for p in group["params"]:
                if p in self.optimizer.state:
                    state = self.optimizer.state[p]
                    if "bit_position" in state:
                        bit_pos = state["bit_position"]
                        total_ulp += bit_pos.sum().item()
                        total_abs_ulp += bit_pos.abs().sum().item()
                        count += bit_pos.numel()

        if count == 0:
            return {}

        return {
            "manifold/bit_position_mean": total_ulp / count,
            "manifold/bit_position_abs_mean": total_abs_ulp / count,
        }
```

Verify integration:
```bash
cd /Users/prwilliams/Repos/altgrad && source .venv/bin/activate && python -c "
import torch
from altgrad.training import GPT, GPTConfig, TrainConfig, Trainer

# Create minimal config and model
config = TrainConfig(
    use_manifold_aware=True,
    manifold_mantissa_bits=2,
    max_steps=1,
    project='',  # Disable W&B
)
model_config = GPTConfig(n_layer=1, n_head=1, n_embd=32, vocab_size=100, block_size=16)
model = GPT(model_config)

# Create trainer (should use ManifoldAdamW)
from altgrad.training.optimizer import ManifoldAdamW
trainer = Trainer(config, model, 'data/eurlex', 'cpu')
assert isinstance(trainer.optimizer, ManifoldAdamW), f'Expected ManifoldAdamW, got {type(trainer.optimizer)}'
print('Trainer ManifoldAdamW integration: OK')
"
```
  </action>
  <verify>Trainer uses ManifoldAdamW when use_manifold_aware=True</verify>
  <done>Trainer integrates ManifoldAdamW with proper parameter groups and bit-position logging</done>
</task>

<task type="auto">
  <name>Task 3: Create example manifold experiment config</name>
  <files>experiments/configs/e5m2_manifold.yaml</files>
  <action>
Create an experiment config demonstrating manifold-aware training with E5M2.

```yaml
# E5M2 format with manifold-aware optimizer
# Compares stiffness-preconditioned updates vs standard training

# Model architecture (10M params)
n_layer: 6
n_head: 6
n_embd: 384
block_size: 256
vocab_size: 50304
dropout: 0.0

# Training hyperparameters
batch_size: 64
learning_rate: 3e-4
max_steps: 500
warmup_steps: 50
grad_clip: 1.0

# Quantization settings
use_fp8: true
fp8_format: "E5M2"
use_shadow: true  # Compare with FP32 reference

# Manifold-aware optimizer (Phase 5)
use_manifold_aware: true
manifold_mantissa_bits: 2  # E5M2 has 2 mantissa bits
manifold_max_stiffness: 1000000.0
log_bit_position: true

# Checkpointing
checkpoint_interval: 100
checkpoint_dir: "checkpoints/e5m2_manifold"
max_checkpoints: 3

# Stability thresholds
nan_patience: 10
bit_stall_threshold: 0.5
overflow_threshold: 0.01
dead_neuron_threshold: 1.0e-8
dead_neuron_window: 100

# Stability interventions
enable_partition_clipping: true
partition_clip_base: 1.0

# Diagnostic sampling
diagnostic_interval: 50
log_stiffness: true
log_grid_alignment: true
log_ulp: true

# Logging
log_interval: 10
eval_interval: 100

# W&B
project: "altgrad"
run_name: "e5m2-manifold-aware"
tags:
  - "e5m2"
  - "manifold-aware"
  - "phase5"

# Reproducibility
seed: 42
```

Also create a standard (non-manifold) comparison config for A/B testing:

Create `experiments/configs/e5m2_standard.yaml`:
```yaml
# E5M2 format with standard optimizer
# Baseline for comparison with manifold-aware training

# Model architecture (10M params)
n_layer: 6
n_head: 6
n_embd: 384
block_size: 256
vocab_size: 50304
dropout: 0.0

# Training hyperparameters
batch_size: 64
learning_rate: 3e-4
max_steps: 500
warmup_steps: 50
grad_clip: 1.0

# Quantization settings
use_fp8: true
fp8_format: "E5M2"
use_shadow: true

# Standard optimizer (no manifold-aware)
use_manifold_aware: false

# Checkpointing
checkpoint_interval: 100
checkpoint_dir: "checkpoints/e5m2_standard"
max_checkpoints: 3

# Stability thresholds
nan_patience: 10
bit_stall_threshold: 0.5
overflow_threshold: 0.01
dead_neuron_threshold: 1.0e-8
dead_neuron_window: 100

# Stability interventions
enable_partition_clipping: true
partition_clip_base: 1.0

# Diagnostic sampling
diagnostic_interval: 50
log_stiffness: true
log_grid_alignment: true
log_ulp: true

# Logging
log_interval: 10
eval_interval: 100

# W&B
project: "altgrad"
run_name: "e5m2-standard"
tags:
  - "e5m2"
  - "standard"
  - "phase5"

# Reproducibility
seed: 42
```

Verify configs load correctly:
```bash
cd /Users/prwilliams/Repos/altgrad && source .venv/bin/activate && python -c "
from altgrad.training.config import load_config

# Load manifold config
manifold = load_config('experiments/configs/e5m2_manifold.yaml')
assert manifold.use_manifold_aware == True
assert manifold.manifold_mantissa_bits == 2
assert manifold.log_bit_position == True
print(f'Manifold config: use_manifold_aware={manifold.use_manifold_aware}')

# Load standard config
standard = load_config('experiments/configs/e5m2_standard.yaml')
assert standard.use_manifold_aware == False
print(f'Standard config: use_manifold_aware={standard.use_manifold_aware}')

print('Experiment configs: OK')
"
```
  </action>
  <verify>Both experiment configs load correctly with expected values</verify>
  <done>Example configs created for manifold vs standard comparison experiments</done>
</task>

</tasks>

<verification>
1. TrainConfig has manifold options: `python -c "from altgrad.training import TrainConfig; c = TrainConfig(); print(c.use_manifold_aware)"`
2. Trainer uses ManifoldAdamW when configured: Integration test in Task 2
3. Experiment configs load: `python -c "from altgrad.training import load_config; print(load_config('experiments/configs/e5m2_manifold.yaml').use_manifold_aware)"`
4. All existing tests pass: `pytest tests/ -v --tb=short`
</verification>

<success_criteria>
- TrainConfig has use_manifold_aware, manifold_mantissa_bits, manifold_max_stiffness, log_bit_position
- Trainer creates ManifoldAdamW when use_manifold_aware=True
- Bit-position statistics logged when log_bit_position=True
- Example configs created for manifold vs standard A/B comparison
- All existing tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/05-manifold-aware-optimizer/05-02-SUMMARY.md`
</output>
