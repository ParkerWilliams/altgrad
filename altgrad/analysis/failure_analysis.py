"""Failure mode analysis from W&B and local reports.

Provides FailureAnalyzer class for extracting failure information from
crashed W&B runs and local markdown failure reports generated by
FormatExperimentRunner.

Key features:
  - Extract failures from crashed W&B runs
  - Parse local markdown failure reports
  - Classify failure modes (NaN, bit stall, overflow, etc.)
  - Merge failure data from multiple sources

Example:
    >>> from altgrad.analysis import FailureAnalyzer
    >>> analyzer = FailureAnalyzer()
    >>> failures = analyzer.extract_failures(runs_df)
    >>> classified = analyzer.classify_failure_mode(failures)
"""

from __future__ import annotations

import re
from pathlib import Path
from typing import Dict, List, Optional

import pandas as pd


class FailureAnalyzer:
    """Analyze failure modes from experiments.

    Extracts and classifies failure information from crashed W&B runs
    and local failure report artifacts.

    Example:
        >>> analyzer = FailureAnalyzer()
        >>> df = loader.get_format_comparison_runs()
        >>> failures = analyzer.extract_failures(df)
        >>> by_mode = analyzer.classify_failure_mode(failures)
    """

    # Known failure mode patterns
    FAILURE_MODES = {
        "nan_loss": ["nan", "inf", "loss was nan"],
        "bit_stall": ["bit stall", "stall rate", "gradient vanishing"],
        "overflow": ["overflow", "exceeds", "clip"],
        "oom": ["out of memory", "cuda", "oom"],
        "runtime": ["runtime error", "exception", "traceback"],
    }

    def extract_failures(self, df: pd.DataFrame) -> pd.DataFrame:
        """Extract failure information from crashed runs.

        Filters DataFrame to non-completed runs and extracts relevant
        failure information for analysis.

        Args:
            df: DataFrame with experiment runs (from ExperimentDataLoader)

        Returns:
            DataFrame with failed runs and extracted failure info:
                - run_id, format, state
                - steps_completed, max_steps
                - final_loss, bit_stall_rate, overflow_rate
                - completion_ratio (steps_completed / max_steps)

        Example:
            >>> failures = analyzer.extract_failures(runs_df)
            >>> print(f"Total failures: {len(failures)}")
        """
        # Filter to non-completed runs
        failed = df[df["completed"] == False].copy()  # noqa: E712

        if failed.empty:
            return pd.DataFrame(columns=[
                "run_id", "format", "state", "steps_completed", "max_steps",
                "final_loss", "bit_stall_rate", "overflow_rate", "completion_ratio",
            ])

        # Compute completion ratio
        failed["completion_ratio"] = failed["steps_completed"] / failed["max_steps"]

        # Select relevant columns
        columns = [
            "run_id", "run_name", "format", "state",
            "steps_completed", "max_steps", "completion_ratio",
            "final_loss", "bit_stall_rate", "overflow_rate",
            "created_at",
        ]

        available_cols = [c for c in columns if c in failed.columns]
        return failed[available_cols].reset_index(drop=True)

    def parse_local_failure_reports(
        self,
        reports_dir: str = "checkpoints/failure_reports",
    ) -> pd.DataFrame:
        """Parse local markdown failure reports.

        Reads failure report artifacts generated by FormatExperimentRunner
        and extracts structured data.

        Args:
            reports_dir: Directory containing *_failure_report.md files

        Returns:
            DataFrame with parsed failure data:
                - report_path, format, collapse_step, collapse_reason
                - final_loss, best_loss, total_steps, duration
                - grad_sparsity (fraction of zero gradients)
                - bit_stall_rate, overflow_rate, underflow_rate

        Example:
            >>> local_failures = analyzer.parse_local_failure_reports()
            >>> combined = analyzer.merge_failure_data(wandb_failures, local_failures)
        """
        reports_path = Path(reports_dir)

        if not reports_path.exists():
            return pd.DataFrame(columns=[
                "report_path", "format", "collapse_step", "collapse_reason",
                "final_loss", "best_loss", "grad_sparsity",
            ])

        reports = []

        for report_file in reports_path.glob("*_failure_report.md"):
            try:
                parsed = self._parse_single_report(report_file)
                if parsed:
                    reports.append(parsed)
            except Exception as e:
                print(f"Warning: Failed to parse {report_file}: {e}")

        if not reports:
            return pd.DataFrame(columns=[
                "report_path", "format", "collapse_step", "collapse_reason",
                "final_loss", "best_loss", "grad_sparsity",
            ])

        return pd.DataFrame(reports)

    def _parse_single_report(self, report_path: Path) -> Optional[Dict]:
        """Parse a single failure report markdown file.

        Args:
            report_path: Path to the markdown report file

        Returns:
            Dictionary with extracted data, or None if parsing fails
        """
        content = report_path.read_text()

        result = {
            "report_path": str(report_path),
            "format": None,
            "collapse_step": None,
            "collapse_reason": None,
            "final_loss": float("nan"),
            "best_loss": float("nan"),
            "total_steps": None,
            "duration": None,
            "grad_sparsity": float("nan"),
            "bit_stall_rate": float("nan"),
            "overflow_rate": float("nan"),
            "underflow_rate": float("nan"),
        }

        # Extract format from filename or content
        format_match = re.search(r"(E\d+M\d+)", report_path.name)
        if format_match:
            result["format"] = format_match.group(1)

        # Parse summary table
        # Format:  | Metric | Value |
        summary_patterns = {
            "Format": ("format", str),
            "Collapse Step": ("collapse_step", int),
            "Collapse Reason": ("collapse_reason", str),
            "Final Loss": ("final_loss", float),
            "Best Loss": ("best_loss", float),
            "Total Steps": ("total_steps", str),  # May be "X / Y" format
            "Duration": ("duration", str),
        }

        for label, (key, dtype) in summary_patterns.items():
            pattern = rf"\|\s*{label}\s*\|\s*([^|]+)\s*\|"
            match = re.search(pattern, content, re.IGNORECASE)
            if match:
                value = match.group(1).strip()
                try:
                    if dtype == int:
                        result[key] = int(value)
                    elif dtype == float:
                        result[key] = float(value)
                    else:
                        result[key] = value
                except (ValueError, TypeError):
                    pass

        # Parse gradient sparsity
        sparsity_match = re.search(
            r"near-zero gradients.*?:\s*\*\*(\d+\.?\d*%?)\*\*",
            content, re.IGNORECASE
        )
        if sparsity_match:
            value = sparsity_match.group(1).rstrip("%")
            try:
                result["grad_sparsity"] = float(value) / 100 if "%" not in sparsity_match.group(1) else float(value) / 100
            except ValueError:
                pass

        # Parse quantization metrics table
        quant_patterns = {
            "Bit-Stall Rate": "bit_stall_rate",
            "Overflow Rate": "overflow_rate",
            "Underflow Rate": "underflow_rate",
        }

        for label, key in quant_patterns.items():
            pattern = rf"\|\s*{label}\s*\|\s*(\d+\.?\d*%?)\s*\|"
            match = re.search(pattern, content, re.IGNORECASE)
            if match:
                value = match.group(1).rstrip("%")
                try:
                    # Convert percentage to decimal
                    result[key] = float(value) / 100
                except ValueError:
                    pass

        return result

    def classify_failure_mode(
        self,
        failures_df: pd.DataFrame,
    ) -> pd.DataFrame:
        """Classify failures into failure mode categories.

        Analyzes failure characteristics to determine the primary
        failure mode for each run.

        Args:
            failures_df: DataFrame from extract_failures or parse_local_failure_reports

        Returns:
            DataFrame with added 'failure_mode' column containing one of:
                - "nan_loss": Training collapsed to NaN/Inf
                - "bit_stall": High bit stall rate (>50%)
                - "overflow": High overflow rate (>10%)
                - "early_stop": Stopped before 10% of max steps
                - "unknown": Could not classify

        Example:
            >>> classified = analyzer.classify_failure_mode(failures)
            >>> print(classified.groupby("failure_mode").size())
        """
        if failures_df.empty:
            return failures_df

        df = failures_df.copy()
        modes = []

        for _, row in df.iterrows():
            mode = self._classify_single_failure(row)
            modes.append(mode)

        df["failure_mode"] = modes
        return df

    def _classify_single_failure(self, row: pd.Series) -> str:
        """Classify a single failure row.

        Args:
            row: Series from failures DataFrame

        Returns:
            Failure mode string
        """
        # Check collapse reason if available
        collapse_reason = row.get("collapse_reason", "")
        if collapse_reason and isinstance(collapse_reason, str):
            collapse_lower = collapse_reason.lower()
            if "nan" in collapse_lower or "inf" in collapse_lower:
                return "nan_loss"

        # Check final loss
        final_loss = row.get("final_loss", float("nan"))
        if pd.isna(final_loss) or (isinstance(final_loss, float) and (final_loss != final_loss)):
            return "nan_loss"

        # Check bit stall rate
        bit_stall = row.get("bit_stall_rate", 0.0)
        if pd.notna(bit_stall) and bit_stall > 0.5:
            return "bit_stall"

        # Check overflow rate
        overflow = row.get("overflow_rate", 0.0)
        if pd.notna(overflow) and overflow > 0.1:
            return "overflow"

        # Check completion ratio
        completion = row.get("completion_ratio", 1.0)
        if pd.notna(completion) and completion < 0.1:
            return "early_stop"

        # Check gradient sparsity
        grad_sparsity = row.get("grad_sparsity", 0.0)
        if pd.notna(grad_sparsity) and grad_sparsity > 0.7:
            return "gradient_vanishing"

        return "unknown"

    def merge_failure_data(
        self,
        wandb_failures: pd.DataFrame,
        local_failures: pd.DataFrame,
    ) -> pd.DataFrame:
        """Merge failure data from W&B and local reports.

        Combines information from both sources, using local reports
        to supplement W&B data where available.

        Args:
            wandb_failures: DataFrame from extract_failures
            local_failures: DataFrame from parse_local_failure_reports

        Returns:
            Combined DataFrame with all failure information
        """
        if wandb_failures.empty and local_failures.empty:
            return pd.DataFrame()

        if wandb_failures.empty:
            return local_failures

        if local_failures.empty:
            return wandb_failures

        # Tag sources
        wandb_failures = wandb_failures.copy()
        wandb_failures["source"] = "wandb"

        local_failures = local_failures.copy()
        local_failures["source"] = "local"

        # Concatenate
        combined = pd.concat([wandb_failures, local_failures], ignore_index=True)

        return combined

    def summarize_failures_by_format(
        self,
        failures_df: pd.DataFrame,
    ) -> pd.DataFrame:
        """Summarize failures by format.

        Args:
            failures_df: DataFrame with classified failures

        Returns:
            DataFrame with failure counts by format and mode
        """
        if failures_df.empty or "failure_mode" not in failures_df.columns:
            return pd.DataFrame()

        summary = failures_df.groupby(["format", "failure_mode"]).size().unstack(fill_value=0)
        summary["total_failures"] = summary.sum(axis=1)

        return summary.reset_index()


__all__ = [
    "FailureAnalyzer",
]
